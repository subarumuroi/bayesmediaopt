{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mini CoCaBO. Adapted from Ru's CoCaBO.\n",
    "\n",
    "# Author: Subaru\n",
    "\n",
    "# 1 Define the problem domain, or the data structure we want to fit the model onto.\n",
    "\n",
    "'''\n",
    "# this is currently unknown, but defined here to integrate with code\n",
    "# f = function GPR model, but can't be created without the experimental results\n",
    "'''\n",
    "#this list denotes the number of discrete variables under each categorical variable\n",
    "categories = [2, 3] # first cateogory hsa 2 values, second has 3 values\n",
    "\n",
    "C = categories\n",
    "\n",
    "# below categoricals have to be congrusent with above \n",
    "bounds = [\n",
    "    {'name': 'h1', 'type': 'categorical', 'domain': (0, 1)}, # 2 values\n",
    "    {'name': 'h2', 'type': 'categorical', 'domain': (0, 1, 2)}, # 3 values\n",
    "    {'name': 'x1', 'type': 'continuous', 'domain': (-1, 1)}, # define bounds for continuous variables\n",
    "    {'name': 'x2', 'type': 'continuous', 'domain': (-1, 1)},\n",
    "    {'name': 'x3', 'type': 'continuous', 'domain': (-1, 1)},\n",
    "    {'name': 'x4', 'type': 'continuous', 'domain': (-1, 1)}\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    h1   h2        x1        x2        x3        x4\n",
      "0  1.0  1.0 -0.926422 -0.074857 -0.738720 -0.132605\n",
      "1  0.0  1.0 -0.448141  0.978315  0.750192 -0.699520\n",
      "2  0.0  1.0 -0.762927  0.260334  0.849782 -0.936721\n",
      "3  1.0  1.0  0.270544 -0.736789  0.454367  0.308418\n",
      "4  1.0  2.0  0.979072  0.668273  0.151272  0.138303\n",
      "5  0.0  0.0 -0.214143 -0.930744 -0.575702 -0.422126\n",
      "6  1.0  2.0  0.639063  0.562540 -0.844868  0.654916\n",
      "7  0.0  2.0  0.026622  0.186255 -0.312729  0.496524\n",
      "8  1.0  1.0 -0.007660 -0.547326 -0.086164 -0.370591\n",
      "9  0.0  1.0  0.455674 -0.251789  0.311866  0.910316\n"
     ]
    }
   ],
   "source": [
    "# Assuming you already have your data as raw lab results:\n",
    "# Sample data for demonstration (replace with actual data from lab)\n",
    "# Categorical variables: randomly generate indices based on the domains\n",
    "# Continuous variables: uses Latin Hypercube Sampling (LHS) to generate samples\n",
    "# LHS is a statistical method for generating a distribution of samples of parameter values from a multidimensional distribution.\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyDOE import lhs\n",
    "\n",
    "\n",
    "# Number of samples to generate\n",
    "n_samples = 10\n",
    "random_seed =42\n",
    "np.random.seed = random_seed\n",
    "\n",
    "# Generate Latin Hypercube Samples for continuous variables\n",
    "n_continuous = len(bounds) - len(categories)  # Continuous variables come after categorical ones\n",
    "continuous_bounds = [b['domain'] for b in bounds[len(categories):]]\n",
    "\n",
    "# Initialize LHS for continuous variables\n",
    "lhs_samples = lhs(n_continuous, samples=n_samples)\n",
    "\n",
    "# Map LHS samples to the continuous variables' bounds\n",
    "continuous_data = np.zeros((n_samples, n_continuous))\n",
    "for i, (lower, upper) in enumerate(continuous_bounds):\n",
    "    continuous_data[:, i] = lhs_samples[:, i] * (upper - lower) + lower\n",
    "\n",
    "# Generate categorical data using LHS-style sampling\n",
    "categorical_data = []\n",
    "for i, cat in enumerate(categories):\n",
    "    categorical_data.append(np.random.choice(range(cat), size=n_samples))\n",
    "\n",
    "# Combine categorical and continuous data into a single data set\n",
    "data = np.column_stack(categorical_data + [continuous_data[:, i] for i in range(n_continuous)])\n",
    "\n",
    "# Convert to pandas DataFrame for easy manipulation\n",
    "columns = [b['name'] for b in bounds]  # Extract column names (variable names)\n",
    "df = pd.DataFrame(data, columns=columns)\n",
    "\n",
    "# Print the generated DataFrame\n",
    "print(df)\n",
    "#print(categorical_data)\n",
    "#print(continuous_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    h1   h2        x1        x2        x3        x4         y\n",
      "0  1.0  1.0 -0.926422 -0.074857 -0.738720 -0.132605  3.427152\n",
      "1  0.0  1.0 -0.448141  0.978315  0.750192 -0.699520  3.210046\n",
      "2  0.0  1.0 -0.762927  0.260334  0.849782 -0.936721  3.249408\n",
      "3  1.0  1.0  0.270544 -0.736789  0.454367  0.308418  2.917624\n",
      "4  1.0  2.0  0.979072  0.668273  0.151272  0.138303  4.447183\n",
      "5  0.0  0.0 -0.214143 -0.930744 -0.575702 -0.422126  1.421764\n",
      "6  1.0  2.0  0.639063  0.562540 -0.844868  0.654916  4.867569\n",
      "7  0.0  2.0  0.026622  0.186255 -0.312729  0.496524  2.379736\n",
      "8  1.0  1.0 -0.007660 -0.547326 -0.086164 -0.370591  2.444386\n",
      "9  0.0  1.0  0.455674 -0.251789  0.311866  0.910316  2.196973\n"
     ]
    }
   ],
   "source": [
    "# Define the fake objective function\n",
    "def fake_objective_function(ht_list, X):\n",
    "    \"\"\"A simple quadratic function for testing purposes\"\"\"\n",
    "    return np.sum(np.square(X)) + np.sum(ht_list)  # Just an example\n",
    "\n",
    "# Create the y values (response) for each row\n",
    "y_values = np.array([fake_objective_function(list(df.iloc[i, :len(categories)]), df.iloc[i, len(categories):]) for i in range(len(df))])\n",
    "\n",
    "df1 = df\n",
    "# Add the fake y values as a new column to the dataframe\n",
    "df1['y'] = y_values\n",
    "\n",
    "print(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "# These definitions outside class are utility functions placed here, so it's all in one place.\n",
    "def with_proba(epsilon):\n",
    "        \"\"\"Bernoulli test: Returns True with probability epsilon, otherwise False.\"\"\"\n",
    "        assert 0 <= epsilon <= 1, \"epsilon must be between 0 and 1\"\n",
    "        return random.random() < epsilon\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Created based on\n",
    "https://jeremykun.com/2013/11/08/adversarial-bandits-and-the-exp3-algorithm/\n",
    "\"\"\"\n",
    "# draw: [float] -> int\n",
    "# pick an index from the given list of floats proportionally\n",
    "# to the size of the entry (i.e. normalize to a probability\n",
    "# distribution and draw according to the probabilities).\n",
    "def draw(weights):\n",
    "    choice = random.uniform(0, sum(weights))\n",
    "    #    print(choice)\n",
    "    choiceIndex = 0\n",
    "\n",
    "    for weight in weights:\n",
    "        choice -= weight\n",
    "        if choice <= 0:\n",
    "            return choiceIndex\n",
    "        choiceIndex += 1\n",
    "\n",
    "# distr: [float] -> (float)\n",
    "# Normalize a list of floats to a probability distribution.  Gamma is an\n",
    "# egalitarianism factor, which tempers the distribtuion toward being uniform as\n",
    "# it grows from zero to one.\n",
    "def distr(weights, gamma=0.0):\n",
    "    theSum = float(sum(weights))\n",
    "    return tuple((1.0 - gamma) * (w / theSum) + (gamma / len(weights)) for w in weights)\n",
    "\n",
    "def DepRound(weights_p, k=1, isWeights=True):\n",
    "    r\"\"\" [[Algorithms for adversarial bandit problems with multiple plays, by T.Uchiya, A.Nakamura and M.Kudo, 2010](http://hdl.handle.net/2115/47057)] Figure 5 (page 15) is a very clean presentation of the algorithm.\n",
    "\n",
    "    - Inputs: :math:`k < K` and weights_p :math:`= (p_1, \\dots, p_K)` such that :math:`\\sum_{i=1}^{K} p_i = k` (or :math:`= 1`).\n",
    "    - Output: A subset of :math:`\\{1,\\dots,K\\}` with exactly :math:`k` elements. Each action :math:`i` is selected with probability exactly :math:`p_i`.\n",
    "\n",
    "    Example:\n",
    "\n",
    "    >>> import numpy as np; import random\n",
    "    >>> np.random.seed(0); random.seed(0)  # for reproductibility!\n",
    "    >>> K = 5\n",
    "    >>> k = 2\n",
    "\n",
    "    >>> weights_p = [ 2, 2, 2, 2, 2 ]  # all equal weights\n",
    "    >>> DepRound(weights_p, k)\n",
    "    [3, 4]\n",
    "    >>> DepRound(weights_p, k)\n",
    "    [3, 4]\n",
    "    >>> DepRound(weights_p, k)\n",
    "    [0, 1]\n",
    "\n",
    "    >>> weights_p = [ 10, 8, 6, 4, 2 ]  # decreasing weights\n",
    "    >>> DepRound(weights_p, k)\n",
    "    [0, 4]\n",
    "    >>> DepRound(weights_p, k)\n",
    "    [1, 2]\n",
    "    >>> DepRound(weights_p, k)\n",
    "    [3, 4]\n",
    "\n",
    "    >>> weights_p = [ 3, 3, 0, 0, 3 ]  # decreasing weights\n",
    "    >>> DepRound(weights_p, k)\n",
    "    [0, 4]\n",
    "    >>> DepRound(weights_p, k)\n",
    "    [0, 4]\n",
    "    >>> DepRound(weights_p, k)\n",
    "    [0, 4]\n",
    "    >>> DepRound(weights_p, k)\n",
    "    [0, 1]\n",
    "\n",
    "    - See [[Gandhi et al, 2006](http://dl.acm.org/citation.cfm?id=1147956)] for the details.\n",
    "    \"\"\"\n",
    "    p = np.array(weights_p)\n",
    "    K = len(p)\n",
    "    # Checks\n",
    "    assert k < K, f\"Error: k = {k} should be < K = {K}.\"  # DEBUG\n",
    "    if not np.isclose(np.sum(p), 1):\n",
    "        p = p / np.sum(p)\n",
    "    assert np.all(0 <= p) and np.all(p <= 1), f\"Error: the weights (p_1, ..., p_K) should all be 0 <= p_i <= 1. Got {p}\"  # DEBUG\n",
    "    assert np.isclose(np.sum(p), 1), f\"Error: the sum of weights p_1 + ... + p_K should =1. Got: {np.sum(p)}\"  # DEBUG\n",
    "    # Main loop\n",
    "    possible_ij = [a for a in range(K) if 0 < p[a] < 1]\n",
    "    while possible_ij:\n",
    "        # Choose distinct i, j with 0 < p_i, p_j < 1\n",
    "        if len(possible_ij) == 1:\n",
    "            i = np.random.choice(possible_ij, size=1)\n",
    "            j = i\n",
    "        else:\n",
    "            i, j = np.random.choice(possible_ij, size=2, replace=False)\n",
    "        pi, pj = p[i], p[j]\n",
    "        assert 0 <= pi <= 1, f\"Error: pi = {pi} (with i = {i}) is not 0 <= pi <= 1.\"  # DEBUG\n",
    "        assert 0 <= pj <= 1, f\"Error: pj = {pj} (with j = {j}) is not 0 <= pj <= 1.\"  # DEBUG\n",
    "        assert i != j, f\"Error: i = {i} is different than with j = {j}.\" # DEBUG\n",
    "\n",
    "        # Set alpha, beta\n",
    "        alpha, beta = min(1 - pi, pj), min(pi, 1 - pj)\n",
    "        proba = alpha / (alpha + beta)\n",
    "        if with_proba(proba):  # with probability = proba = alpha/(alpha+beta)\n",
    "            pi, pj = pi + alpha, pj - alpha\n",
    "        else:            # with probability = 1 - proba = beta/(alpha+beta)\n",
    "            pi, pj = pi - beta, pj + beta\n",
    "\n",
    "        # Store\n",
    "        p[i], p[j] = pi, pj\n",
    "        # And update\n",
    "        possible_ij = [a for a in range(K) if 0 < p[a] < 1]\n",
    "        if len([a for a in range(K) if np.isclose(p[a], 0)]) == K - k:\n",
    "            break\n",
    "    # Final step\n",
    "    subset = [a for a in range(K) if np.isclose(p[a], 1)]\n",
    "    if len(subset) < k:\n",
    "        subset = [a for a in range(K) if not np.isclose(p[a], 0)]\n",
    "    assert len(subset) == k, f\"Error: DepRound({weights_p}, {k}) is supposed to return a set of size {k}, but {subset} has size {len(subset)}...\" # DEBUG\n",
    "    return subset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed for draw function.\n"
     ]
    }
   ],
   "source": [
    "def test_draw():\n",
    "    weights = [1, 2, 3]\n",
    "    result = draw(weights)\n",
    "    assert result in [0, 1, 2], f\"Test failed. Result was: {result}\"\n",
    "    print(\"Test passed for draw function.\")\n",
    "\n",
    "test_draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed for distr function.\n"
     ]
    }
   ],
   "source": [
    "def test_distr():\n",
    "    weights = [2, 2, 2]\n",
    "    result = distr(weights)\n",
    "    assert sum(result) == 1.0, f\"Test failed. Sum of probabilities is: {sum(result)}\"\n",
    "    assert all(0 <= x <= 1 for x in result), f\"Test failed. Result contains out-of-bound values: {result}\"\n",
    "    print(\"Test passed for distr function.\")\n",
    "\n",
    "test_distr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed for DepRound function. Result: [1, 2]\n"
     ]
    }
   ],
   "source": [
    "def test_depround():\n",
    "    weights_p = [2, 2, 2, 2, 2]\n",
    "    k = 2\n",
    "    result = DepRound(weights_p, k)\n",
    "    assert len(result) == k, f\"Test failed. Expected subset of size {k}, but got {len(result)}\"\n",
    "    print(f\"Test passed for DepRound function. Result: {result}\")\n",
    "\n",
    "test_depround()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed for with_proba function.\n"
     ]
    }
   ],
   "source": [
    "def test_with_proba():\n",
    "    prob = 0.7\n",
    "    result = with_proba(prob)\n",
    "    assert result in [True, False], f\"Test failed. Result was: {result}\"\n",
    "    print(\"Test passed for with_proba function.\")\n",
    "\n",
    "test_with_proba()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "\n",
    "\n",
    "class miniCoCaBO(): # combining baseBO and CoCaBO_base from Ru\n",
    "\n",
    "     # Ru BaseBO and CoCaBO_base __init__  \n",
    "    def __init__(self, objfn, initN, bounds, C, \n",
    "                  acq_type, kernel_mix=0.5, mix_lr=10, \n",
    "                  model_update_interval =10,\n",
    "                  ard=False, rand_seed=42, debug=False,\n",
    "                  batch_size=1, **kwargs):\n",
    "        self.f = objfn  # function to optimise\n",
    "        self.bounds = bounds  # function bounds\n",
    "        self.batch_size = batch_size\n",
    "        self.C = C  # no of categories\n",
    "        self.C_list = C # Ensure compatibility with existing functions\n",
    "        self.initN = initN  # no: of initial points\n",
    "        self.nDim = len(self.bounds)  # dimension\n",
    "        self.acq_type = acq_type # Acquisition function type\n",
    "        self.rand_seed = rand_seed\n",
    "        self.debug = debug\n",
    "        self.saving_path = None\n",
    "        self.kwargs = kwargs\n",
    "        self.best_val_list =[]\n",
    "\n",
    "        self.x_bounds = np.vstack([d['domain'] for d in self.bounds\n",
    "                                   if d['type'] == 'continuous'])\n",
    "\n",
    "        # Store the ht recommendations for each iteration\n",
    "        self.ht_recommendations = []\n",
    "        self.ht_hist_batch = []\n",
    "\n",
    "        '''\n",
    "        # Store the name of the alogrithm'\n",
    "        self. policy = None'\n",
    "        '''\n",
    "\n",
    "        self.X=[]\n",
    "        self.Y=[]\n",
    "\n",
    "        # To check the best vals\n",
    "        self.gp_bestvals = []\n",
    "        self.ARD = ard\n",
    "\n",
    "        # Keeping track of current interation helps control mix learning\n",
    "        self.iteration = None\n",
    "\n",
    "        self.model_hp = None\n",
    "        self.default_cont_lengthscale = 0.2\n",
    "\n",
    "        self.mix = kernel_mix\n",
    "\n",
    "        if ((model_update_interval % mix_lr ==0) or (mix_lr % model_update_interval ==0)):\n",
    "            self.mix_learn_rate = mix_lr\n",
    "            self.model_update_interval = model_update_interval\n",
    "        else:\n",
    "            self.mix_learn_rate = min(mix_lr, model_update_interval)\n",
    "            self.model_update_interval = min(mix_lr, model_update_interval)\n",
    "        self.mix_used = 0.5\n",
    "\n",
    "        self.name = \"miniCoCaBO\"\n",
    "\n",
    "     #Ru BaseBO function   \n",
    "    def my_func(self, Z):\n",
    "        Z = np.atleast_2d(Z)\n",
    "        if len(Z) == 1:\n",
    "            X = Z[0, len(self.C):]\n",
    "            ht_list = list(Z[0, :len(self.C)])\n",
    "            return self.f(ht_list, X)\n",
    "        else:\n",
    "            f_vals = np.zeros(len(Z))\n",
    "            for ii in range(len(Z)):\n",
    "                X = Z[ii, len(self.C):]\n",
    "                ht_list = list(Z[ii, :len(self.C)].astype(int))\n",
    "                f_vals[ii] = self.f(ht_list, X)\n",
    "            return f_vals \n",
    "    \n",
    "\n",
    "    # Ru CoCaBO_base function This functions controls how categorical weights (Wc) \n",
    "    # are updated. If a certain category is overrepresented alpha reduces their dominance\n",
    "    # and if a certain category is underrepresented alpha increases their probability of being selected\n",
    "    # in the next iteration. This is controled by the gamma parameter. \n",
    "\n",
    "    # Large gamma = explore, small gamma = exploit\n",
    "    def estimate_alpha(self, batch_size, gamma, Wc, C):\n",
    "\n",
    "        def single_evaluation(alpha):\n",
    "            denominator = sum([alpha if val > alpha else val for idx, val in enumerate(Wc)])\n",
    "            rightside = (1 / batch_size - gamma / C) / (1 - gamma)\n",
    "            output = np.abs(alpha / denominator - rightside)\n",
    "\n",
    "            return output\n",
    "\n",
    "        x_tries = np.random.uniform(0, np.max(Wc), size=(100, 1))\n",
    "        y_tries = [single_evaluation(val) for val in x_tries]\n",
    "        # find x optimal for init\n",
    "        # print(f'ytry_len={len(y_tries)}')\n",
    "        idx_min = np.argmin(y_tries)\n",
    "        x_init_min = x_tries[idx_min]\n",
    "\n",
    "        res = minimize(single_evaluation, x_init_min, method='BFGS', options={'gtol': 1e-6, 'disp': False})\n",
    "        if isinstance(res, float):\n",
    "            return res\n",
    "        else:\n",
    "            return res.x\n",
    "        \n",
    "    # Ru CoCaBO_base function\n",
    "    def compute_reward_for_all_cat_variable(self, ht_next_batch_list, batch_size):\n",
    "        # Obtain the reward for each categorical variable: B x len(self.C_list)\n",
    "        ht_batch_list_rewards = np.zeros((batch_size, len(self.C_list)))\n",
    "        for b in range(batch_size):\n",
    "            ht_next_list = ht_next_batch_list[b, :] #select categorical values for this batch\n",
    "\n",
    "            for i in range(len(ht_next_list)):\n",
    "                #Find rows where this categorical variables match\n",
    "                idices = np.where(self.data[0][:, i] == ht_next_list[i])\n",
    "                ht_result = self.result[0][idices]\n",
    "                ht_reward = np.max(ht_result * -1)\n",
    "                ht_batch_list_rewards[b, i] = ht_reward\n",
    "        return ht_batch_list_rewards\n",
    "    \n",
    "    # Ru CoCaBO_base function\n",
    "    def update_weights_for_all_cat_var(self, Gt_ht_list, ht_batch_list, Wc_list, gamma_list,\n",
    "                                        probabilityDistribution_list, batch_size, S0=None):\n",
    "        for j in range(len(self.C_list)):\n",
    "            Wc = Wc_list[j]\n",
    "            C = self.C_list[j]\n",
    "            gamma = gamma_list[j]\n",
    "            probabilityDistribution = probabilityDistribution_list[j]\n",
    "            #\n",
    "            print(f'cat_var={j}, prob={probabilityDistribution}')\n",
    "\n",
    "            if batch_size > 1:\n",
    "                ht_batch_list = ht_batch_list.astype(int)\n",
    "                Gt_ht = Gt_ht_list[:, j]\n",
    "                mybatch_ht = ht_batch_list[:, j]  # 1xB\n",
    "                for ii, ht in enumerate(mybatch_ht):\n",
    "                    Gt_ht_b = Gt_ht[ii]\n",
    "                    estimatedReward = 1.0 * Gt_ht_b / probabilityDistribution[ht]\n",
    "                    if ht not in S0:\n",
    "                        Wc[ht] *= np.exp(batch_size * estimatedReward * gamma / C)\n",
    "            else:\n",
    "                Gt_ht = Gt_ht_list[j]\n",
    "                ht = ht_batch_list[j]  # 1xB\n",
    "                estimatedReward = 1.0 * Gt_ht / probabilityDistribution[ht]\n",
    "                Wc[ht] *= np.exp(estimatedReward * gamma / C)\n",
    "\n",
    "        return Wc_list\n",
    "    \n",
    "    # Ru CoCaBO_base function\n",
    "    def compute_prob_dist_and_draw_hts(self, Wc_list, gamma_list, batch_size):\n",
    "\n",
    "        if batch_size > 1:\n",
    "            ht_batch_list = np.zeros((batch_size, len(self.C_list)))\n",
    "            probabilityDistribution_list = []\n",
    "\n",
    "            for j in range(len(self.C_list)):\n",
    "                Wc = Wc_list[j]\n",
    "                gamma = gamma_list[j]\n",
    "                C = self.C_list[j]\n",
    "                # perform some truncation here\n",
    "                maxW = np.max(Wc)\n",
    "                temp = np.sum(Wc) * (1.0 / batch_size - gamma / C) / (1 - gamma)\n",
    "                if gamma < 1 and maxW >= temp:\n",
    "                    # find a threshold alpha\n",
    "                    alpha = self.estimate_alpha(batch_size, gamma, Wc, C)\n",
    "                    S0 = [idx for idx, val in enumerate(Wc) if val > alpha]\n",
    "                else:\n",
    "                    S0 = []\n",
    "\n",
    "                # Compute the probability for each category\n",
    "                probabilityDistribution = distr(Wc, gamma)\n",
    "\n",
    "                # draw a batch here\n",
    "                if batch_size < C:\n",
    "                    mybatch_ht = DepRound(probabilityDistribution, k=batch_size)\n",
    "                else:\n",
    "                    mybatch_ht = np.random.choice(len(probabilityDistribution), batch_size, p=probabilityDistribution)\n",
    "\n",
    "                # ht_batch_list size: len(self.C_list) x B\n",
    "                ht_batch_list[:, j] = mybatch_ht[:]\n",
    "\n",
    "                # ht_batch_list.append(mybatch_ht)\n",
    "                probabilityDistribution_list.append(probabilityDistribution)\n",
    "\n",
    "            return ht_batch_list, probabilityDistribution_list, S0\n",
    "\n",
    "        else:\n",
    "            ht_list = []\n",
    "            probabilityDistribution_list = []\n",
    "            for j in range(len(self.C_list)):\n",
    "                Wc = Wc_list[j]\n",
    "                gamma = gamma_list[j]\n",
    "                # Compute the probability for each category\n",
    "                probabilityDistribution = distr(Wc, gamma)\n",
    "                # Choose a categorical variable at random\n",
    "                ht = draw(probabilityDistribution)\n",
    "                ht_list.append(ht)\n",
    "                probabilityDistribution_list.append(probabilityDistribution)\n",
    "\n",
    "            return ht_list, probabilityDistribution_list\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected: 3.4271521831146656 Computed: 3.4271521831146656\n"
     ]
    }
   ],
   "source": [
    "# Initialize miniCoCaBO with fake data\n",
    "cocabo = miniCoCaBO(\n",
    "    objfn=fake_objective_function,\n",
    "    initN=len(df),  # Using all data points as initialization\n",
    "    bounds=bounds,\n",
    "    C=categories,\n",
    "    acq_type=\"UCB\",\n",
    "    batch_size=1,\n",
    ")\n",
    "\n",
    "# Test my_func to ensure it computes correct values\n",
    "test_input = df.iloc[0, :-1].values  # Taking the first row, excluding 'y'\n",
    "computed_y = cocabo.my_func(test_input)\n",
    "\n",
    "# Print test result\n",
    "print(\"Expected:\", df.iloc[0, -1], \"Computed:\", computed_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected categorical values:\n",
      " [[0. 1.]\n",
      " [0. 2.]]\n",
      "Prob distribution list: [(0.5, 0.5), (0.3333333333333333, 0.3333333333333333, 0.3333333333333333)]\n",
      "Overly dominant categories (S0): []\n"
     ]
    }
   ],
   "source": [
    "# Split df1 into data and results\n",
    "data = df1.drop(columns=['y']).values  # All columns except 'y'\n",
    "results = df1['y'].values  # Only the 'y' column (rewards)\n",
    "\n",
    "# Separate categorical and continuous values in `data`\n",
    "categorical_data = data[:, :len(categories)]  # First 'len(categories)' columns are categorical\n",
    "continuous_data = data[:, len(categories):]  # The remaining columns are continuous\n",
    "\n",
    "#print(\"Categorical data:\\n\", categorical_data)\n",
    "#print(\"Continuous data:\\n\", continuous_data)\n",
    "\n",
    "# Initialize some test categorical weights\n",
    "Wc_list = [np.ones(c) / c for c in categories]  # Start with uniform weights\n",
    "gamma_list = [0.5] * len(categories)  # Mid-range exploration-exploitation\n",
    "batch_size = 2\n",
    "\n",
    "# Simulate selecting categorical values\n",
    "if batch_size>1:\n",
    "    ht_batch_list, probabilityDistribution_list, S0 = cocabo.compute_prob_dist_and_draw_hts(Wc_list, gamma_list, batch_size)\n",
    "    print(\"Selected categorical values:\\n\", ht_batch_list)\n",
    "    print(\"Prob distribution list:\", probabilityDistribution_list)\n",
    "    print(\"Overly dominant categories (S0):\", S0)\n",
    "else: \n",
    "    ht_batch_list, probabilityDistribution_list = cocabo.compute_prob_dist_and_draw_hts(Wc_list, gamma_list, batch_size)\n",
    "    print(\"Selected categorical values:\", ht_batch_list)\n",
    "    print(\"Prob distribution list:\", probabilityDistribution_list)\n",
    "    \n",
    "# Initialize a list to store the rewards (Gt_ht_list)\n",
    "num_categorical_vars = len(categories)\n",
    "Gt_ht_list = np.zeros((batch_size, num_categorical_vars))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([0.5, 0.5]), array([0.33333333, 0.33333333, 0.33333333])]\n"
     ]
    }
   ],
   "source": [
    "print (Wc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated categorical weights: [array([53.41145854,  0.5       ]), array([0.33333333, 0.33333333, 0.33333333])]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Update categorical weights based on the rewards\n",
    "updated_Wc_list = cocabo.update_weights_for_all_cat_var(Gt_ht_list, ht_batch_list, Wc_list, gamma_list, probabilityDistribution_list, batch_size, S0=S0)\n",
    "\n",
    "# Print updated weights\n",
    "print(\"Updated categorical weights:\", updated_Wc_list)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
