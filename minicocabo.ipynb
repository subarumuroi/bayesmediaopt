{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mini CoCaBO. Adapted from Ru's CoCaBO.\n",
    "\n",
    "# Author: Subaru\n",
    "\n",
    "# 1 Define the problem domain, or the data structure we want to fit the model onto.\n",
    "\n",
    "'''\n",
    "# this is currently unknown, but defined here to integrate with code\n",
    "# f = function GPR model, but can't be created without the experimental results\n",
    "'''\n",
    "#this list denotes the number of discrete variables under each categorical variable\n",
    "categories = [2, 3] # first cateogory hsa 2 values, second has 3 values\n",
    "\n",
    "C = categories\n",
    "\n",
    "# below categoricals have to be congrusent with above \n",
    "bounds = [\n",
    "    {'name': 'h1', 'type': 'categorical', 'domain': (0, 1)}, # 2 values\n",
    "    {'name': 'h2', 'type': 'categorical', 'domain': (0, 1, 2)}, # 3 values\n",
    "    {'name': 'x1', 'type': 'continuous', 'domain': (-1, 1)}, # define bounds for continuous variables\n",
    "    {'name': 'x2', 'type': 'continuous', 'domain': (-1, 1)},\n",
    "    {'name': 'x3', 'type': 'continuous', 'domain': (-1, 1)},\n",
    "    {'name': 'x4', 'type': 'continuous', 'domain': (-1, 1)}\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    h1   h2        x1        x2        x3        x4\n",
      "0  0.0  1.0 -0.575216  0.581358 -0.747151 -0.641946\n",
      "1  0.0  2.0 -0.716558 -0.867246  0.528978  0.396408\n",
      "2  0.0  2.0  0.216330 -0.298497  0.997582 -0.596586\n",
      "3  1.0  0.0  0.781137  0.867297 -0.487535 -0.089762\n",
      "4  1.0  0.0  0.975178 -0.093507 -0.307806 -0.993832\n",
      "5  1.0  2.0  0.447835  0.224983  0.155318  0.510881\n",
      "6  0.0  1.0  0.164268 -0.770282 -0.063662  0.927097\n",
      "7  1.0  0.0 -0.252905 -0.511202 -0.950587  0.046089\n",
      "8  1.0  1.0 -0.894295  0.797901  0.637078 -0.218914\n",
      "9  1.0  2.0 -0.004292  0.032407  0.395199  0.798524\n"
     ]
    }
   ],
   "source": [
    "# Assuming you already have your data as raw lab results:\n",
    "# Sample data for demonstration (replace with actual data from lab)\n",
    "# Categorical variables: randomly generate indices based on the domains\n",
    "# Continuous variables: uses Latin Hypercube Sampling (LHS) to generate samples\n",
    "# LHS is a statistical method for generating a distribution of samples of parameter values from a multidimensional distribution.\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyDOE import lhs\n",
    "\n",
    "\n",
    "# Number of samples to generate\n",
    "n_samples = 10\n",
    "random_seed =42\n",
    "np.random.seed = random_seed\n",
    "\n",
    "# Generate Latin Hypercube Samples for continuous variables\n",
    "n_continuous = len(bounds) - len(categories)  # Continuous variables come after categorical ones\n",
    "continuous_bounds = [b['domain'] for b in bounds[len(categories):]]\n",
    "\n",
    "# Initialize LHS for continuous variables\n",
    "lhs_samples = lhs(n_continuous, samples=n_samples)\n",
    "\n",
    "# Map LHS samples to the continuous variables' bounds\n",
    "continuous_data = np.zeros((n_samples, n_continuous))\n",
    "for i, (lower, upper) in enumerate(continuous_bounds):\n",
    "    continuous_data[:, i] = lhs_samples[:, i] * (upper - lower) + lower\n",
    "\n",
    "# Generate categorical data using LHS-style sampling\n",
    "categorical_data = []\n",
    "for i, cat in enumerate(categories):\n",
    "    categorical_data.append(np.random.choice(range(cat), size=n_samples))\n",
    "\n",
    "# Combine categorical and continuous data into a single data set\n",
    "data = np.column_stack(categorical_data + [continuous_data[:, i] for i in range(n_continuous)])\n",
    "\n",
    "# Convert to pandas DataFrame for easy manipulation\n",
    "columns = [b['name'] for b in bounds]  # Extract column names (variable names)\n",
    "df = pd.DataFrame(data, columns=columns)\n",
    "\n",
    "# Print the generated DataFrame\n",
    "print(df)\n",
    "#print(categorical_data)\n",
    "#print(continuous_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    h1   h2        x1        x2        x3        x4         y\n",
      "0  0.0  1.0 -0.575216  0.581358 -0.747151 -0.641946  2.639181\n",
      "1  0.0  2.0 -0.716558 -0.867246  0.528978  0.396408  3.702527\n",
      "2  0.0  2.0  0.216330 -0.298497  0.997582 -0.596586  3.486984\n",
      "3  1.0  0.0  0.781137  0.867297 -0.487535 -0.089762  2.608127\n",
      "4  1.0  0.0  0.975178 -0.093507 -0.307806 -0.993832  3.042163\n",
      "5  1.0  2.0  0.447835  0.224983  0.155318  0.510881  3.536297\n",
      "6  0.0  1.0  0.164268 -0.770282 -0.063662  0.927097  2.483880\n",
      "7  1.0  0.0 -0.252905 -0.511202 -0.950587  0.046089  2.231030\n",
      "8  1.0  1.0 -0.894295  0.797901  0.637078 -0.218914  3.890202\n",
      "9  1.0  2.0 -0.004292  0.032407  0.395199  0.798524  3.794891\n"
     ]
    }
   ],
   "source": [
    "# Define the fake objective function\n",
    "def fake_objective_function(ht_list, X):\n",
    "    \"\"\"A simple quadratic function for testing purposes\"\"\"\n",
    "    return np.sum(np.square(X)) + np.sum(ht_list)  # Just an example\n",
    "\n",
    "# Create the y values (response) for each row\n",
    "y_values = np.array([fake_objective_function(list(df.iloc[i, :len(categories)]), df.iloc[i, len(categories):]) for i in range(len(df))])\n",
    "\n",
    "df1 = df\n",
    "# Add the fake y values as a new column to the dataframe\n",
    "df1['y'] = y_values\n",
    "\n",
    "print(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# These definitions outside class are utility functions placed here, so it's all in one place.\n",
    "def with_proba(epsilon):\n",
    "        \"\"\"Bernoulli test: Returns True with probability epsilon, otherwise False.\"\"\"\n",
    "        assert 0 <= epsilon <= 1, \"epsilon must be between 0 and 1\"\n",
    "        return random.random() < epsilon\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Created based on\n",
    "https://jeremykun.com/2013/11/08/adversarial-bandits-and-the-exp3-algorithm/\n",
    "\"\"\"\n",
    "# draw: [float] -> int\n",
    "# pick an index from the given list of floats proportionally\n",
    "# to the size of the entry (i.e. normalize to a probability\n",
    "# distribution and draw according to the probabilities).\n",
    "def draw(weights):\n",
    "    choice = random.uniform(0, sum(weights))\n",
    "    #    print(choice)\n",
    "    choiceIndex = 0\n",
    "\n",
    "    for weight in weights:\n",
    "        choice -= weight\n",
    "        if choice <= 0:\n",
    "            return choiceIndex\n",
    "        choiceIndex += 1\n",
    "\n",
    "# distr: [float] -> (float)\n",
    "# Normalize a list of floats to a probability distribution.  Gamma is an\n",
    "# egalitarianism factor, which tempers the distribtuion toward being uniform as\n",
    "# it grows from zero to one.\n",
    "def distr(weights, gamma=0.0):\n",
    "    theSum = float(sum(weights))\n",
    "    return tuple((1.0 - gamma) * (w / theSum) + (gamma / len(weights)) for w in weights)\n",
    "\n",
    "def DepRound(weights_p, k=1, isWeights=True):\n",
    "    r\"\"\" [[Algorithms for adversarial bandit problems with multiple plays, by T.Uchiya, A.Nakamura and M.Kudo, 2010](http://hdl.handle.net/2115/47057)] Figure 5 (page 15) is a very clean presentation of the algorithm.\n",
    "\n",
    "    - Inputs: :math:`k < K` and weights_p :math:`= (p_1, \\dots, p_K)` such that :math:`\\sum_{i=1}^{K} p_i = k` (or :math:`= 1`).\n",
    "    - Output: A subset of :math:`\\{1,\\dots,K\\}` with exactly :math:`k` elements. Each action :math:`i` is selected with probability exactly :math:`p_i`.\n",
    "\n",
    "    Example:\n",
    "\n",
    "    >>> import numpy as np; import random\n",
    "    >>> np.random.seed(0); random.seed(0)  # for reproductibility!\n",
    "    >>> K = 5\n",
    "    >>> k = 2\n",
    "\n",
    "    >>> weights_p = [ 2, 2, 2, 2, 2 ]  # all equal weights\n",
    "    >>> DepRound(weights_p, k)\n",
    "    [3, 4]\n",
    "    >>> DepRound(weights_p, k)\n",
    "    [3, 4]\n",
    "    >>> DepRound(weights_p, k)\n",
    "    [0, 1]\n",
    "\n",
    "    >>> weights_p = [ 10, 8, 6, 4, 2 ]  # decreasing weights\n",
    "    >>> DepRound(weights_p, k)\n",
    "    [0, 4]\n",
    "    >>> DepRound(weights_p, k)\n",
    "    [1, 2]\n",
    "    >>> DepRound(weights_p, k)\n",
    "    [3, 4]\n",
    "\n",
    "    >>> weights_p = [ 3, 3, 0, 0, 3 ]  # decreasing weights\n",
    "    >>> DepRound(weights_p, k)\n",
    "    [0, 4]\n",
    "    >>> DepRound(weights_p, k)\n",
    "    [0, 4]\n",
    "    >>> DepRound(weights_p, k)\n",
    "    [0, 4]\n",
    "    >>> DepRound(weights_p, k)\n",
    "    [0, 1]\n",
    "\n",
    "    - See [[Gandhi et al, 2006](http://dl.acm.org/citation.cfm?id=1147956)] for the details.\n",
    "    \"\"\"\n",
    "    p = np.array(weights_p)\n",
    "    K = len(p)\n",
    "    # Checks\n",
    "    assert k < K, f\"Error: k = {k} should be < K = {K}.\"  # DEBUG\n",
    "    if not np.isclose(np.sum(p), 1):\n",
    "        p = p / np.sum(p)\n",
    "    assert np.all(0 <= p) and np.all(p <= 1), f\"Error: the weights (p_1, ..., p_K) should all be 0 <= p_i <= 1. Got {p}\"  # DEBUG\n",
    "    assert np.isclose(np.sum(p), 1), f\"Error: the sum of weights p_1 + ... + p_K should =1. Got: {np.sum(p)}\"  # DEBUG\n",
    "    # Main loop\n",
    "    possible_ij = [a for a in range(K) if 0 < p[a] < 1]\n",
    "    while possible_ij:\n",
    "        # Choose distinct i, j with 0 < p_i, p_j < 1\n",
    "        if len(possible_ij) == 1:\n",
    "            i = np.random.choice(possible_ij, size=1)\n",
    "            j = i\n",
    "        else:\n",
    "            i, j = np.random.choice(possible_ij, size=2, replace=False)\n",
    "        pi, pj = p[i], p[j]\n",
    "        assert 0 <= pi <= 1, f\"Error: pi = {pi} (with i = {i}) is not 0 <= pi <= 1.\"  # DEBUG\n",
    "        assert 0 <= pj <= 1, f\"Error: pj = {pj} (with j = {j}) is not 0 <= pj <= 1.\"  # DEBUG\n",
    "        assert i != j, f\"Error: i = {i} is different than with j = {j}.\" # DEBUG\n",
    "\n",
    "        # Set alpha, beta\n",
    "        alpha, beta = min(1 - pi, pj), min(pi, 1 - pj)\n",
    "        proba = alpha / (alpha + beta)\n",
    "        if with_proba(proba):  # with probability = proba = alpha/(alpha+beta)\n",
    "            pi, pj = pi + alpha, pj - alpha\n",
    "        else:            # with probability = 1 - proba = beta/(alpha+beta)\n",
    "            pi, pj = pi - beta, pj + beta\n",
    "\n",
    "        # Store\n",
    "        p[i], p[j] = pi, pj\n",
    "        # And update\n",
    "        possible_ij = [a for a in range(K) if 0 < p[a] < 1]\n",
    "        if len([a for a in range(K) if np.isclose(p[a], 0)]) == K - k:\n",
    "            break\n",
    "    # Final step\n",
    "    subset = [a for a in range(K) if np.isclose(p[a], 1)]\n",
    "    if len(subset) < k:\n",
    "        subset = [a for a in range(K) if not np.isclose(p[a], 0)]\n",
    "    assert len(subset) == k, f\"Error: DepRound({weights_p}, {k}) is supposed to return a set of size {k}, but {subset} has size {len(subset)}...\" # DEBUG\n",
    "    return subset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed for draw function.\n"
     ]
    }
   ],
   "source": [
    "def test_draw():\n",
    "    weights = [1, 2, 3]\n",
    "    result = draw(weights)\n",
    "    assert result in [0, 1, 2], f\"Test failed. Result was: {result}\"\n",
    "    print(\"Test passed for draw function.\")\n",
    "\n",
    "test_draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed for distr function.\n"
     ]
    }
   ],
   "source": [
    "def test_distr():\n",
    "    weights = [2, 2, 2]\n",
    "    result = distr(weights)\n",
    "    assert sum(result) == 1.0, f\"Test failed. Sum of probabilities is: {sum(result)}\"\n",
    "    assert all(0 <= x <= 1 for x in result), f\"Test failed. Result contains out-of-bound values: {result}\"\n",
    "    print(\"Test passed for distr function.\")\n",
    "\n",
    "test_distr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed for DepRound function. Result: [1, 2]\n"
     ]
    }
   ],
   "source": [
    "def test_depround():\n",
    "    weights_p = [2, 2, 2, 2, 2]\n",
    "    k = 2\n",
    "    result = DepRound(weights_p, k)\n",
    "    assert len(result) == k, f\"Test failed. Expected subset of size {k}, but got {len(result)}\"\n",
    "    print(f\"Test passed for DepRound function. Result: {result}\")\n",
    "\n",
    "test_depround()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed for with_proba function.\n"
     ]
    }
   ],
   "source": [
    "def test_with_proba():\n",
    "    prob = 0.7\n",
    "    result = with_proba(prob)\n",
    "    assert result in [True, False], f\"Test failed. Result was: {result}\"\n",
    "    print(\"Test passed for with_proba function.\")\n",
    "\n",
    "test_with_proba()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is prob not correct GPyTorch implement\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import gpytorch\n",
    "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
    "from botorch.models import SingleTaskGP\n",
    "from botorch.acquisition import LogExpectedImprovement, UpperConfidenceBound\n",
    "\n",
    "\n",
    "class GPyTorchGP:\n",
    "    def __init__(self, X, Y, kernel, y_norm='meanstd', opt_params=None):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.y_norm = y_norm\n",
    "        self.opt_params = opt_params if opt_params else {}\n",
    "        \n",
    "        self.Y_mean = Y.mean() if y_norm == 'meanstd' else 0\n",
    "        self.Y_std = Y.std() if y_norm == 'meanstd' else 1\n",
    "        \n",
    "        self.model = self._create_gp_model(X, Y, kernel)\n",
    "        self.mll = ExactMarginalLogLikelihood(self.model.likelihood, self.model)\n",
    "        \n",
    "    def _create_gp_model(self, X, Y, kernel):\n",
    "        Y_norm = (Y - self.Y_mean) / self.Y_std if self.y_norm == 'meanstd' else Y\n",
    "        likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "        model = SingleTaskGP(X, Y_norm.unsqueeze(-1), likelihood=likelihood)\n",
    "        return model.to(self.device)\n",
    "    \n",
    "    def optimize(self):\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=0.1)\n",
    "        mll = self.mll\n",
    "        steps = self.opt_params.get('num_restarts', 5) * 50  # Example heuristic for optimization steps\n",
    "        \n",
    "        for _ in range(steps):\n",
    "            optimizer.zero_grad()\n",
    "            output = self.model(self.model.train_inputs[0])\n",
    "            loss = -mll(output, self.model.train_targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    def get_acquisition_function(self, acq_type, best_f=None):\n",
    "        if acq_type == 'EI':\n",
    "            return LogExpectedImprovement(self.model, best_f=best_f)\n",
    "        elif acq_type == 'UCB':\n",
    "            return UpperConfidenceBound(self.model, beta=2.0)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported acquisition function\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\uqkmuroi\\gitcode\\bayesmediaopt\\venv\\Lib\\site-packages\\botorch\\models\\utils\\assorted.py:264: InputDataWarning: Data (input features) is not contained to the unit cube. Please consider min-max scaling the input data.\n",
      "  check_min_max_scaling(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 1. Expected size 4 but got size 1 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[101]\u001b[39m\u001b[32m, line 43\u001b[39m\n\u001b[32m     41\u001b[39m \u001b[38;5;66;03m# Find the next point to evaluate using the acquisition function\u001b[39;00m\n\u001b[32m     42\u001b[39m bounds = (-\u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m)  \u001b[38;5;66;03m# Define the bounds for optimization\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m next_point = \u001b[43moptimize_acquisition\u001b[49m\u001b[43m(\u001b[49m\u001b[43macq_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbounds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[38;5;66;03m# Plot the data and acquisition function\u001b[39;00m\n\u001b[32m     46\u001b[39m X_test = torch.tensor(np.linspace(-\u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m1000\u001b[39m).reshape(-\u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m), dtype=torch.float32)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[101]\u001b[39m\u001b[32m, line 37\u001b[39m, in \u001b[36moptimize_acquisition\u001b[39m\u001b[34m(acq_func, X, bounds)\u001b[39m\n\u001b[32m     35\u001b[39m X_new = torch.linspace(bounds[\u001b[32m0\u001b[39m], bounds[\u001b[32m1\u001b[39m], \u001b[32m1000\u001b[39m).reshape(-\u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m)\n\u001b[32m     36\u001b[39m X_new = X_new.unsqueeze(\u001b[32m1\u001b[39m) \u001b[38;5;66;03m# now 1000x1x1, adding q=1\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m acq_values = \u001b[43macq_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_new\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     38\u001b[39m best_index = torch.argmax(acq_values)\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m X_new[best_index].squeeze().item()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\uqkmuroi\\gitcode\\bayesmediaopt\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\uqkmuroi\\gitcode\\bayesmediaopt\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\uqkmuroi\\gitcode\\bayesmediaopt\\venv\\Lib\\site-packages\\botorch\\utils\\transforms.py:298\u001b[39m, in \u001b[36mt_batch_mode_transform.<locals>.decorator.<locals>.decorated\u001b[39m\u001b[34m(acqf, X, *args, **kwargs)\u001b[39m\n\u001b[32m    296\u001b[39m \u001b[38;5;66;03m# add t-batch dim\u001b[39;00m\n\u001b[32m    297\u001b[39m X = X \u001b[38;5;28;01mif\u001b[39;00m X.dim() > \u001b[32m2\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m X.unsqueeze(\u001b[32m0\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m298\u001b[39m output = \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[43macqf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    299\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(acqf, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m is_ensemble(acqf.model):\n\u001b[32m    300\u001b[39m     \u001b[38;5;66;03m# IDEA: this could be wrapped into SampleReducingMCAcquisitionFunction\u001b[39;00m\n\u001b[32m    301\u001b[39m     output = (\n\u001b[32m    302\u001b[39m         output.mean(dim=-\u001b[32m1\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m acqf._log \u001b[38;5;28;01melse\u001b[39;00m logmeanexp(output, dim=-\u001b[32m1\u001b[39m)\n\u001b[32m    303\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\uqkmuroi\\gitcode\\bayesmediaopt\\venv\\Lib\\site-packages\\botorch\\acquisition\\analytic.py:795\u001b[39m, in \u001b[36mUpperConfidenceBound.forward\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m    784\u001b[39m \u001b[38;5;129m@t_batch_mode_transform\u001b[39m(expected_q=\u001b[32m1\u001b[39m)\n\u001b[32m    785\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: Tensor) -> Tensor:\n\u001b[32m    786\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Evaluate the Upper Confidence Bound on the candidate set X.\u001b[39;00m\n\u001b[32m    787\u001b[39m \n\u001b[32m    788\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    793\u001b[39m \u001b[33;03m        given design points `X`.\u001b[39;00m\n\u001b[32m    794\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m795\u001b[39m     mean, sigma = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_mean_and_sigma\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    796\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (mean \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.maximize \u001b[38;5;28;01melse\u001b[39;00m -mean) + \u001b[38;5;28mself\u001b[39m.beta.sqrt() * sigma\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\uqkmuroi\\gitcode\\bayesmediaopt\\venv\\Lib\\site-packages\\botorch\\acquisition\\analytic.py:100\u001b[39m, in \u001b[36mAnalyticAcquisitionFunction._mean_and_sigma\u001b[39m\u001b[34m(self, X, compute_sigma, min_var)\u001b[39m\n\u001b[32m     86\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Computes the first and second moments of the model posterior.\u001b[39;00m\n\u001b[32m     87\u001b[39m \n\u001b[32m     88\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     97\u001b[39m \u001b[33;03m    returns a single tensor of means if compute_sigma is True.\u001b[39;00m\n\u001b[32m     98\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     99\u001b[39m \u001b[38;5;28mself\u001b[39m.to(device=X.device)  \u001b[38;5;66;03m# ensures buffers / parameters are on the same device\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m100\u001b[39m posterior = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mposterior\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    101\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposterior_transform\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mposterior_transform\u001b[49m\n\u001b[32m    102\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    103\u001b[39m mean = posterior.mean.squeeze(-\u001b[32m2\u001b[39m).squeeze(-\u001b[32m1\u001b[39m)  \u001b[38;5;66;03m# removing redundant dimensions\u001b[39;00m\n\u001b[32m    104\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m compute_sigma:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\uqkmuroi\\gitcode\\bayesmediaopt\\venv\\Lib\\site-packages\\botorch\\models\\gpytorch.py:448\u001b[39m, in \u001b[36mBatchedMultiOutputGPyTorchModel.posterior\u001b[39m\u001b[34m(self, X, output_indices, observation_noise, posterior_transform)\u001b[39m\n\u001b[32m    442\u001b[39m     X, output_dim_idx = add_output_dim(\n\u001b[32m    443\u001b[39m         X=X, original_batch_shape=\u001b[38;5;28mself\u001b[39m._input_batch_shape\n\u001b[32m    444\u001b[39m     )\n\u001b[32m    445\u001b[39m \u001b[38;5;66;03m# NOTE: BoTorch's GPyTorchModels also inherit from GPyTorch's ExactGP, thus\u001b[39;00m\n\u001b[32m    446\u001b[39m \u001b[38;5;66;03m# self(X) calls GPyTorch's ExactGP's __call__, which computes the posterior,\u001b[39;00m\n\u001b[32m    447\u001b[39m \u001b[38;5;66;03m# rather than e.g. SingleTaskGP's forward, which computes the prior.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m448\u001b[39m mvn = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    449\u001b[39m mvn = \u001b[38;5;28mself\u001b[39m._apply_noise(X=X, mvn=mvn, observation_noise=observation_noise)\n\u001b[32m    450\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_outputs > \u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\uqkmuroi\\gitcode\\bayesmediaopt\\venv\\Lib\\site-packages\\gpytorch\\models\\exact_gp.py:325\u001b[39m, in \u001b[36mExactGP.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    323\u001b[39m         train_input = train_input.expand(*batch_shape, *train_input.shape[-\u001b[32m2\u001b[39m:])\n\u001b[32m    324\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[38;5;28minput\u001b[39m.expand(*batch_shape, *\u001b[38;5;28minput\u001b[39m.shape[-\u001b[32m2\u001b[39m:])\n\u001b[32m--> \u001b[39m\u001b[32m325\u001b[39m     full_inputs.append(\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m=\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[32m    327\u001b[39m \u001b[38;5;66;03m# Get the joint distribution for training/test data\u001b[39;00m\n\u001b[32m    328\u001b[39m full_output = \u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m(*full_inputs, **kwargs)\n",
      "\u001b[31mRuntimeError\u001b[39m: Sizes of tensors must match except in dimension 1. Expected size 4 but got size 1 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "# this was a GPyTorch test (still incomplete)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import gpytorch\n",
    "from botorch.models import SingleTaskGP\n",
    "from botorch.acquisition import ExpectedImprovement, UpperConfidenceBound\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming df1's first two columns are categorical, last column is y, others are continuous\n",
    "data = df1.drop(columns=['y']).values  # All columns except 'y'\n",
    "results = df1['y'].values  # Only the 'y' column (rewards)\n",
    "\n",
    "continuous_data = data[:, len(categories):]  # The remaining columns are continuous\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X = torch.tensor(continuous_data, dtype=torch.float64)\n",
    "Y = torch.tensor(results, dtype=torch.float64)\n",
    "\n",
    "\n",
    "\n",
    "# Initialize the kernel (RBF kernel for simplicity)\n",
    "kernel = gpytorch.kernels.MaternKernel()\n",
    "\n",
    "# Test the model with synthetic data\n",
    "# Initialize the GPyTorchGP model\n",
    "model = GPyTorchGP(X, Y, kernel, y_norm='meanstd', opt_params={'num_restarts': 5})\n",
    "\n",
    "# Train the model\n",
    "model.optimize()\n",
    "\n",
    "# Get the acquisition function (Expected Improvement)\n",
    "acq_func = model.get_acquisition_function(acq_type='UCB', best_f=Y.max().item())\n",
    "\n",
    "# Example of using the acquisition function to get the next point\n",
    "def optimize_acquisition(acq_func, X, bounds):\n",
    "    X_new = torch.linspace(bounds[0], bounds[1], 1000).reshape(-1, 1)\n",
    "    X_new = X_new.unsqueeze(1) # now 1000x1x1, adding q=1\n",
    "    acq_values = acq_func(X_new)\n",
    "    best_index = torch.argmax(acq_values)\n",
    "    return X_new[best_index].squeeze().item()\n",
    "\n",
    "# Find the next point to evaluate using the acquisition function\n",
    "bounds = (-1, 1)  # Define the bounds for optimization\n",
    "next_point = optimize_acquisition(acq_func, X, bounds)\n",
    "\n",
    "# Plot the data and acquisition function\n",
    "X_test = torch.tensor(np.linspace(-1, 1, 1000).reshape(-1, 1), dtype=torch.float32)\n",
    "acq_values = acq_func(X_test)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(X.numpy(), Y.numpy(), 'k.', label=\"Observed data\")\n",
    "plt.plot(X_test.numpy(), acq_values.numpy(), label=\"Acquisition function\")\n",
    "plt.axvline(x=next_point, color='r', linestyle='--', label=\"Next point to evaluate\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Next point to evaluate: {next_point}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "#these imports are for RewardperCategoryviaBO\n",
    "import torch\n",
    "from botorch.optim.optimize import sample_then_minimize\n",
    "from botorch.acquisition import ExpectedImprovement, UpperConfidenceBound\n",
    "from gpytorch import kernels\n",
    "# end packages required for RewardperCategoryviaBO\n",
    "\n",
    "class miniCoCaBO(): # combining baseBO and CoCaBO_base from Ru\n",
    "\n",
    "     # Ru BaseBO and CoCaBO_base __init__  \n",
    "    def __init__(self, objfn, initN, bounds, C, \n",
    "                  acq_type, kernel_mix=0.5, mix_lr=10, \n",
    "                  model_update_interval =10,\n",
    "                  ard=False, rand_seed=42, debug=False,\n",
    "                  batch_size=1, **kwargs):\n",
    "        self.f = objfn  # function to optimise\n",
    "        self.bounds = bounds  # function bounds\n",
    "        self.batch_size = batch_size\n",
    "        self.C = C  # no of categories\n",
    "        self.C_list = C # Ensure compatibility with existing functions\n",
    "        self.initN = initN  # no: of initial points\n",
    "        self.nDim = len(self.bounds)  # dimension\n",
    "        self.acq_type = acq_type # Acquisition function type\n",
    "        self.rand_seed = rand_seed\n",
    "        self.debug = debug\n",
    "        self.saving_path = None\n",
    "        self.kwargs = kwargs\n",
    "        self.best_val_list =[]\n",
    "\n",
    "        self.x_bounds = np.vstack([d['domain'] for d in self.bounds\n",
    "                                   if d['type'] == 'continuous'])\n",
    "\n",
    "        # Store the ht recommendations for each iteration\n",
    "        self.ht_recommendations = []\n",
    "        self.ht_hist_batch = []\n",
    "\n",
    "        '''\n",
    "        # Store the name of the alogrithm'\n",
    "        self. policy = None'\n",
    "        '''\n",
    "\n",
    "        self.X=[]\n",
    "        self.Y=[]\n",
    "\n",
    "        # To check the best vals\n",
    "        self.gp_bestvals = []\n",
    "        self.ARD = ard\n",
    "\n",
    "        # Keeping track of current interation helps control mix learning\n",
    "        self.iteration = None\n",
    "\n",
    "        self.model_hp = None\n",
    "        self.default_cont_lengthscale = 0.2\n",
    "\n",
    "        self.mix = kernel_mix\n",
    "\n",
    "        if ((model_update_interval % mix_lr ==0) or (mix_lr % model_update_interval ==0)):\n",
    "            self.mix_learn_rate = mix_lr\n",
    "            self.model_update_interval = model_update_interval\n",
    "        else:\n",
    "            self.mix_learn_rate = min(mix_lr, model_update_interval)\n",
    "            self.model_update_interval = min(mix_lr, model_update_interval)\n",
    "        self.mix_used = 0.5\n",
    "\n",
    "        self.name = \"miniCoCaBO\"\n",
    "\n",
    "     #Ru BaseBO function   \n",
    "    def my_func(self, Z):\n",
    "        Z = np.atleast_2d(Z)\n",
    "        if len(Z) == 1:\n",
    "            X = Z[0, len(self.C):]\n",
    "            ht_list = list(Z[0, :len(self.C)])\n",
    "            return self.f(ht_list, X)\n",
    "        else:\n",
    "            f_vals = np.zeros(len(Z))\n",
    "            for ii in range(len(Z)):\n",
    "                X = Z[ii, len(self.C):]\n",
    "                ht_list = list(Z[ii, :len(self.C)].astype(int))\n",
    "                f_vals[ii] = self.f(ht_list, X)\n",
    "            return f_vals \n",
    "    \n",
    "\n",
    "    # Ru CoCaBO_base function This functions controls how categorical weights (Wc) \n",
    "    # are updated. If a certain category is overrepresented alpha reduces their dominance\n",
    "    # and if a certain category is underrepresented alpha increases their probability of being selected\n",
    "    # in the next iteration. This is controled by the gamma parameter. \n",
    "\n",
    "    # Large gamma = explore, small gamma = exploit\n",
    "    def estimate_alpha(self, batch_size, gamma, Wc, C):\n",
    "\n",
    "        def single_evaluation(alpha):\n",
    "            denominator = sum([alpha if val > alpha else val for idx, val in enumerate(Wc)])\n",
    "            rightside = (1 / batch_size - gamma / C) / (1 - gamma)\n",
    "            output = np.abs(alpha / denominator - rightside)\n",
    "\n",
    "            return output\n",
    "\n",
    "        x_tries = np.random.uniform(0, np.max(Wc), size=(100, 1))\n",
    "        y_tries = [single_evaluation(val) for val in x_tries]\n",
    "        # find x optimal for init\n",
    "        # print(f'ytry_len={len(y_tries)}')\n",
    "        idx_min = np.argmin(y_tries)\n",
    "        x_init_min = x_tries[idx_min]\n",
    "\n",
    "        res = minimize(single_evaluation, x_init_min, method='BFGS', options={'gtol': 1e-6, 'disp': False})\n",
    "        if isinstance(res, float):\n",
    "            return res\n",
    "        else:\n",
    "            return res.x\n",
    "        \n",
    "    # Ru CoCaBO_base function\n",
    "    def compute_reward_for_all_cat_variable(self, ht_next_batch_list, batch_size):\n",
    "        # Obtain the reward for each categorical variable: B x len(self.C_list)\n",
    "        ht_batch_list_rewards = np.zeros((batch_size, len(self.C_list)))\n",
    "        for b in range(batch_size):\n",
    "            ht_next_list = ht_next_batch_list[b, :] #select categorical values for this batch\n",
    "\n",
    "            for i in range(len(ht_next_list)):\n",
    "                #Find rows where this categorical variables match\n",
    "                idices = np.where(self.data[0][:, i] == ht_next_list[i])\n",
    "                ht_result = self.result[0][idices]\n",
    "                ht_reward = np.max(ht_result * -1)\n",
    "                ht_batch_list_rewards[b, i] = ht_reward\n",
    "        return ht_batch_list_rewards\n",
    "    \n",
    "    # Ru CoCaBO_base function\n",
    "    def update_weights_for_all_cat_var(self, Gt_ht_list, ht_batch_list, Wc_list, gamma_list,\n",
    "                                        probabilityDistribution_list, batch_size, S0=None):\n",
    "        for j in range(len(self.C_list)):\n",
    "            Wc = Wc_list[j]\n",
    "            C = self.C_list[j]\n",
    "            gamma = gamma_list[j]\n",
    "            probabilityDistribution = probabilityDistribution_list[j]\n",
    "            #\n",
    "            print(f'cat_var={j}, prob={probabilityDistribution}')\n",
    "\n",
    "            if batch_size > 1:\n",
    "                ht_batch_list = ht_batch_list.astype(int)\n",
    "                Gt_ht = Gt_ht_list[:, j]\n",
    "                mybatch_ht = ht_batch_list[:, j]  # 1xB\n",
    "                for ii, ht in enumerate(mybatch_ht):\n",
    "                    Gt_ht_b = Gt_ht[ii]\n",
    "                    estimatedReward = 1.0 * Gt_ht_b / probabilityDistribution[ht]\n",
    "                    if ht not in S0:\n",
    "                        Wc[ht] *= np.exp(batch_size * estimatedReward * gamma / C)\n",
    "            else:\n",
    "                Gt_ht = Gt_ht_list[j]\n",
    "                ht = ht_batch_list[j]  # 1xB\n",
    "                estimatedReward = 1.0 * Gt_ht / probabilityDistribution[ht]\n",
    "                Wc[ht] *= np.exp(estimatedReward * gamma / C)\n",
    "\n",
    "        return Wc_list\n",
    "    \n",
    "    # Ru CoCaBO_base function\n",
    "    def compute_prob_dist_and_draw_hts(self, Wc_list, gamma_list, batch_size):\n",
    "\n",
    "        if batch_size > 1:\n",
    "            ht_batch_list = np.zeros((batch_size, len(self.C_list)))\n",
    "            probabilityDistribution_list = []\n",
    "\n",
    "            for j in range(len(self.C_list)):\n",
    "                Wc = Wc_list[j]\n",
    "                gamma = gamma_list[j]\n",
    "                C = self.C_list[j]\n",
    "                # perform some truncation here\n",
    "                maxW = np.max(Wc)\n",
    "                temp = np.sum(Wc) * (1.0 / batch_size - gamma / C) / (1 - gamma)\n",
    "                if gamma < 1 and maxW >= temp:\n",
    "                    # find a threshold alpha\n",
    "                    alpha = self.estimate_alpha(batch_size, gamma, Wc, C)\n",
    "                    S0 = [idx for idx, val in enumerate(Wc) if val > alpha]\n",
    "                else:\n",
    "                    S0 = []\n",
    "\n",
    "                # Compute the probability for each category\n",
    "                probabilityDistribution = distr(Wc, gamma)\n",
    "\n",
    "                # draw a batch here\n",
    "                if batch_size < C:\n",
    "                    mybatch_ht = DepRound(probabilityDistribution, k=batch_size)\n",
    "                else:\n",
    "                    mybatch_ht = np.random.choice(len(probabilityDistribution), batch_size, p=probabilityDistribution)\n",
    "\n",
    "                # ht_batch_list size: len(self.C_list) x B\n",
    "                ht_batch_list[:, j] = mybatch_ht[:]\n",
    "\n",
    "                # ht_batch_list.append(mybatch_ht)\n",
    "                probabilityDistribution_list.append(probabilityDistribution)\n",
    "\n",
    "            return ht_batch_list, probabilityDistribution_list, S0\n",
    "\n",
    "        else:\n",
    "            ht_list = []\n",
    "            probabilityDistribution_list = []\n",
    "            for j in range(len(self.C_list)):\n",
    "                Wc = Wc_list[j]\n",
    "                gamma = gamma_list[j]\n",
    "                # Compute the probability for each category\n",
    "                probabilityDistribution = distr(Wc, gamma)\n",
    "                # Choose a categorical variable at random\n",
    "                ht = draw(probabilityDistribution)\n",
    "                ht_list.append(ht)\n",
    "                probabilityDistribution_list.append(probabilityDistribution)\n",
    "\n",
    "            return ht_list, probabilityDistribution_list\n",
    "\n",
    "    #Ru Cocabo base function    \n",
    "    def getBestVal2(self, my_list):\n",
    "        temp = [np.max(i * -1) for i in my_list]\n",
    "        indx1 = [np.argmax(i * -1) for i in my_list]\n",
    "        indx2 = np.argmax(temp)\n",
    "        val = np.max(temp)\n",
    "        list_indx = indx2\n",
    "        val_indx = indx1[indx2]\n",
    "        return val, list_indx, val_indx\n",
    "    \n",
    "    #Ru Cocabo base function\n",
    "    def set_model_params_and_opt_flag(self, model):\n",
    "        \"\"\"\n",
    "        Returns opt_flag, model\n",
    "        \"\"\"\n",
    "        if ((self.iteration >= self.model_update_interval) and\n",
    "                (self.iteration % self.model_update_interval == 0)):\n",
    "            return True, model\n",
    "        else:\n",
    "            # No previous model_hp, so optimise\n",
    "            if self.model_hp is None:\n",
    "                self.model_hp = model.param_array\n",
    "            else:\n",
    "                # print(self.model_hp)\n",
    "                # print(model.param_array)\n",
    "                # previous iter learned mix, so remove mix before setting\n",
    "                if len(model.param_array) < len(self.model_hp):\n",
    "                    model.param_array = self.model_hp[1:]\n",
    "                else:\n",
    "                    model.param_array = self.model_hp\n",
    "\n",
    "            return False, model\n",
    "\n",
    "    # Ru Cocabo function\n",
    "    def RewardperCategoryviaBO(self, objfn, ht_next_list, categorical_dims, continuous_dims, bounds, acq_type, b):\n",
    "\n",
    "        #Get Observation data as PyTorch tensors\n",
    "        Zt = torch.tensor(self.data[0], dtype=torch.float64)\n",
    "        yt = torch.tensor(self.results[0], dtype=torch.float64)\n",
    "\n",
    "        my_kernel, hp_bounds = self.get_kernel(categorical_dims, continuous_dims)\n",
    "\n",
    "        gp_opt_params = {'methods': 'multigrad',\n",
    "                         'num_restarts': 5,\n",
    "                         'restart_bounds': hp_bounds,\n",
    "                         'hp_bounds': hp_bounds,\n",
    "                         'verbose': False}\n",
    "        \n",
    "        gp = GPyTorchGP(Zt, yt, my_kernel, normalize = True)\n",
    "\n",
    "        opt_flag, gp = self.get_kernel(categorical_dims, continuous_dims)\n",
    "        if opt_flag:\n",
    "            print(\"\\noptimising\\n\")\n",
    "            gp.optimize()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected: 3.4271521831146656 Computed: 3.4271521831146656\n"
     ]
    }
   ],
   "source": [
    "# Initialize miniCoCaBO with fake data\n",
    "cocabo = miniCoCaBO(\n",
    "    objfn=fake_objective_function,\n",
    "    initN=len(df),  # Using all data points as initialization\n",
    "    bounds=bounds,\n",
    "    C=categories,\n",
    "    acq_type=\"UCB\",\n",
    "    batch_size=1,\n",
    ")\n",
    "\n",
    "# Test my_func to ensure it computes correct values\n",
    "test_input = df.iloc[0, :-1].values  # Taking the first row, excluding 'y'\n",
    "computed_y = cocabo.my_func(test_input)\n",
    "\n",
    "# Print test result\n",
    "print(\"Expected:\", df.iloc[0, -1], \"Computed:\", computed_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidIndexError",
     "evalue": "(slice(None, None, None), slice(None, 2, None))",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\uqkmuroi\\gitcode\\bayesmediaopt\\venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3804\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3805\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3806\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mindex.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mindex.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: (slice(None, None, None), slice(None, 2, None))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mInvalidIndexError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[107]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m results = df1[\u001b[33m'\u001b[39m\u001b[33my\u001b[39m\u001b[33m'\u001b[39m].values  \u001b[38;5;66;03m# Only the 'y' column (rewards)\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Separate categorical and continuous values in `data`\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m categorical_data = \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcategories\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m  \u001b[38;5;66;03m# First 'len(categories)' columns are categorical\u001b[39;00m\n\u001b[32m      7\u001b[39m continuous_data = data[:, \u001b[38;5;28mlen\u001b[39m(categories):]  \u001b[38;5;66;03m# The remaining columns are continuous\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m#print(\"Categorical data:\\n\", categorical_data)\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m#print(\"Continuous data:\\n\", continuous_data)\u001b[39;00m\n\u001b[32m     11\u001b[39m \n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Initialize some test categorical weights\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\uqkmuroi\\gitcode\\bayesmediaopt\\venv\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4100\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4101\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4102\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4103\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4104\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\uqkmuroi\\gitcode\\bayesmediaopt\\venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3811\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3806\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m   3807\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3808\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3809\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3810\u001b[39m     ):\n\u001b[32m-> \u001b[39m\u001b[32m3811\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m   3812\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3814\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3815\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3816\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n",
      "\u001b[31mInvalidIndexError\u001b[39m: (slice(None, None, None), slice(None, 2, None))"
     ]
    }
   ],
   "source": [
    "# Split df1 into data and results\n",
    "data = df  \n",
    "results = df1['y'].values  # Only the 'y' column (rewards)\n",
    "\n",
    "# Separate categorical and continuous values in `data`\n",
    "categorical_data = data[:, :len(categories)]  # First 'len(categories)' columns are categorical\n",
    "continuous_data = data[:, len(categories):]  # The remaining columns are continuous\n",
    "\n",
    "#print(\"Categorical data:\\n\", categorical_data)\n",
    "#print(\"Continuous data:\\n\", continuous_data)\n",
    "\n",
    "# Initialize some test categorical weights\n",
    "Wc_list = [np.ones(c) / c for c in categories]  # Start with uniform weights\n",
    "gamma_list = [0.5] * len(categories)  # Mid-range exploration-exploitation\n",
    "batch_size = 2\n",
    "\n",
    "# Simulate selecting categorical values\n",
    "if batch_size>1:\n",
    "    ht_batch_list, probabilityDistribution_list, S0 = cocabo.compute_prob_dist_and_draw_hts(Wc_list, gamma_list, batch_size)\n",
    "    print(\"Selected categorical values:\\n\", ht_batch_list)\n",
    "    print(\"Prob distribution list:\", probabilityDistribution_list)\n",
    "    print(\"Overly dominant categories (S0):\", S0)\n",
    "else: \n",
    "    ht_batch_list, probabilityDistribution_list = cocabo.compute_prob_dist_and_draw_hts(Wc_list, gamma_list, batch_size)\n",
    "    print(\"Selected categorical values:\", ht_batch_list)\n",
    "    print(\"Prob distribution list:\", probabilityDistribution_list)\n",
    "    \n",
    "# Initialize a list to store the rewards (Gt_ht_list)\n",
    "num_categorical_vars = len(categories)\n",
    "Gt_ht_list = np.zeros((batch_size, num_categorical_vars))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([0.5, 0.5]), array([0.33333333, 0.33333333, 0.33333333])]\n",
      "[[0. 0.]\n",
      " [0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print (Wc_list)\n",
    "print(Gt_ht_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated categorical weights: [array([53.41145854,  0.5       ]), array([0.33333333, 0.33333333, 0.33333333])]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Update categorical weights based on the rewards\n",
    "updated_Wc_list = cocabo.update_weights_for_all_cat_var(Gt_ht_list, ht_batch_list, Wc_list, gamma_list, probabilityDistribution_list, batch_size, S0=S0)\n",
    "\n",
    "# Print updated weights\n",
    "print(\"Updated categorical weights:\", updated_Wc_list)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
