{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Super simple version for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best continuous: [0.6], Best categorical: [1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\n",
    "\n",
    "class SimpleCoCaBO:\n",
    "    def __init__(self, continuous_dim, categorical_dim, kernel=None, noise_var=1e-5):\n",
    "        # Initialize the model with dimensions of continuous and categorical variables\n",
    "        self.continuous_dim = continuous_dim\n",
    "        self.categorical_dim = categorical_dim\n",
    "        \n",
    "        # Define the kernel for the Gaussian Process (RBF + constant kernel)\n",
    "        self.kernel = kernel if kernel else C(1.0, (1e-4, 1e1)) * RBF(1.0, (1e-4, 1e1))\n",
    "        \n",
    "        # Initialize the Gaussian Process Regressor\n",
    "        self.gpr = GaussianProcessRegressor(kernel=self.kernel, alpha=noise_var)\n",
    "\n",
    "        # Storage for past data\n",
    "        self.X = []  # Stores continuous + categorical variables\n",
    "        self.y = []  # Stores corresponding objective function values\n",
    "\n",
    "    def fit(self, X_cont, X_cat, y):\n",
    "        \"\"\"Fit the Gaussian Process model on both continuous and categorical data.\"\"\"\n",
    "        # Combine continuous and categorical data\n",
    "        X_combined = np.hstack((X_cont, X_cat))\n",
    "        \n",
    "        # Fit the Gaussian Process Regressor model\n",
    "        self.gpr.fit(X_combined, y)\n",
    "        \n",
    "        # Store the data for future optimization\n",
    "        self.X.extend(X_combined)\n",
    "        self.y.extend(y)\n",
    "\n",
    "    def predict(self, X_cont, X_cat):\n",
    "        \"\"\"Predict mean and variance for new points.\"\"\"\n",
    "        X_combined = np.hstack((X_cont, X_cat))\n",
    "        mean, std = self.gpr.predict(X_combined, return_std=True)\n",
    "        return mean, std\n",
    "\n",
    "    def ucb(self, X_cont, X_cat, kappa=2.0):\n",
    "        \"\"\"Upper Confidence Bound (UCB) acquisition function.\"\"\"\n",
    "        mean, std = self.predict(X_cont, X_cat)\n",
    "        ucb_values = mean + kappa * std\n",
    "        return ucb_values\n",
    "\n",
    "    def optimize(self, X_cont, X_cat, kappa=2.0):\n",
    "        \"\"\"Optimize the acquisition function (UCB).\"\"\"\n",
    "        ucb_values = self.ucb(X_cont, X_cat, kappa)\n",
    "        best_idx = np.argmax(ucb_values)  # Select the index with the highest UCB value\n",
    "        return X_cont[best_idx], X_cat[best_idx]\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Example continuous and categorical variables\n",
    "    X_cont = np.array([[0.5], [0.2], [0.7]])  # Example continuous variables\n",
    "    X_cat = np.array([[0], [1], [0]])  # Example categorical variables (just encoded as 0 or 1)\n",
    "    y = np.array([0.3, 0.7, 0.5])  # Objective values\n",
    "\n",
    "    # Instantiate the SimpleCoCaBO object\n",
    "    optimizer = SimpleCoCaBO(continuous_dim=1, categorical_dim=1)\n",
    "\n",
    "    # Fit the model to the data\n",
    "    optimizer.fit(X_cont, X_cat, y)\n",
    "\n",
    "    # Predict UCB values for new points\n",
    "    new_cont = np.array([[0.6], [0.3]])  # New continuous points to evaluate\n",
    "    new_cat = np.array([[1], [0]])  # New categorical points\n",
    "\n",
    "    best_cont, best_cat = optimizer.optimize(new_cont, new_cat)\n",
    "    print(f\"Best continuous: {best_cont}, Best categorical: {best_cat}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next batch of experiments: [[4.21253457 0.10835534]\n",
      " [4.19030749 0.09587925]\n",
      " [1.37641022 2.76727368]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/s/PhD/gitrepo/bayesmediaopt/venv/lib/python3.13/site-packages/sklearn/gaussian_process/kernels.py:442: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/Users/s/PhD/gitrepo/bayesmediaopt/venv/lib/python3.13/site-packages/sklearn/gaussian_process/kernels.py:442: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "from scipy.stats import qmc\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern\n",
    "\n",
    "class BayesianOptimizer:\n",
    "    def __init__(self, bounds, is_categorical, batch_size=3, beta=2.0):\n",
    "        self.bounds = np.array(bounds)  # [(low, high), ...] for each dimension\n",
    "        self.is_categorical = np.array(is_categorical)  # Boolean mask for categorical variables\n",
    "        self.batch_size = batch_size\n",
    "        self.beta = beta  # Controls exploration vs. exploitation\n",
    "        \n",
    "        # Define GP model with Matern kernel\n",
    "        self.kernel = Matern(length_scale=1.0, nu=2.5)\n",
    "        self.gp = GaussianProcessRegressor(kernel=self.kernel, alpha=1e-6, normalize_y=True)\n",
    "\n",
    "        # Store observed data\n",
    "        self.X_train = None\n",
    "        self.y_train = None\n",
    "\n",
    "    def add_observations(self, X_new, y_new):\n",
    "        \"\"\"Update the dataset with new observations.\"\"\"\n",
    "        if self.X_train is None:\n",
    "            self.X_train = np.array(X_new)\n",
    "            self.y_train = np.array(y_new)\n",
    "        else:\n",
    "            self.X_train = np.vstack((self.X_train, X_new))\n",
    "            self.y_train = np.append(self.y_train, y_new)\n",
    "        self.gp.fit(self.X_train, self.y_train)  # Retrain GP\n",
    "\n",
    "    def ucb_acquisition(self, X):\n",
    "        \"\"\"Upper Confidence Bound (UCB) acquisition function.\"\"\"\n",
    "        mean, std = self.gp.predict(X, return_std=True)\n",
    "        return mean + self.beta * std  # Encourages exploration & exploitation\n",
    "\n",
    "    def optimize_acquisition(self):\n",
    "        \"\"\"Finds the next experiment to run using different strategies for continuous & categorical variables.\"\"\"\n",
    "        if np.any(self.is_categorical):\n",
    "            # Latin Hypercube Sampling (LHS) for categorical variables\n",
    "            sampler = qmc.LatinHypercube(d=len(self.bounds))\n",
    "            sample_points = qmc.scale(sampler.random(n=10000), self.bounds[:, 0], self.bounds[:, 1])\n",
    "            best_idx = np.argmax(self.ucb_acquisition(sample_points))\n",
    "            return sample_points[best_idx]\n",
    "        else:\n",
    "            # Use LBFGS for continuous optimization\n",
    "            best_x = None\n",
    "            best_value = -np.inf\n",
    "            for _ in range(10):  # Multi-start optimization\n",
    "                x0 = np.random.uniform(self.bounds[:, 0], self.bounds[:, 1])\n",
    "                res = minimize(lambda x: -self.ucb_acquisition(x.reshape(1, -1)), x0, bounds=self.bounds, method=\"L-BFGS-B\")\n",
    "                if res.fun < best_value:\n",
    "                    best_value = res.fun\n",
    "                    best_x = res.x\n",
    "            return best_x\n",
    "\n",
    "    def batch_selection(self):\n",
    "        \"\"\"Select multiple experiments using the 'constant liar' approach.\"\"\"\n",
    "        selected_points = []\n",
    "        for _ in range(self.batch_size):\n",
    "            next_x = self.optimize_acquisition()\n",
    "            selected_points.append(next_x)\n",
    "            \n",
    "            # \"Lying\" step: Assume a mean value for the next point before real data comes in\n",
    "            fake_y = self.gp.predict(next_x.reshape(1, -1)).mean()\n",
    "            self.add_observations(next_x.reshape(1, -1), fake_y)\n",
    "        \n",
    "        return np.array(selected_points)\n",
    "\n",
    "# Example usage\n",
    "bounds = [(0, 10), (0, 5)]  # Example bounds for 2 variables\n",
    "is_categorical = [False, True]  # First variable is continuous, second is categorical\n",
    "\n",
    "bo = BayesianOptimizer(bounds, is_categorical)\n",
    "\n",
    "# Assume we already have some observations\n",
    "X_initial = np.array([[2, 1], [4, 0], [6, 1]])  # Example (continuous, categorical)\n",
    "y_initial = np.array([0.5, 1.2, 0.8])  # Example target values\n",
    "bo.add_observations(X_initial, y_initial)\n",
    "\n",
    "# Get the next batch of experiments\n",
    "next_experiments = bo.batch_selection()\n",
    "print(\"Next batch of experiments:\", next_experiments)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's try to combine MultiArmBandit UCB into a Gaussian Process Regressor kernel. Constant liar "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction 1: Best continuous: [0.3], Best categorical: omic1, UCB value: 0.5025013626390923, Reward: 0.5993428306022466\n",
      "Prediction 2: Best continuous: [0.6], Best categorical: omic1, UCB value: 1.3976455652602264, Reward: 0.4723471397657631\n",
      "Prediction 3: Best continuous: [0.3], Best categorical: omic1, UCB value: 1.2828692572957117, Reward: 0.6295377076201385\n",
      "Prediction 4: Best continuous: [0.6], Best categorical: omic1, UCB value: 1.4249351250927602, Reward: 0.804605971281605\n",
      "Prediction 5: Best continuous: [0.3], Best categorical: omic1, UCB value: 1.5831585168666864, Reward: 0.4531693250553328\n",
      "\n",
      "Predictions with UCB values:\n",
      "Best continuous: [0.3], Best categorical: omic1, UCB value: 0.5025013626390923\n",
      "Best continuous: [0.6], Best categorical: omic1, UCB value: 1.3976455652602264\n",
      "Best continuous: [0.3], Best categorical: omic1, UCB value: 1.2828692572957117\n",
      "Best continuous: [0.6], Best categorical: omic1, UCB value: 1.4249351250927602\n",
      "Best continuous: [0.3], Best categorical: omic1, UCB value: 1.5831585168666864\n",
      "\n",
      "Best continuous: [0.3], Best categorical: omic1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern\n",
    "\n",
    "class CoCaBO:\n",
    "    def __init__(self, continuous_dim, categorical_names, kernel=None, noise_var=1e-5):\n",
    "        \"\"\"\n",
    "        CoCaBO algorithm initialization\n",
    "        :param continuous_dim: Dimensionality of continuous variables\n",
    "        :param categorical_names: List of categorical names (e.g., ['omic1', 'omic2', ..., 'omic5'])\n",
    "        :param kernel: Kernel for the Gaussian Process (Matern kernel by default)\n",
    "        :param noise_var: Noise variance for Gaussian Process\n",
    "        \"\"\"\n",
    "        self.continuous_dim = continuous_dim\n",
    "        self.categorical_names = categorical_names\n",
    "        self.categorical_dim = len(categorical_names)  # Number of categorical arms\n",
    "        \n",
    "        # Initialize the kernel for Gaussian Process (Matern kernel)\n",
    "        self.kernel = kernel if kernel else Matern(length_scale=1.0, nu=1.5)  # Matern kernel with nu=1.5\n",
    "        \n",
    "        # Initialize the Gaussian Process Regressor\n",
    "        self.gpr = GaussianProcessRegressor(kernel=self.kernel, alpha=noise_var)\n",
    "        \n",
    "        # Storage for past data\n",
    "        self.X_cont = []  # Stores continuous variables\n",
    "        self.X_cat = []   # Stores categorical variables (as indices)\n",
    "        self.y = []       # Stores corresponding objective function values\n",
    "        \n",
    "        # Initialize Multi-Armed Bandit (MAB) for categorical variable\n",
    "        self.mab_rewards = np.zeros(self.categorical_dim)  # Rewards for each categorical arm\n",
    "        self.mab_counts = np.zeros(self.categorical_dim)   # Count of pulls for each categorical arm\n",
    "    \n",
    "    def fit(self, X_cont, X_cat, y):\n",
    "        \"\"\"\n",
    "        Fit the Gaussian Process model on both continuous and categorical data.\n",
    "        :param X_cont: Continuous variables\n",
    "        :param X_cat: Categorical variables (as indices)\n",
    "        :param y: Objective function values (rewards)\n",
    "        \"\"\"\n",
    "        # Combine continuous and categorical data\n",
    "        X_combined = np.hstack((X_cont, X_cat))\n",
    "        \n",
    "        # Fit the Gaussian Process Regressor model\n",
    "        self.gpr.fit(X_combined, y)\n",
    "        \n",
    "        # Store the data for future optimization\n",
    "        self.X_cont.extend(X_cont)\n",
    "        self.X_cat.extend(X_cat)\n",
    "        self.y.extend(y)\n",
    "    \n",
    "    def predict(self, X_cont, X_cat):\n",
    "        \"\"\"\n",
    "        Predict mean and variance for new points.\n",
    "        :param X_cont: Continuous variables\n",
    "        :param X_cat: Categorical variables (as indices)\n",
    "        :return: Mean and standard deviation from GP\n",
    "        \"\"\"\n",
    "        X_combined = np.hstack((X_cont, X_cat))\n",
    "        mean, std = self.gpr.predict(X_combined, return_std=True)\n",
    "        return mean, std\n",
    "    \n",
    "    def ucb(self, X_cont, X_cat, kappa=2.0):\n",
    "        \"\"\"\n",
    "        Upper Confidence Bound (UCB) acquisition function for continuous variables.\n",
    "        :param X_cont: Continuous variables\n",
    "        :param X_cat: Categorical variables (as indices)\n",
    "        :param kappa: Exploration parameter for UCB\n",
    "        :return: UCB values\n",
    "        \"\"\"\n",
    "        mean, std = self.predict(X_cont, X_cat)\n",
    "        ucb_values = mean + kappa * std\n",
    "        return ucb_values\n",
    "    \n",
    "    def select_categorical_arm(self):\n",
    "        \"\"\"\n",
    "        Select the categorical arm using the best-performing arm (exploitation).\n",
    "        :return: Best categorical arm based on historical rewards\n",
    "        \"\"\"\n",
    "        # Select the categorical arm with the highest average reward\n",
    "        best_arm_idx = np.argmax(self.mab_rewards)\n",
    "        return self.categorical_names[best_arm_idx]  # Return the string name of the selected arm\n",
    "    \n",
    "    def update_mab(self, arm_idx, reward):\n",
    "        \"\"\"\n",
    "        Update the reward distribution of the Multi-Armed Bandit (MAB).\n",
    "        :param arm_idx: The index of the arm that was pulled\n",
    "        :param reward: The reward received for pulling the arm\n",
    "        \"\"\"\n",
    "        self.mab_counts[arm_idx] += 1\n",
    "        # Update the reward for the selected arm (simple average reward)\n",
    "        self.mab_rewards[arm_idx] = ((self.mab_counts[arm_idx] - 1) * self.mab_rewards[arm_idx] + reward) / self.mab_counts[arm_idx]\n",
    "    \n",
    "    def constant_liars_algorithm(self, X_cont, X_cat, n_predictions=5, kappa=2.0):\n",
    "        \"\"\"\n",
    "        Constant Liar's Algorithm - Make multiple predictions using the GP and select the best-performing ones.\n",
    "        :param X_cont: New continuous points to evaluate\n",
    "        :param X_cat: New categorical points\n",
    "        :param n_predictions: Number of predictions to make\n",
    "        :param kappa: Exploration parameter for UCB\n",
    "        :return: Best continuous and categorical values from the predictions\n",
    "        \"\"\"\n",
    "        best_predictions = []\n",
    "        \n",
    "        for i in range(n_predictions):\n",
    "            # Select categorical arm using MAB (best-performing arm based on reward distribution)\n",
    "            h_t = self.select_categorical_arm()\n",
    "            h_t_idx = self.categorical_names.index(h_t)  # Convert the arm name to its index\n",
    "            \n",
    "            # Predict UCB values for continuous and categorical variables\n",
    "            ucb_values = self.ucb(X_cont, np.full((X_cont.shape[0], 1), h_t_idx), kappa)\n",
    "            \n",
    "            best_idx = np.argmax(ucb_values)  # Select the index with the highest UCB value\n",
    "            best_cont = X_cont[best_idx]\n",
    "            best_cat = h_t\n",
    "            \n",
    "            # Append prediction results (continuous, categorical)\n",
    "            best_predictions.append((best_cont, best_cat, ucb_values[best_idx]))\n",
    "            \n",
    "            # Simulate querying the function and getting a new reward for the selected arm\n",
    "            ft = np.random.normal(loc=0.5, scale=0.2)  # Simulated reward\n",
    "            \n",
    "            # Update MAB and GP model with new data\n",
    "            self.update_mab(h_t_idx, ft)  # Update the MAB with the new reward\n",
    "            self.fit(np.array([best_cont]), np.array([[h_t_idx]]), np.array([ft]))  # Update the GP model\n",
    "            \n",
    "            print(f\"Prediction {i+1}: Best continuous: {best_cont}, Best categorical: {best_cat}, UCB value: {ucb_values[best_idx]}, Reward: {ft}\")\n",
    "        \n",
    "        # Print all predictions with corresponding UCB values\n",
    "        print(\"\\nPredictions with UCB values:\")\n",
    "        for cont, cat, ucb_value in best_predictions:\n",
    "            print(f\"Best continuous: {cont}, Best categorical: {cat}, UCB value: {ucb_value}\")\n",
    "        \n",
    "        # Return the best prediction (highest UCB)\n",
    "        return max(best_predictions, key=lambda x: x[2])[:2]\n",
    "\n",
    "    def optimize(self, X_cont, X_cat, n_predictions=5, kappa=2.0):\n",
    "        \"\"\"\n",
    "        Optimize the acquisition function (UCB) and select the best continuous and categorical values.\n",
    "        :param X_cont: New continuous points to evaluate\n",
    "        :param X_cat: New categorical points\n",
    "        :param n_predictions: Number of predictions to make using constant liar's algorithm\n",
    "        :param kappa: Exploration parameter for UCB\n",
    "        :return: Best continuous and categorical values\n",
    "        \"\"\"\n",
    "        # Apply the constant liar's algorithm to make n predictions\n",
    "        best_cont, best_cat = self.constant_liars_algorithm(X_cont, X_cat, n_predictions, kappa)\n",
    "        \n",
    "        # Return the best continuous and categorical pair\n",
    "        return best_cont, best_cat\n",
    "\n",
    "# Example Usage\n",
    "\n",
    "np.random.seed(42)  # For reproducibility of results\n",
    "\n",
    "# Categorical names as strings\n",
    "categorical_names = ['omic1', 'omic2', 'omic3', 'omic4', 'omic5']\n",
    "\n",
    "# Example continuous and categorical variables (encoded as indices for MAB)\n",
    "X_cont = np.array([[0.5], [0.2], [0.7]])  # Example continuous variables\n",
    "X_cat = np.array([[0], [1], [2]])  # Example categorical variables (as indices)\n",
    "y = np.array([0.3, 0.7, 0.5])  # Objective values (rewards)\n",
    "\n",
    "# Instantiate the CoCaBO object with categorical names and Matern kernel\n",
    "optimizer = CoCaBO(continuous_dim=1, categorical_names=categorical_names)\n",
    "\n",
    "# Fit the model to the data\n",
    "optimizer.fit(X_cont, X_cat, y)\n",
    "\n",
    "# Predict UCB values for new points\n",
    "new_cont = np.array([[0.6], [0.3]])  # New continuous points to evaluate\n",
    "new_cat = np.array([[1], [0]])  # New categorical points (encoded as indices)\n",
    "\n",
    "# Optimize to get the best combination of continuous and categorical variables\n",
    "best_cont, best_cat = optimizer.optimize(new_cont, new_cat, n_predictions=5)\n",
    "print(f\"\\nBest continuous: {best_cont}, Best categorical: {best_cat}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax version for categorical as that is more common for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction 1: Best continuous: [0.6], Best categorical: omic2, UCB value: 0.8510456065515899, Reward: 0.2776239763906159\n",
      "Prediction 2: Best continuous: [0.3], Best categorical: omic1, UCB value: 1.90326688006542, Reward: 0.5637804369378767\n",
      "Prediction 3: Best continuous: [0.6], Best categorical: omic1, UCB value: 1.3655049452101133, Reward: 0.5558082584400276\n",
      "Prediction 4: Best continuous: [0.6], Best categorical: omic4, UCB value: 2.017893659580081, Reward: 0.7021030569613053\n",
      "Prediction 5: Best continuous: [0.6], Best categorical: omic1, UCB value: 2.022913458382277, Reward: 0.38382437319529705\n",
      "\n",
      "Predictions with UCB values:\n",
      "Best continuous: [0.6], Best categorical: omic2, UCB value: 0.8510456065515899\n",
      "Best continuous: [0.3], Best categorical: omic1, UCB value: 1.90326688006542\n",
      "Best continuous: [0.6], Best categorical: omic1, UCB value: 1.3655049452101133\n",
      "Best continuous: [0.6], Best categorical: omic4, UCB value: 2.017893659580081\n",
      "Best continuous: [0.6], Best categorical: omic1, UCB value: 2.022913458382277\n",
      "\n",
      "Best continuous: [0.6], Best categorical: omic1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern\n",
    "\n",
    "class CoCaBO:\n",
    "    def __init__(self, continuous_dim, categorical_names, kernel=None, noise_var=1e-5, tau=1.0):\n",
    "        \"\"\"\n",
    "        CoCaBO algorithm initialization\n",
    "        :param continuous_dim: Dimensionality of continuous variables\n",
    "        :param categorical_names: List of categorical names (e.g., ['omic1', 'omic2', ..., 'omic5'])\n",
    "        :param kernel: Kernel for the Gaussian Process (Matern kernel by default)\n",
    "        :param noise_var: Noise variance for Gaussian Process\n",
    "        :param tau: Softmax temperature parameter (controls exploration-exploitation)\n",
    "        \"\"\"\n",
    "        self.continuous_dim = continuous_dim\n",
    "        self.categorical_names = categorical_names\n",
    "        self.categorical_dim = len(categorical_names)  # Number of categorical arms\n",
    "        \n",
    "        # Initialize the kernel for Gaussian Process (Matern kernel)\n",
    "        self.kernel = kernel if kernel else Matern(length_scale=1.0, nu=1.5)  # Matern kernel with nu=1.5\n",
    "        \n",
    "        # Initialize the Gaussian Process Regressor\n",
    "        self.gpr = GaussianProcessRegressor(kernel=self.kernel, alpha=noise_var)\n",
    "        \n",
    "        # Storage for past data\n",
    "        self.X_cont = []  # Stores continuous variables\n",
    "        self.X_cat = []   # Stores categorical variables (as indices)\n",
    "        self.y = []       # Stores corresponding objective function values\n",
    "        \n",
    "        # Initialize Multi-Armed Bandit (MAB) for categorical variable\n",
    "        self.mab_rewards = np.zeros(self.categorical_dim)  # Rewards for each categorical arm\n",
    "        self.mab_counts = np.zeros(self.categorical_dim)   # Count of pulls for each categorical arm\n",
    "        self.tau = tau  # Softmax temperature for exploration-exploitation trade-off\n",
    "    \n",
    "    def fit(self, X_cont, X_cat, y):\n",
    "        \"\"\"\n",
    "        Fit the Gaussian Process model on both continuous and categorical data.\n",
    "        :param X_cont: Continuous variables\n",
    "        :param X_cat: Categorical variables (as indices)\n",
    "        :param y: Objective function values (rewards)\n",
    "        \"\"\"\n",
    "        # Combine continuous and categorical data\n",
    "        X_combined = np.hstack((X_cont, X_cat))\n",
    "        \n",
    "        # Fit the Gaussian Process Regressor model\n",
    "        self.gpr.fit(X_combined, y)\n",
    "        \n",
    "        # Store the data for future optimization\n",
    "        self.X_cont.extend(X_cont)\n",
    "        self.X_cat.extend(X_cat)\n",
    "        self.y.extend(y)\n",
    "    \n",
    "    def predict(self, X_cont, X_cat):\n",
    "        \"\"\"\n",
    "        Predict mean and variance for new points.\n",
    "        :param X_cont: Continuous variables\n",
    "        :param X_cat: Categorical variables (as indices)\n",
    "        :return: Mean and standard deviation from GP\n",
    "        \"\"\"\n",
    "        X_combined = np.hstack((X_cont, X_cat))\n",
    "        mean, std = self.gpr.predict(X_combined, return_std=True)\n",
    "        return mean, std\n",
    "    \n",
    "    def ucb(self, X_cont, X_cat, kappa=2.0):\n",
    "        \"\"\"\n",
    "        Upper Confidence Bound (UCB) acquisition function for continuous variables.\n",
    "        :param X_cont: Continuous variables\n",
    "        :param X_cat: Categorical variables (as indices)\n",
    "        :param kappa: Exploration parameter for UCB\n",
    "        :return: UCB values\n",
    "        \"\"\"\n",
    "        mean, std = self.predict(X_cont, X_cat)\n",
    "        ucb_values = mean + kappa * std\n",
    "        return ucb_values\n",
    "    \n",
    "    def softmax_arm_selection(self):\n",
    "        \"\"\"\n",
    "        Select the categorical arm using softmax-based exploration-exploitation.\n",
    "        :return: Best categorical arm based on softmax selection\n",
    "        \"\"\"\n",
    "        # Compute softmax probabilities for each arm based on rewards\n",
    "        exp_rewards = np.exp(self.mab_rewards / self.tau)\n",
    "        softmax_probs = exp_rewards / np.sum(exp_rewards)\n",
    "        \n",
    "        # Sample an arm based on softmax probabilities (exploration-exploitation balance)\n",
    "        arm_idx = np.random.choice(self.categorical_dim, p=softmax_probs)\n",
    "        return self.categorical_names[arm_idx]  # Return the string name of the selected arm\n",
    "    \n",
    "    def update_mab(self, arm_idx, reward):\n",
    "        \"\"\"\n",
    "        Update the reward distribution of the Multi-Armed Bandit (MAB).\n",
    "        :param arm_idx: The index of the arm that was pulled\n",
    "        :param reward: The reward received for pulling the arm\n",
    "        \"\"\"\n",
    "        self.mab_counts[arm_idx] += 1\n",
    "        # Update the reward for the selected arm (simple average reward)\n",
    "        self.mab_rewards[arm_idx] = ((self.mab_counts[arm_idx] - 1) * self.mab_rewards[arm_idx] + reward) / self.mab_counts[arm_idx]\n",
    "    \n",
    "    def constant_liars_algorithm(self, X_cont, X_cat, n_predictions=5, kappa=2.0):\n",
    "        \"\"\"\n",
    "        Constant Liar's Algorithm - Make multiple predictions using the GP and select the best-performing ones.\n",
    "        :param X_cont: New continuous points to evaluate\n",
    "        :param X_cat: New categorical points\n",
    "        :param n_predictions: Number of predictions to make\n",
    "        :param kappa: Exploration parameter for UCB\n",
    "        :return: Best continuous and categorical values from the predictions\n",
    "        \"\"\"\n",
    "        best_predictions = []\n",
    "        \n",
    "        for i in range(n_predictions):\n",
    "            # Select categorical arm using softmax (exploration-exploitation balance)\n",
    "            h_t = self.softmax_arm_selection()\n",
    "            h_t_idx = self.categorical_names.index(h_t)  # Convert the arm name to its index\n",
    "            \n",
    "            # Predict UCB values for continuous and categorical variables\n",
    "            ucb_values = self.ucb(X_cont, np.full((X_cont.shape[0], 1), h_t_idx), kappa)\n",
    "            \n",
    "            best_idx = np.argmax(ucb_values)  # Select the index with the highest UCB value\n",
    "            best_cont = X_cont[best_idx]\n",
    "            best_cat = h_t\n",
    "            \n",
    "            # Append prediction results (continuous, categorical)\n",
    "            best_predictions.append((best_cont, best_cat, ucb_values[best_idx]))\n",
    "            \n",
    "            # Simulate querying the function and getting a new reward for the selected arm\n",
    "            ft = np.random.normal(loc=0.5, scale=0.2)  # Simulated reward\n",
    "            \n",
    "            # Update MAB and GP model with new data\n",
    "            self.update_mab(h_t_idx, ft)  # Update the MAB with the new reward\n",
    "            self.fit(np.array([best_cont]), np.array([[h_t_idx]]), np.array([ft]))  # Update the GP model\n",
    "            \n",
    "            print(f\"Prediction {i+1}: Best continuous: {best_cont}, Best categorical: {best_cat}, UCB value: {ucb_values[best_idx]}, Reward: {ft}\")\n",
    "        \n",
    "        # Print all predictions with corresponding UCB values\n",
    "        print(\"\\nPredictions with UCB values:\")\n",
    "        for cont, cat, ucb_value in best_predictions:\n",
    "            print(f\"Best continuous: {cont}, Best categorical: {cat}, UCB value: {ucb_value}\")\n",
    "        \n",
    "        # Return the best prediction (highest UCB)\n",
    "        return max(best_predictions, key=lambda x: x[2])[:2]\n",
    "\n",
    "    def optimize(self, X_cont, X_cat, n_predictions=5, kappa=2.0):\n",
    "        \"\"\"\n",
    "        Optimize the acquisition function (UCB) and select the best continuous and categorical values.\n",
    "        :param X_cont: New continuous points to evaluate\n",
    "        :param X_cat: New categorical points\n",
    "        :param n_predictions: Number of predictions to make using constant liar's algorithm\n",
    "        :param kappa: Exploration parameter for UCB\n",
    "        :return: Best continuous and categorical values\n",
    "        \"\"\"\n",
    "        # Apply the constant liar's algorithm to make n predictions\n",
    "        best_cont, best_cat = self.constant_liars_algorithm(X_cont, X_cat, n_predictions, kappa)\n",
    "        \n",
    "        # Return the best continuous and categorical pair\n",
    "        return best_cont, best_cat\n",
    "\n",
    "# Example Usage\n",
    "\n",
    "np.random.seed(42)  # For reproducibility of results\n",
    "\n",
    "# Categorical names as strings\n",
    "categorical_names = ['omic1', 'omic2', 'omic3', 'omic4', 'omic5']\n",
    "\n",
    "# Example continuous and categorical variables (encoded as indices for MAB)\n",
    "X_cont = np.array([[0.5], [0.2], [0.7]])  # Example continuous variables\n",
    "X_cat = np.array([[0], [1], [2]])  # Example categorical variables (as indices)\n",
    "y = np.array([0.3, 0.7, 0.5])  # Objective values (rewards)\n",
    "\n",
    "# Instantiate the CoCaBO object with categorical names and Matern kernel\n",
    "optimizer = CoCaBO(continuous_dim=1, categorical_names=categorical_names)\n",
    "\n",
    "# Fit the model to the data\n",
    "optimizer.fit(X_cont, X_cat, y)\n",
    "\n",
    "# Predict UCB values for new points\n",
    "new_cont = np.array([[0.6], [0.3]])  # New continuous points to evaluate\n",
    "new_cat = np.array([[1], [0]])  # New categorical points (encoded as indices)\n",
    "\n",
    "# Optimize to get the best combination of continuous and categorical variables\n",
    "best_cont, best_cat = optimizer.optimize(new_cont, new_cat, n_predictions=5)\n",
    "print(f\"\\nBest continuous: {best_cont}, Best categorical: {best_cat}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
