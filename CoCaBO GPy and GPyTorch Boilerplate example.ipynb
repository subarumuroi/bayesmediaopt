{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the GPyTorch version of a custom bayesian optimizer being developed for IDEABio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import qmc\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, Matern, RationalQuadratic, Kernel, Hyperparameter\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "import GPy\n",
    "import gpytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an old GPy attempt at a category overlap kernel for reference (can delete later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CategoryOverlapKernelGPy(GPy.kern.Kern):\n",
    "    \"\"\"GPy implementation of a categorical overlap kernel.\"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, variance=1.0, active_dims=None, name='catoverlap'):\n",
    "        super().__init__(input_dim, active_dims=active_dims, name=name)\n",
    "        self.variance = GPy.core.parameterization.Param('variance', variance)\n",
    "        self.link_parameter(self.variance)\n",
    "\n",
    "    def K(self, X, X2=None):\n",
    "        if X2 is None:\n",
    "            X2 = X\n",
    "\n",
    "        diff = X[:, None] - X2[None, :]\n",
    "        diff[np.where(np.abs(diff))] = 1  # Mark different categories\n",
    "        k_cat = self.variance * (1 - np.mean(diff, axis=-1))  # Normalize overlap\n",
    "        return k_cat\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just a test to ensure kernel matrix looks correct for categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPy Kernel Matrix:\n",
      " [[1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 1. 0. 0. 0.]\n",
      " [0. 1. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "X_test = np.array([[4], [2], [2], [1], [0], [8]]) #each row is a category\n",
    "\n",
    "# Create kernel and compute the kernel matrix\n",
    "gpy_kernel = CategoryOverlapKernelGPy(input_dim=1, variance=1.0)\n",
    "K_gpy = gpy_kernel.K(X_test)\n",
    "print(\"GPy Kernel Matrix:\\n\", K_gpy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a chatgpt generated model for a simple GP using Matern kernel for me to study how this all works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: tensor([0.4926, 0.4977]), Std: tensor([0.0669, 0.0671])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gpytorch\n",
    "\n",
    "# Define a simple Gaussian Process model using GPyTorch\n",
    "class SimpleGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super().__init__(train_x, train_y, likelihood)\n",
    "        \n",
    "        # Define the mean function (constant mean assumes data has an underlying constant trend)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        \n",
    "        # Define the covariance function (Matern kernel models spatial correlations with smoothness parameter nu=2.5)\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.MaternKernel(nu=2.5))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Compute the mean and covariance matrix for input data\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        \n",
    "        # Return a Gaussian distribution with computed mean and covariance\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "# Wrapper class for training and prediction using the GP model\n",
    "class SimpleGPR:\n",
    "    def __init__(self, input_dim, noise_var=1e-5):\n",
    "        self.input_dim = input_dim  # Number of input dimensions (features)\n",
    "        self.likelihood = gpytorch.likelihoods.GaussianLikelihood()  # Gaussian likelihood models observation noise\n",
    "        self.model = None  # Placeholder for GP model\n",
    "        self.X, self.y = None, None  # Storage for training data\n",
    "        self.noise_var = noise_var  # Noise variance\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        # Store training data (ensure y is a 1D tensor)\n",
    "        self.X, self.y = X, y.squeeze()\n",
    "        \n",
    "        # Initialize the Gaussian Process model\n",
    "        self.model = SimpleGPModel(self.X, self.y, self.likelihood)\n",
    "        \n",
    "        # Set the model and likelihood to training mode\n",
    "        self.model.train()\n",
    "        self.likelihood.train()\n",
    "        \n",
    "        # Use Adam optimizer to maximize marginal likelihood\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=0.1)\n",
    "        \n",
    "        # Define the loss function (negative marginal log likelihood)\n",
    "        mll = gpytorch.mlls.ExactMarginalLogLikelihood(self.likelihood, self.model)\n",
    "        \n",
    "        # Training loop (optimize parameters for 50 iterations)\n",
    "        for _ in range(50):\n",
    "            optimizer.zero_grad()  # Reset gradients\n",
    "            output = self.model(self.X)  # Compute GP predictions\n",
    "            loss = -mll(output, self.y)  # Compute negative log likelihood\n",
    "            loss.backward()  # Compute gradients\n",
    "            optimizer.step()  # Update model parameters\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # Set the model and likelihood to evaluation mode\n",
    "        self.model.eval()\n",
    "        self.likelihood.eval()\n",
    "        \n",
    "        # Disable gradient computation for prediction\n",
    "        with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "            preds = self.model(X)  # Get GP predictions\n",
    "        \n",
    "        # Return mean prediction and standard deviation\n",
    "        return preds.mean, preds.variance.sqrt()\n",
    "\n",
    "# Example Usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Example training data (3 continuous input features)\n",
    "    X = torch.tensor([\n",
    "        [0.5, 0.2, 0.1],\n",
    "        [0.2, 0.8, 0.3],\n",
    "        [0.7, 0.5, 0.6]\n",
    "    ])  # Training inputs\n",
    "    y = torch.tensor([0.3, 0.7, 0.5])  # Corresponding target values\n",
    "    \n",
    "    # Instantiate and train the Gaussian Process Regression model\n",
    "    gpr = SimpleGPR(input_dim=3)\n",
    "    gpr.fit(X, y)\n",
    "    \n",
    "    # New test points for prediction (3 features each)\n",
    "    new_X = torch.tensor([\n",
    "        [0.6, 0.3, 0.4],\n",
    "        [0.3, 0.7, 0.2]\n",
    "    ])\n",
    "    mean, std = gpr.predict(new_X)\n",
    "    \n",
    "    # Print mean and standard deviation of predictions\n",
    "    print(f\"Mean: {mean}, Std: {std}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This below is the same as above but with chatgpt trying to integrate the cat overlap. it will clearly not work, but could potentially be useful for further understanding gpytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\uqkmuroi\\gitcode\\bayesmediaopt\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\uqkmuroi\\gitcode\\bayesmediaopt\\venv\\Lib\\site-packages\\linear_operator\\utils\\cholesky.py:40: NumericalWarning: A not p.d., added jitter of 1.0e-06 to the diagonal\n",
      "  warnings.warn(\n",
      "c:\\Users\\uqkmuroi\\gitcode\\bayesmediaopt\\venv\\Lib\\site-packages\\linear_operator\\utils\\cholesky.py:40: NumericalWarning: A not p.d., added jitter of 1.0e-05 to the diagonal\n",
      "  warnings.warn(\n",
      "c:\\Users\\uqkmuroi\\gitcode\\bayesmediaopt\\venv\\Lib\\site-packages\\linear_operator\\utils\\cholesky.py:40: NumericalWarning: A not p.d., added jitter of 1.0e-04 to the diagonal\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "NotPSDError",
     "evalue": "Matrix not positive definite after repeatedly adding jitter up to 1.0e-04.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotPSDError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 90\u001b[39m\n\u001b[32m     88\u001b[39m \u001b[38;5;66;03m# Instantiate and train the Gaussian Process Regression model\u001b[39;00m\n\u001b[32m     89\u001b[39m gpr = SimpleGPR(input_dim=\u001b[32m3\u001b[39m, categorical_dim=\u001b[32m1\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m \u001b[43mgpr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     92\u001b[39m \u001b[38;5;66;03m# New test points for prediction (3 continuous features + 1 categorical)\u001b[39;00m\n\u001b[32m     93\u001b[39m new_X = torch.tensor([\n\u001b[32m     94\u001b[39m     [\u001b[32m0.6\u001b[39m, \u001b[32m0.3\u001b[39m, \u001b[32m0.4\u001b[39m, \u001b[32m1\u001b[39m],\n\u001b[32m     95\u001b[39m     [\u001b[32m0.3\u001b[39m, \u001b[32m0.7\u001b[39m, \u001b[32m0.2\u001b[39m, \u001b[32m0\u001b[39m]\n\u001b[32m     96\u001b[39m ])\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 65\u001b[39m, in \u001b[36mSimpleGPR.fit\u001b[39m\u001b[34m(self, X, y)\u001b[39m\n\u001b[32m     63\u001b[39m optimizer.zero_grad()  \u001b[38;5;66;03m# Reset gradients\u001b[39;00m\n\u001b[32m     64\u001b[39m output = \u001b[38;5;28mself\u001b[39m.model(\u001b[38;5;28mself\u001b[39m.X)  \u001b[38;5;66;03m# Compute GP predictions\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m loss = -\u001b[43mmll\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Compute negative log likelihood\u001b[39;00m\n\u001b[32m     66\u001b[39m loss.backward()  \u001b[38;5;66;03m# Compute gradients\u001b[39;00m\n\u001b[32m     67\u001b[39m optimizer.step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\uqkmuroi\\gitcode\\bayesmediaopt\\venv\\Lib\\site-packages\\gpytorch\\module.py:82\u001b[39m, in \u001b[36mModule.__call__\u001b[39m\u001b[34m(self, *inputs, **kwargs)\u001b[39m\n\u001b[32m     81\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *inputs, **kwargs) -> Union[Tensor, Distribution, LinearOperator]:\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     83\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;28mlist\u001b[39m):\n\u001b[32m     84\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m [_validate_module_outputs(output) \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m outputs]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\uqkmuroi\\gitcode\\bayesmediaopt\\venv\\Lib\\site-packages\\gpytorch\\mlls\\exact_marginal_log_likelihood.py:82\u001b[39m, in \u001b[36mExactMarginalLogLikelihood.forward\u001b[39m\u001b[34m(self, function_dist, target, *params, **kwargs)\u001b[39m\n\u001b[32m     79\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mNaN observation policy \u001b[39m\u001b[33m'\u001b[39m\u001b[33mfill\u001b[39m\u001b[33m'\u001b[39m\u001b[33m is not supported by ExactMarginalLogLikelihood!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     81\u001b[39m \u001b[38;5;66;03m# Get the log prob of the marginal distribution\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m res = \u001b[43moutput\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlog_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     83\u001b[39m res = \u001b[38;5;28mself\u001b[39m._add_other_terms(res, params)\n\u001b[32m     85\u001b[39m \u001b[38;5;66;03m# Scale by the amount of data we have\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\uqkmuroi\\gitcode\\bayesmediaopt\\venv\\Lib\\site-packages\\gpytorch\\distributions\\multivariate_normal.py:250\u001b[39m, in \u001b[36mMultivariateNormal.log_prob\u001b[39m\u001b[34m(self, value)\u001b[39m\n\u001b[32m    248\u001b[39m \u001b[38;5;66;03m# Get log determininant and first part of quadratic form\u001b[39;00m\n\u001b[32m    249\u001b[39m covar = covar.evaluate_kernel()\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m inv_quad, logdet = \u001b[43mcovar\u001b[49m\u001b[43m.\u001b[49m\u001b[43minv_quad_logdet\u001b[49m\u001b[43m(\u001b[49m\u001b[43minv_quad_rhs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdiff\u001b[49m\u001b[43m.\u001b[49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogdet\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    252\u001b[39m res = -\u001b[32m0.5\u001b[39m * \u001b[38;5;28msum\u001b[39m([inv_quad, logdet, diff.size(-\u001b[32m1\u001b[39m) * math.log(\u001b[32m2\u001b[39m * math.pi)])\n\u001b[32m    253\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\uqkmuroi\\gitcode\\bayesmediaopt\\venv\\Lib\\site-packages\\linear_operator\\operators\\_linear_operator.py:1709\u001b[39m, in \u001b[36mLinearOperator.inv_quad_logdet\u001b[39m\u001b[34m(self, inv_quad_rhs, logdet, reduce_inv_quad)\u001b[39m\n\u001b[32m   1707\u001b[39m             will_need_cholesky = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   1708\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m will_need_cholesky:\n\u001b[32m-> \u001b[39m\u001b[32m1709\u001b[39m         cholesky = CholLinearOperator(TriangularLinearOperator(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcholesky\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[32m   1710\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cholesky.inv_quad_logdet(\n\u001b[32m   1711\u001b[39m         inv_quad_rhs=inv_quad_rhs,\n\u001b[32m   1712\u001b[39m         logdet=logdet,\n\u001b[32m   1713\u001b[39m         reduce_inv_quad=reduce_inv_quad,\n\u001b[32m   1714\u001b[39m     )\n\u001b[32m   1716\u001b[39m \u001b[38;5;66;03m# Short circuit to inv_quad function if we're not computing logdet\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\uqkmuroi\\gitcode\\bayesmediaopt\\venv\\Lib\\site-packages\\linear_operator\\operators\\_linear_operator.py:1311\u001b[39m, in \u001b[36mLinearOperator.cholesky\u001b[39m\u001b[34m(self, upper)\u001b[39m\n\u001b[32m   1301\u001b[39m \u001b[38;5;129m@_implements\u001b[39m(torch.linalg.cholesky)\n\u001b[32m   1302\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcholesky\u001b[39m(\n\u001b[32m   1303\u001b[39m     \u001b[38;5;28mself\u001b[39m: Float[LinearOperator, \u001b[33m\"\u001b[39m\u001b[33m*batch N N\u001b[39m\u001b[33m\"\u001b[39m], upper: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   1304\u001b[39m ) -> Float[LinearOperator, \u001b[33m\"\u001b[39m\u001b[33m*batch N N\u001b[39m\u001b[33m\"\u001b[39m]:  \u001b[38;5;66;03m# returns TriangularLinearOperator\u001b[39;00m\n\u001b[32m   1305\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1306\u001b[39m \u001b[33;03m    Cholesky-factorizes the LinearOperator.\u001b[39;00m\n\u001b[32m   1307\u001b[39m \n\u001b[32m   1308\u001b[39m \u001b[33;03m    :param upper: Upper triangular or lower triangular factor (default: False).\u001b[39;00m\n\u001b[32m   1309\u001b[39m \u001b[33;03m    :return: Cholesky factor (lower or upper triangular)\u001b[39;00m\n\u001b[32m   1310\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1311\u001b[39m     chol = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_cholesky\u001b[49m\u001b[43m(\u001b[49m\u001b[43mupper\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   1312\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m upper:\n\u001b[32m   1313\u001b[39m         chol = chol._transpose_nonbatch()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\uqkmuroi\\gitcode\\bayesmediaopt\\venv\\Lib\\site-packages\\linear_operator\\utils\\memoize.py:59\u001b[39m, in \u001b[36m_cached.<locals>.g\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     57\u001b[39m kwargs_pkl = pickle.dumps(kwargs)\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_in_cache(\u001b[38;5;28mself\u001b[39m, cache_name, *args, kwargs_pkl=kwargs_pkl):\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _add_to_cache(\u001b[38;5;28mself\u001b[39m, cache_name, \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m, *args, kwargs_pkl=kwargs_pkl)\n\u001b[32m     60\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _get_from_cache(\u001b[38;5;28mself\u001b[39m, cache_name, *args, kwargs_pkl=kwargs_pkl)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\uqkmuroi\\gitcode\\bayesmediaopt\\venv\\Lib\\site-packages\\linear_operator\\operators\\_linear_operator.py:528\u001b[39m, in \u001b[36mLinearOperator._cholesky\u001b[39m\u001b[34m(self, upper)\u001b[39m\n\u001b[32m    525\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m TriangularLinearOperator(evaluated_mat.clamp_min(\u001b[32m0.0\u001b[39m).sqrt())\n\u001b[32m    527\u001b[39m \u001b[38;5;66;03m# contiguous call is necessary here\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m528\u001b[39m cholesky = \u001b[43mpsd_safe_cholesky\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluated_mat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupper\u001b[49m\u001b[43m=\u001b[49m\u001b[43mupper\u001b[49m\u001b[43m)\u001b[49m.contiguous()\n\u001b[32m    529\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m TriangularLinearOperator(cholesky, upper=upper)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\uqkmuroi\\gitcode\\bayesmediaopt\\venv\\Lib\\site-packages\\linear_operator\\utils\\cholesky.py:65\u001b[39m, in \u001b[36mpsd_safe_cholesky\u001b[39m\u001b[34m(A, upper, out, jitter, max_tries)\u001b[39m\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpsd_safe_cholesky\u001b[39m(A, upper=\u001b[38;5;28;01mFalse\u001b[39;00m, out=\u001b[38;5;28;01mNone\u001b[39;00m, jitter=\u001b[38;5;28;01mNone\u001b[39;00m, max_tries=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m     51\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Compute the Cholesky decomposition of A. If A is only p.s.d, add a small jitter to the diagonal.\u001b[39;00m\n\u001b[32m     52\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m     53\u001b[39m \u001b[33;03m        :attr:`A` (Tensor):\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     63\u001b[39m \u001b[33;03m            Number of attempts (with successively increasing jitter) to make before raising an error.\u001b[39;00m\n\u001b[32m     64\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m     L = \u001b[43m_psd_safe_cholesky\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjitter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mjitter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_tries\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     66\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m upper:\n\u001b[32m     67\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\uqkmuroi\\gitcode\\bayesmediaopt\\venv\\Lib\\site-packages\\linear_operator\\utils\\cholesky.py:47\u001b[39m, in \u001b[36m_psd_safe_cholesky\u001b[39m\u001b[34m(A, out, jitter, max_tries)\u001b[39m\n\u001b[32m     45\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch.any(info):\n\u001b[32m     46\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m L\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m NotPSDError(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMatrix not positive definite after repeatedly adding jitter up to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mjitter_new\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.1e\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNotPSDError\u001b[39m: Matrix not positive definite after repeatedly adding jitter up to 1.0e-04."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gpytorch\n",
    "\n",
    "# Define a custom categorical kernel using category overlap\n",
    "class CategoricalKernel(gpytorch.kernels.Kernel):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(has_lengthscale=False, **kwargs)  # No lengthscale for categorical kernel\n",
    "        self.overlap_variance = torch.nn.Parameter(torch.tensor(1.0))  # Trainable variance parameter\n",
    "\n",
    "    def forward(self, x1, x2, **params):\n",
    "        # Compute category overlap matrix (1 if same, 0 if different)\n",
    "        overlap_matrix = (x1[:, None] == x2[None, :]).float().sum(dim=-1)\n",
    "        return self.overlap_variance * overlap_matrix\n",
    "\n",
    "# Define a Gaussian Process model using both Matern kernel (for continuous) and categorical kernel\n",
    "class SimpleGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood, cat_dim):\n",
    "        super().__init__(train_x, train_y, likelihood)\n",
    "        \n",
    "        # Define the mean function (assumes an underlying constant trend)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        \n",
    "        # Define the covariance function\n",
    "        self.continuous_covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.MaternKernel(nu=2.5))\n",
    "        self.categorical_covar_module = CategoricalKernel()\n",
    "        \n",
    "        # Combine continuous and categorical kernels additively\n",
    "        self.covar_module = self.continuous_covar_module + self.categorical_covar_module\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "# Wrapper class for training and prediction using the GP model\n",
    "class SimpleGPR:\n",
    "    def __init__(self, input_dim=3, categorical_dim=1, noise_var=1e-5):\n",
    "        self.input_dim = input_dim  # Number of continuous input dimensions\n",
    "        self.categorical_dim = categorical_dim  # Number of categorical variables\n",
    "        self.likelihood = gpytorch.likelihoods.GaussianLikelihood()  # Gaussian likelihood models observation noise\n",
    "        self.model = None  # Placeholder for GP model\n",
    "        self.X, self.y = None, None  # Storage for training data\n",
    "        self.noise_var = noise_var  # Noise variance\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.X, self.y = X, y.squeeze()\n",
    "        \n",
    "        # Initialize the Gaussian Process model\n",
    "        self.model = SimpleGPModel(self.X, self.y, self.likelihood, self.categorical_dim)\n",
    "        \n",
    "        # Set the model and likelihood to training mode\n",
    "        self.model.train()\n",
    "        self.likelihood.train()\n",
    "        \n",
    "        # Use Adam optimizer to maximize marginal likelihood\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=0.1)\n",
    "        \n",
    "        # Define the loss function (negative marginal log likelihood)\n",
    "        mll = gpytorch.mlls.ExactMarginalLogLikelihood(self.likelihood, self.model)\n",
    "        \n",
    "        # Training loop (optimize parameters for 50 iterations)\n",
    "        for _ in range(50):\n",
    "            optimizer.zero_grad()  # Reset gradients\n",
    "            output = self.model(self.X)  # Compute GP predictions\n",
    "            loss = -mll(output, self.y)  # Compute negative log likelihood\n",
    "            loss.backward()  # Compute gradients\n",
    "            optimizer.step()  # Update model parameters\n",
    "    \n",
    "    def predict(self, X):\n",
    "        self.model.eval()\n",
    "        self.likelihood.eval()\n",
    "        \n",
    "        with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "            preds = self.model(X)\n",
    "        \n",
    "        return preds.mean, preds.variance.sqrt()\n",
    "\n",
    "# Example Usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Example training data (3 continuous input features + 1 categorical variable)\n",
    "    X = torch.tensor([\n",
    "        [0.5, 0.2, 0.1, 0],\n",
    "        [0.2, 0.8, 0.3, 1],\n",
    "        [0.7, 0.5, 0.6, 0]\n",
    "    ])  # Training inputs (last column is categorical)\n",
    "    y = torch.tensor([0.3, 0.7, 0.5])  # Corresponding target values\n",
    "    \n",
    "    # Instantiate and train the Gaussian Process Regression model\n",
    "    gpr = SimpleGPR(input_dim=3, categorical_dim=1)\n",
    "    gpr.fit(X, y)\n",
    "    \n",
    "    # New test points for prediction (3 continuous features + 1 categorical)\n",
    "    new_X = torch.tensor([\n",
    "        [0.6, 0.3, 0.4, 1],\n",
    "        [0.3, 0.7, 0.2, 0]\n",
    "    ])\n",
    "    mean, std = gpr.predict(new_X)\n",
    "    \n",
    "    print(f\"Mean: {mean}, Std: {std}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an attempt at making the GPyTorch model work with the cocabo concept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gpytorch\n",
    "\n",
    "class CategoryOverlapKernel(gpytorch.kernels.Kernel):\n",
    "    \"\"\"Custom kernel for cateogrical varaibles using category overlap similarity.\"\"\"\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(has_lengthsacle = False, **kwargs)\n",
    "    \n",
    "    def forward(self, x1, x2, diag = False, **params):\n",
    "        if diag:\n",
    "            return torch.ones(x1.shape[0], dtype=x1.dtype, device=x1.device)\n",
    "        \n",
    "        #check if categorical values are teh same (Kronecker delta function)\n",
    "        overlap = x1[:, None] == x2[None, :].float()\n",
    "        return overlap\n",
    "    \n",
    "    \n",
    "class CombinedKernel(gpytorch.kernels.Kernel):\n",
    "    \"\"\"Combined Matern kernel and Category Overlap Kernel\"\"\"\n",
    "\n",
    "    def __init__(self, matern_nu=2.5, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.matern_kernel = gpytorch.kernels.MaternKernel(nu=matern_nu)\n",
    "        self.category_kernel = CategoryOverlapKernel()\n",
    "\n",
    "    def forward(self, x1, x2, diag = False, **params):\n",
    "        #split cont and cat features\n",
    "        x1_cont, x1_cat = x1[..., :-1], x1[..., -1]\n",
    "        x2_cont, x2_cat = x2[..., :-1], x2[..., -1]\n",
    "\n",
    "        # computer kernel values\n",
    "        matern_val = self.matern_kernel(x1_cont, x2_cont, diag=diag, **params)\n",
    "        category_val = self.category_kernel(x1_cat, x2_cat, diag=diag, **params)\n",
    "\n",
    "        #combine kernels\n",
    "        return matern_val * category_val\n",
    "    \n",
    "class CustomGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__ (self, train_x, train_y, likelihood):\n",
    "        super().__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = CombinedKernel()\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just another ill attempt, needs fix and such"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\uqkmuroi\\AppData\\Local\\Temp\\ipykernel_31464\\4120062266.py:66: InputDataWarning: The model inputs are of type torch.float32. It is strongly recommended to use double precision in BoTorch, as this improves both precision and stability and can help avoid numerical errors. See https://github.com/pytorch/botorch/discussions/1444\n",
      "  model = SingleTaskGP(train_x, train_y.unsqueeze(-1), likelihood=likelihood)\n",
      "c:\\Users\\uqkmuroi\\gitcode\\bayesmediaopt\\venv\\Lib\\site-packages\\botorch\\models\\utils\\assorted.py:264: InputDataWarning: Data (input features) is not contained to the unit cube. Please consider min-max scaling the input data.\n",
      "  check_min_max_scaling(\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Expected X to be `batch_shape x q=1 x d`, but got X with shape torch.Size([20, 3, 3]).",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 117\u001b[39m\n\u001b[32m    115\u001b[39m \u001b[38;5;66;03m# Optimize acquisition function to get batch suggestions\u001b[39;00m\n\u001b[32m    116\u001b[39m bounds = torch.tensor([[\u001b[32m0.0\u001b[39m, \u001b[32m0.0\u001b[39m, \u001b[32m0\u001b[39m], [\u001b[32m1.0\u001b[39m, \u001b[32m5.0\u001b[39m, \u001b[32m2\u001b[39m]])  \u001b[38;5;66;03m# Adjust for problem\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m batch_candidates, acq_values = \u001b[43moptimize_acqf\u001b[49m\u001b[43m(\u001b[49m\u001b[43macq_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbounds\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbounds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_restarts\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraw_samples\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    119\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mSuggested batch points:\u001b[39m\u001b[33m\"\u001b[39m, batch_candidates)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\uqkmuroi\\gitcode\\bayesmediaopt\\venv\\Lib\\site-packages\\botorch\\optim\\optimize.py:631\u001b[39m, in \u001b[36moptimize_acqf\u001b[39m\u001b[34m(acq_function, bounds, q, num_restarts, raw_samples, options, inequality_constraints, equality_constraints, nonlinear_inequality_constraints, fixed_features, post_processing_func, batch_initial_conditions, return_best_only, gen_candidates, sequential, ic_generator, timeout_sec, return_full_tree, retry_on_optimization_warning, **ic_gen_kwargs)\u001b[39m\n\u001b[32m    608\u001b[39m     gen_candidates = gen_candidates_scipy\n\u001b[32m    609\u001b[39m opt_acqf_inputs = OptimizeAcqfInputs(\n\u001b[32m    610\u001b[39m     acq_function=acq_function,\n\u001b[32m    611\u001b[39m     bounds=bounds,\n\u001b[32m   (...)\u001b[39m\u001b[32m    629\u001b[39m     ic_gen_kwargs=ic_gen_kwargs,\n\u001b[32m    630\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m631\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_optimize_acqf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopt_inputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mopt_acqf_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\uqkmuroi\\gitcode\\bayesmediaopt\\venv\\Lib\\site-packages\\botorch\\optim\\optimize.py:652\u001b[39m, in \u001b[36m_optimize_acqf\u001b[39m\u001b[34m(opt_inputs)\u001b[39m\n\u001b[32m    649\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _optimize_acqf_sequential_q(opt_inputs=opt_inputs)\n\u001b[32m    651\u001b[39m \u001b[38;5;66;03m# Batch optimization (including the case q=1)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m652\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_optimize_acqf_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopt_inputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mopt_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\uqkmuroi\\gitcode\\bayesmediaopt\\venv\\Lib\\site-packages\\botorch\\optim\\optimize.py:326\u001b[39m, in \u001b[36m_optimize_acqf_batch\u001b[39m\u001b[34m(opt_inputs)\u001b[39m\n\u001b[32m    321\u001b[39m     required_num_restarts -= provided_initial_conditions.shape[\u001b[32m0\u001b[39m]\n\u001b[32m    323\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m opt_inputs.raw_samples \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m required_num_restarts > \u001b[32m0\u001b[39m:\n\u001b[32m    324\u001b[39m     \u001b[38;5;66;03m# pyre-ignore[28]: Unexpected keyword argument `acq_function`\u001b[39;00m\n\u001b[32m    325\u001b[39m     \u001b[38;5;66;03m# to anonymous call.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m     generated_initial_conditions = \u001b[43mopt_inputs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_ic_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    327\u001b[39m \u001b[43m        \u001b[49m\u001b[43macq_function\u001b[49m\u001b[43m=\u001b[49m\u001b[43mopt_inputs\u001b[49m\u001b[43m.\u001b[49m\u001b[43macq_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    328\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbounds\u001b[49m\u001b[43m=\u001b[49m\u001b[43mopt_inputs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    329\u001b[39m \u001b[43m        \u001b[49m\u001b[43mq\u001b[49m\u001b[43m=\u001b[49m\u001b[43mopt_inputs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    330\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_restarts\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequired_num_restarts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    331\u001b[39m \u001b[43m        \u001b[49m\u001b[43mraw_samples\u001b[49m\u001b[43m=\u001b[49m\u001b[43mopt_inputs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraw_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    332\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfixed_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mopt_inputs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfixed_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    333\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    334\u001b[39m \u001b[43m        \u001b[49m\u001b[43minequality_constraints\u001b[49m\u001b[43m=\u001b[49m\u001b[43mopt_inputs\u001b[49m\u001b[43m.\u001b[49m\u001b[43minequality_constraints\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    335\u001b[39m \u001b[43m        \u001b[49m\u001b[43mequality_constraints\u001b[49m\u001b[43m=\u001b[49m\u001b[43mopt_inputs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mequality_constraints\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    336\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mopt_inputs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mic_gen_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    337\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    339\u001b[39m batch_initial_conditions = _combine_initial_conditions(\n\u001b[32m    340\u001b[39m     provided_initial_conditions=provided_initial_conditions,\n\u001b[32m    341\u001b[39m     generated_initial_conditions=generated_initial_conditions,\n\u001b[32m    342\u001b[39m )\n\u001b[32m    344\u001b[39m batch_limit: \u001b[38;5;28mint\u001b[39m = options.get(\n\u001b[32m    345\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mbatch_limit\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    346\u001b[39m     (\n\u001b[32m   (...)\u001b[39m\u001b[32m    350\u001b[39m     ),\n\u001b[32m    351\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\uqkmuroi\\gitcode\\bayesmediaopt\\venv\\Lib\\site-packages\\botorch\\optim\\initializers.py:441\u001b[39m, in \u001b[36mgen_batch_initial_conditions\u001b[39m\u001b[34m(acq_function, bounds, q, num_restarts, raw_samples, fixed_features, options, inequality_constraints, equality_constraints, generator, fixed_X_fantasies)\u001b[39m\n\u001b[32m    436\u001b[39m         batch_limit = X_rnd.shape[\u001b[32m0\u001b[39m]\n\u001b[32m    437\u001b[39m     \u001b[38;5;66;03m# Evaluate the acquisition function on `X_rnd` using `batch_limit`\u001b[39;00m\n\u001b[32m    438\u001b[39m     \u001b[38;5;66;03m# sized chunks.\u001b[39;00m\n\u001b[32m    439\u001b[39m     acq_vals = torch.cat(\n\u001b[32m    440\u001b[39m         [\n\u001b[32m--> \u001b[39m\u001b[32m441\u001b[39m             \u001b[43macq_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m.cpu()\n\u001b[32m    442\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m x_ \u001b[38;5;129;01min\u001b[39;00m X_rnd.split(split_size=batch_limit, dim=\u001b[32m0\u001b[39m)\n\u001b[32m    443\u001b[39m         ],\n\u001b[32m    444\u001b[39m         dim=\u001b[32m0\u001b[39m,\n\u001b[32m    445\u001b[39m     )\n\u001b[32m    447\u001b[39m \u001b[38;5;66;03m# Downselect the initial conditions based on the acquisition function values\u001b[39;00m\n\u001b[32m    448\u001b[39m batch_initial_conditions, _ = init_func(\n\u001b[32m    449\u001b[39m     X=X_rnd, acq_vals=acq_vals, n=num_restarts, **init_kwargs\n\u001b[32m    450\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\uqkmuroi\\gitcode\\bayesmediaopt\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\uqkmuroi\\gitcode\\bayesmediaopt\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\uqkmuroi\\gitcode\\bayesmediaopt\\venv\\Lib\\site-packages\\botorch\\utils\\transforms.py:292\u001b[39m, in \u001b[36mt_batch_mode_transform.<locals>.decorator.<locals>.decorated\u001b[39m\u001b[34m(acqf, X, *args, **kwargs)\u001b[39m\n\u001b[32m    287\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    288\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(acqf).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m requires X to have at least 2 dimensions,\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    289\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m but received X with only \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX.dim()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m dimensions.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    290\u001b[39m     )\n\u001b[32m    291\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m expected_q \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m X.shape[-\u001b[32m2\u001b[39m] != expected_q:\n\u001b[32m--> \u001b[39m\u001b[32m292\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[32m    293\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExpected X to be `batch_shape x q=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected_q\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m x d`, but\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    294\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m got X with shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    295\u001b[39m     )\n\u001b[32m    296\u001b[39m \u001b[38;5;66;03m# add t-batch dim\u001b[39;00m\n\u001b[32m    297\u001b[39m X = X \u001b[38;5;28;01mif\u001b[39;00m X.dim() > \u001b[32m2\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m X.unsqueeze(\u001b[32m0\u001b[39m)\n",
      "\u001b[31mAssertionError\u001b[39m: Expected X to be `batch_shape x q=1 x d`, but got X with shape torch.Size([20, 3, 3])."
     ]
    }
   ],
   "source": [
    "\"\"\"Hybrid kernel\"\"\"\n",
    "\n",
    "import torch\n",
    "#from botorch.models.kernels import MaternKernel, ScaleKernel\n",
    "from gpytorch.kernels import MaternKernel, ScaleKernel, Kernel\n",
    "from gpytorch.constraints import Interval\n",
    "\n",
    "class CategoricalKernel(Kernel):\n",
    "    \"\"\"Kernel for categorical variables using a similarity matrix.\"\"\"\n",
    "    \n",
    "    def __init__(self, similarity_matrix, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.similarity_matrix = similarity_matrix\n",
    "\n",
    "    def forward(self, x1, x2, diag=False, **params):\n",
    "        x1, x2 = x1.long().squeeze(), x2.long().squeeze()\n",
    "        return self.similarity_matrix[x1, x2]\n",
    "\n",
    "# Define similarity matrix for categorical variables (example: 3 categories)\n",
    "similarity_matrix = torch.tensor([\n",
    "    [1.0, 0.5, 0.2],\n",
    "    [0.5, 1.0, 0.3],\n",
    "    [0.2, 0.3, 1.0],\n",
    "])\n",
    "\n",
    "# Instantiate kernels\n",
    "cat_kernel = CategoricalKernel(similarity_matrix)\n",
    "cont_kernel = ScaleKernel(MaternKernel(nu=2.5))\n",
    "\n",
    "class HybridKernel(Kernel):\n",
    "    \"\"\"Kernel combining continuous and categorical variables.\"\"\"\n",
    "\n",
    "    def __init__(self, cont_kernel, cat_kernel, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.cont_kernel = cont_kernel\n",
    "        self.cat_kernel = cat_kernel\n",
    "        self.lambda_param = torch.nn.Parameter(torch.tensor(0.5), requires_grad=True)\n",
    "        self.register_constraint(\"lambda_param\", Interval(0.0, 1.0))\n",
    "\n",
    "    def forward(self, x1, x2, diag=False, **params):\n",
    "        x1_cont, x1_cat = x1[:, :-1], x1[:, -1]\n",
    "        x2_cont, x2_cat = x2[:, :-1], x2[:, -1]\n",
    "        k_x = self.cont_kernel(x1_cont, x2_cont, diag=diag, **params)\n",
    "        k_h = self.cat_kernel(x1_cat, x2_cat, diag=diag, **params)\n",
    "        return self.lambda_param * k_x + (1 - self.lambda_param) * k_h\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\" GP Model for Batch CoCaBO \"\"\"\n",
    "\n",
    "from botorch.models import SingleTaskGP\n",
    "from gpytorch.likelihoods import GaussianLikelihood\n",
    "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
    "\n",
    "# Example training data: 2 continuous + 1 categorical (last column)\n",
    "train_x = torch.tensor([\n",
    "    [0.1, 1.2, 0],\n",
    "    [0.5, 2.3, 1],\n",
    "    [0.9, 3.5, 2],\n",
    "])\n",
    "train_y = torch.tensor([0.2, 0.8, 1.5])\n",
    "\n",
    "# Define Gaussian Process model\n",
    "likelihood = GaussianLikelihood()\n",
    "model = SingleTaskGP(train_x, train_y.unsqueeze(-1), likelihood=likelihood)\n",
    "\n",
    "# Assign Hybrid Kernel\n",
    "model.covar_module.base_kernel = HybridKernel(cont_kernel, cat_kernel)\n",
    "\n",
    "# Train model\n",
    "mll = ExactMarginalLogLikelihood(model.likelihood, model)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\" MAB for categorical selection \"\"\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class ThompsonSamplingMAB:\n",
    "    \"\"\"Multi-Armed Bandit using Thompson Sampling.\"\"\"\n",
    "    \n",
    "    def __init__(self, n_arms):\n",
    "        self.n_arms = n_arms\n",
    "        self.alpha = np.ones(n_arms)\n",
    "        self.beta = np.ones(n_arms)\n",
    "\n",
    "    def select_arm(self):\n",
    "        \"\"\"Selects an arm based on Thompson Sampling.\"\"\"\n",
    "        sampled_values = np.random.beta(self.alpha, self.beta)\n",
    "        return np.argmax(sampled_values)\n",
    "\n",
    "    def update(self, arm, reward):\n",
    "        \"\"\"Updates posterior distributions based on observed reward.\"\"\"\n",
    "        self.alpha[arm] += reward\n",
    "        self.beta[arm] += (1 - reward)\n",
    "\n",
    "# Example: 3 categorical choices\n",
    "mab = ThompsonSamplingMAB(n_arms=3)\n",
    "\n",
    "# Select best categorical option\n",
    "selected_arm = mab.select_arm()\n",
    "\n",
    "\n",
    "\n",
    "\"\"\" Bayesian Optimization with Batch Selection \"\"\"\n",
    "\n",
    "from botorch.optim.optimize import optimize_acqf\n",
    "from botorch.acquisition import UpperConfidenceBound\n",
    "\n",
    "# Define UCB acquisition function\n",
    "acq_func = UpperConfidenceBound(model, beta=2.0)\n",
    "\n",
    "# Optimize acquisition function to get batch suggestions\n",
    "bounds = torch.tensor([[0.0, 0.0, 0], [1.0, 5.0, 2]])  # Adjust for problem\n",
    "batch_candidates, acq_values = optimize_acqf(acq_func, bounds=bounds, q=3, num_restarts=5, raw_samples=20)\n",
    "\n",
    "print(\"Suggested batch points:\", batch_candidates)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a boiler plate for a sci-kit learn implementation, technically I should do both for good practice, but ultimately the team will probably want to develop in GPyTorch for consistency, longevity, and for future development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best continuous: [0.6], Best categorical: [1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\n",
    "\n",
    "class SimpleCoCaBO:\n",
    "    def __init__(self, continuous_dim, categorical_dim, kernel=None, noise_var=1e-5):\n",
    "        # Initialize the model with dimensions of continuous and categorical variables\n",
    "        self.continuous_dim = continuous_dim\n",
    "        self.categorical_dim = categorical_dim\n",
    "        \n",
    "        # Define the kernel for the Gaussian Process (RBF + constant kernel)\n",
    "        self.kernel = kernel if kernel else C(1.0, (1e-4, 1e1)) * RBF(1.0, (1e-4, 1e1))\n",
    "        \n",
    "        # Initialize the Gaussian Process Regressor\n",
    "        self.gpr = GaussianProcessRegressor(kernel=self.kernel, alpha=noise_var)\n",
    "\n",
    "        # Storage for past data\n",
    "        self.X = []  # Stores continuous + categorical variables\n",
    "        self.y = []  # Stores corresponding objective function values\n",
    "\n",
    "    def fit(self, X_cont, X_cat, y):\n",
    "        \"\"\"Fit the Gaussian Process model on both continuous and categorical data.\"\"\"\n",
    "        # Combine continuous and categorical data\n",
    "        X_combined = np.hstack((X_cont, X_cat))\n",
    "        \n",
    "        # Fit the Gaussian Process Regressor model\n",
    "        self.gpr.fit(X_combined, y)\n",
    "        \n",
    "        # Store the data for future optimization\n",
    "        self.X.extend(X_combined)\n",
    "        self.y.extend(y)\n",
    "\n",
    "    def predict(self, X_cont, X_cat):\n",
    "        \"\"\"Predict mean and variance for new points.\"\"\"\n",
    "        X_combined = np.hstack((X_cont, X_cat))\n",
    "        mean, std = self.gpr.predict(X_combined, return_std=True)\n",
    "        return mean, std\n",
    "\n",
    "    def ucb(self, X_cont, X_cat, kappa=2.0):\n",
    "        \"\"\"Upper Confidence Bound (UCB) acquisition function.\"\"\"\n",
    "        mean, std = self.predict(X_cont, X_cat)\n",
    "        ucb_values = mean + kappa * std\n",
    "        return ucb_values\n",
    "\n",
    "    def optimize(self, X_cont, X_cat, kappa=2.0):\n",
    "        \"\"\"Optimize the acquisition function (UCB).\"\"\"\n",
    "        ucb_values = self.ucb(X_cont, X_cat, kappa)\n",
    "        best_idx = np.argmax(ucb_values)  # Select the index with the highest UCB value\n",
    "        return X_cont[best_idx], X_cat[best_idx]\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Example continuous and categorical variables\n",
    "    X_cont = np.array([[0.5], [0.2], [0.7]])  # Example continuous variables\n",
    "    X_cat = np.array([[0], [1], [0]])  # Example categorical variables (just encoded as 0 or 1)\n",
    "    y = np.array([0.3, 0.7, 0.5])  # Objective values\n",
    "\n",
    "    # Instantiate the SimpleCoCaBO object\n",
    "    optimizer = SimpleCoCaBO(continuous_dim=1, categorical_dim=1)\n",
    "\n",
    "    # Fit the model to the data\n",
    "    optimizer.fit(X_cont, X_cat, y)\n",
    "\n",
    "    # Predict UCB values for new points\n",
    "    new_cont = np.array([[0.6], [0.3]])  # New continuous points to evaluate\n",
    "    new_cat = np.array([[1], [0]])  # New categorical points\n",
    "\n",
    "    best_cont, best_cat = optimizer.optimize(new_cont, new_cat)\n",
    "    print(f\"Best continuous: {best_cont}, Best categorical: {best_cat}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
