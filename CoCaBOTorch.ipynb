{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Continuous Bounds: [[0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]]\n",
      "Labeled Input Space: ['h1', 'h2', 'x1', 'x2', 'x3', 'x4']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def create_input_space(categorical_data, continuous_data):\n",
    "    \"\"\"\n",
    "    Creates a simple structured input space.\n",
    "    \n",
    "    Parameters:\n",
    "    - categorical_data (dict): Keys are categorical variable names, values are lists of possible categories.\n",
    "    - continuous_data (dict): Keys are variable names, values are (min, max) tuples.\n",
    "    \n",
    "    Returns:\n",
    "    - bounds (np.array): Bounds for continuous variables.\n",
    "    - labeled_input_space (list): List of input labels in the format h1, h2, x1, x2...\n",
    "    \"\"\"\n",
    "    labeled_input_space = []\n",
    "    continuous_count = 1\n",
    "    bounds = []\n",
    "    \n",
    "    # Add labels for categorical variables (just integers for simplicity)\n",
    "    categorical_count = 1\n",
    "    for cat_var, categories in categorical_data.items():\n",
    "        labeled_input_space.append(f\"h{categorical_count}\")\n",
    "        categorical_count += 1\n",
    "    \n",
    "    # Add labels for continuous variables and bounds\n",
    "    for cont_var, (min_val, max_val) in continuous_data.items():\n",
    "        bounds.append([min_val, max_val])\n",
    "        labeled_input_space.append(f\"x{continuous_count}\")\n",
    "        continuous_count += 1\n",
    "    \n",
    "  \n",
    "    return np.array(bounds), labeled_input_space\n",
    "\n",
    "# Example case\n",
    "categorical_data = {\n",
    "    \"Category1\": [0, 1],  # 2 choices\n",
    "    \"Category2\": [0, 1, 2]  # 3 choices\n",
    "}\n",
    "continuous_data = {\n",
    "    \"Var1\": (0, 1),\n",
    "    \"Var2\": (0, 1),\n",
    "    \"Var3\": (0, 1),\n",
    "    \"Var4\": (0, 1)\n",
    "}\n",
    "\n",
    "bounds, labeled_input_space = create_input_space(categorical_data, continuous_data)\n",
    "print(\"Continuous Bounds:\", bounds)\n",
    "print(\"Labeled Input Space:\", labeled_input_space)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Categorical Arm: 2\n",
      "Suggested Continuous Candidate: tensor([[0.2908, 0.6281, 0.9655, 1.0000]])\n",
      "Final input zt: tensor([[2.0000, 0.2908, 0.6281, 0.9655, 1.0000]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from botorch.models import SingleTaskGP\n",
    "from gpytorch.likelihoods import GaussianLikelihood\n",
    "from botorch.acquisition import UpperConfidenceBound\n",
    "from botorch.optim import optimize_acqf\n",
    "\n",
    "# Define the EXP3 algorithm for categorical optimization\n",
    "class EXP3:\n",
    "    def __init__(self, n_arms, gamma=0.1):\n",
    "        self.n_arms = n_arms\n",
    "        self.gamma = gamma\n",
    "        self.weights = np.ones(n_arms)  # Initialize the weights\n",
    "        self.total_reward = 0\n",
    "\n",
    "    def select_arm(self):\n",
    "        # Compute the probability distribution over arms\n",
    "        prob_dist = (1 - self.gamma) * (self.weights / np.sum(self.weights)) + (self.gamma / self.n_arms)\n",
    "        # Select an arm based on the probability distribution\n",
    "        return np.random.choice(self.n_arms, p=prob_dist)\n",
    "\n",
    "    def update(self, arm, reward):\n",
    "        # Update the weights based on the observed reward\n",
    "        self.total_reward += reward\n",
    "        self.weights[arm] *= np.exp(self.gamma * reward / (self.n_arms * np.sum(self.weights)))\n",
    "\n",
    "# Example: Define categorical and continuous input spaces\n",
    "categorical_data = {\n",
    "    \"Category1\": [0, 1],  # 2 choices\n",
    "    \"Category2\": [0, 1, 2]  # 3 choices\n",
    "}\n",
    "continuous_data = {\n",
    "    \"Var1\": (0, 1),\n",
    "    \"Var2\": (0, 1),\n",
    "    \"Var3\": (0, 1),\n",
    "    \"Var4\": (0, 1)\n",
    "}\n",
    "\n",
    "# Set up the GP model for continuous variables\n",
    "def create_gp_model(train_x, train_y):\n",
    "    likelihood = GaussianLikelihood()\n",
    "    model = SingleTaskGP(train_x, train_y.unsqueeze(-1), likelihood=likelihood)\n",
    "    return model\n",
    "\n",
    "# Define the acquisition function\n",
    "def optimize_continuous(model, bounds):\n",
    "    acq_func = UpperConfidenceBound(model, beta=2.0)\n",
    "    candidates, _ = optimize_acqf(acq_func, bounds=bounds, q=1, num_restarts=5, raw_samples=20)\n",
    "    return candidates\n",
    "\n",
    "# Initialize the EXP3 model for categorical optimization\n",
    "n_arms = 3  # Example: 3 categorical choices\n",
    "exp3 = EXP3(n_arms)\n",
    "\n",
    "# Example: Initial continuous points for training the GP\n",
    "train_x = torch.tensor([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8], [0.9, 0.8, 0.7, 0.6]], dtype=torch.float64)\n",
    "train_y = torch.tensor([0.5, 0.8, 0.6], dtype=torch.float64)  # Example target values\n",
    "\n",
    "# Train the GP model\n",
    "model = create_gp_model(train_x, train_y)\n",
    "\n",
    "# Get the bounds for the continuous variables\n",
    "bounds = torch.tensor([[0.0, 0.0, 0.0, 0.0], [1.0, 1.0, 1.0, 1.0]])\n",
    "\n",
    "# Simulate the optimization process\n",
    "# Select a categorical arm (e.g., choose a category for Category1 and Category2)\n",
    "selected_arm = exp3.select_arm()\n",
    "print(f\"Selected Categorical Arm: {selected_arm}\")\n",
    "\n",
    "# Optimize the continuous space using the GP model\n",
    "continuous_candidate = optimize_continuous(model, bounds)\n",
    "print(f\"Suggested Continuous Candidate: {continuous_candidate}\")\n",
    "\n",
    "# Combine the results: The final input zt = [ht, xt]\n",
    "\n",
    "# First reshape the selected_arm to match the continuous candidate shape\n",
    "selected_arm_tensor = torch.tensor([selected_arm]).unsqueeze(0) #Shape [1,1] \n",
    "continuous_candidate_tensor = continuous_candidate # Shape [1,4]\n",
    "\n",
    "# Here we assume `ht` is the selected categorical arm, and `xt` is the optimized continuous candidate\n",
    "zt = torch.cat((selected_arm_tensor, continuous_candidate_tensor), dim=-1)  #BECAREFUL HERE AS THE INTEGER CATEGORIES WILL CONVERT TO FLOATS\n",
    "print(f\"Final input zt: {zt}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from gpytorch.kernels import Kernel, MaternKernel, ScaleKernel\n",
    "from gpytorch.constraints import Interval\n",
    "\n",
    "# Define a categorical kernel using a similarity matrix\n",
    "class CategoricalKernel(Kernel):\n",
    "    def __init__(self, similarity_matrix, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.similarity_matrix = similarity_matrix\n",
    "\n",
    "    def forward(self, x1, x2, diag=False, **params):\n",
    "        x1, x2 = x1.long().squeeze(), x2.long().squeeze()\n",
    "        return self.similarity_matrix[x1, x2]\n",
    "\n",
    "# Define the hybrid kernel (continuous + categorical)\n",
    "class HybridKernel(Kernel):\n",
    "    def __init__(self, cont_kernel, cat_kernel, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.cont_kernel = cont_kernel\n",
    "        self.cat_kernel = cat_kernel\n",
    "        self.lambda_param = torch.nn.Parameter(torch.tensor(0.5, dtype=torch.float64), requires_grad=True)\n",
    "        self.register_constraint(\"lambda_param\", Interval(0.0, 1.0))\n",
    "\n",
    "    def forward(self, x1, x2, diag=False, **params):\n",
    "        x1_cat, x1_cont = x1[:, 0], x1[:, 1:]\n",
    "        x2_cat, x2_cont = x2[:, 0], x2[:, 1:]\n",
    "        \n",
    "        k_cat = self.cat_kernel(x1_cat, x2_cat, diag=diag, **params)\n",
    "        k_cont = self.cont_kernel(x1_cont, x2_cont, diag=diag, **params)\n",
    "        \n",
    "        return self.lambda_param * k_cont + (1 - self.lambda_param) * k_cat\n",
    "\n",
    "# Define a similarity matrix for categorical variables (example with 3 categories)\n",
    "similarity_matrix = torch.tensor([\n",
    "    [1.0, 0.5, 0.2],\n",
    "    [0.5, 1.0, 0.3],\n",
    "    [0.2, 0.3, 1.0],\n",
    "], dtype=torch.float64)\n",
    "\n",
    "# Instantiate the kernels\n",
    "cat_kernel = CategoricalKernel(similarity_matrix)\n",
    "cont_kernel = ScaleKernel(MaternKernel(nu=2.5))\n",
    "\n",
    "# Create the hybrid kernel\n",
    "hybrid_kernel = HybridKernel(cont_kernel, cat_kernel)\n",
    "\n",
    "# Apply hybrid kernel to GP model\n",
    "model.covar_module = hybrid_kernel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# below is some chatgpt shit. Dont take it for granted!!!@!!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from botorch.models import SingleTaskGP\n",
    "from gpytorch.likelihoods import GaussianLikelihood\n",
    "from botorch.acquisition import UpperConfidenceBound\n",
    "from botorch.optim import optimize_acqf\n",
    "\n",
    "# Simulated objective function (for testing optimization)\n",
    "def objective_function(h1, h2, x1, x2, x3, x4):\n",
    "    return (h1 + h2) * torch.sin(3 * x1) + 0.5 * torch.cos(2 * x2) + x3**2 - 0.3 * x4\n",
    "\n",
    "# Categorical (integer encoded) and continuous variables\n",
    "categorical_choices = torch.tensor([\n",
    "    [0, 0], [0, 1], [0, 2],  # Category1 (h1) & Category2 (h2)\n",
    "    [1, 0], [1, 1], [1, 2]\n",
    "], dtype=torch.float64)\n",
    "\n",
    "continuous_bounds = torch.tensor([[0, 0, 0, 0], [1, 1, 1, 1]], dtype=torch.float64)\n",
    "\n",
    "# Generate initial training data (random sampling)\n",
    "n_initial = 6\n",
    "train_x_cat = categorical_choices[torch.randint(0, len(categorical_choices), (n_initial,))]\n",
    "train_x_cont = torch.rand((n_initial, 4), dtype=torch.float64)\n",
    "train_x = torch.cat([train_x_cat, train_x_cont], dim=1)\n",
    "\n",
    "train_y = torch.tensor([\n",
    "    objective_function(*point) for point in train_x\n",
    "], dtype=torch.float64).unsqueeze(-1)  # Ensure shape is (N,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\uqkmuroi\\gitcode\\bayesmediaopt\\venv\\Lib\\site-packages\\botorch\\models\\utils\\assorted.py:264: InputDataWarning: Data (input features) is not contained to the unit cube. Please consider min-max scaling the input data.\n",
      "  check_min_max_scaling(\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'gpytorch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m model.covar_module = hybrid_kernel  \u001b[38;5;66;03m# Use hybrid kernel\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Training loop (simple)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m mll = \u001b[43mgpytorch\u001b[49m.mlls.ExactMarginalLogLikelihood(likelihood, model)\n\u001b[32m      8\u001b[39m optimizer = torch.optim.Adam(model.parameters(), lr=\u001b[32m0.1\u001b[39m)\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m50\u001b[39m):  \u001b[38;5;66;03m# Train for 50 iterations\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'gpytorch' is not defined"
     ]
    }
   ],
   "source": [
    "# Create and train the GP model\n",
    "likelihood = GaussianLikelihood()\n",
    "model = SingleTaskGP(train_x, train_y, likelihood=likelihood)\n",
    "model.covar_module = hybrid_kernel  # Use hybrid kernel\n",
    "\n",
    "# Training loop (simple)\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "\n",
    "for i in range(50):  # Train for 50 iterations\n",
    "    optimizer.zero_grad()\n",
    "    output = model(train_x)\n",
    "    loss = -mll(output, train_y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(\"Model Training Completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 1. Expected size 6 but got size 4 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Optimize continuous variables using BO\u001b[39;00m\n\u001b[32m     13\u001b[39m acq_func = UpperConfidenceBound(model, beta=\u001b[32m2.0\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m candidates, _ = \u001b[43moptimize_acqf\u001b[49m\u001b[43m(\u001b[49m\u001b[43macq_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbounds\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontinuous_bounds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_restarts\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraw_samples\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Form final candidate [h1, h2, x1, x2, x3, x4]\u001b[39;00m\n\u001b[32m     17\u001b[39m zt = torch.cat((selected_cat, candidates), dim=-\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\uqkmuroi\\gitcode\\bayesmediaopt\\venv\\Lib\\site-packages\\botorch\\optim\\optimize.py:631\u001b[39m, in \u001b[36moptimize_acqf\u001b[39m\u001b[34m(acq_function, bounds, q, num_restarts, raw_samples, options, inequality_constraints, equality_constraints, nonlinear_inequality_constraints, fixed_features, post_processing_func, batch_initial_conditions, return_best_only, gen_candidates, sequential, ic_generator, timeout_sec, return_full_tree, retry_on_optimization_warning, **ic_gen_kwargs)\u001b[39m\n\u001b[32m    608\u001b[39m     gen_candidates = gen_candidates_scipy\n\u001b[32m    609\u001b[39m opt_acqf_inputs = OptimizeAcqfInputs(\n\u001b[32m    610\u001b[39m     acq_function=acq_function,\n\u001b[32m    611\u001b[39m     bounds=bounds,\n\u001b[32m   (...)\u001b[39m\u001b[32m    629\u001b[39m     ic_gen_kwargs=ic_gen_kwargs,\n\u001b[32m    630\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m631\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_optimize_acqf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopt_inputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mopt_acqf_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\uqkmuroi\\gitcode\\bayesmediaopt\\venv\\Lib\\site-packages\\botorch\\optim\\optimize.py:652\u001b[39m, in \u001b[36m_optimize_acqf\u001b[39m\u001b[34m(opt_inputs)\u001b[39m\n\u001b[32m    649\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _optimize_acqf_sequential_q(opt_inputs=opt_inputs)\n\u001b[32m    651\u001b[39m \u001b[38;5;66;03m# Batch optimization (including the case q=1)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m652\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_optimize_acqf_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopt_inputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mopt_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\uqkmuroi\\gitcode\\bayesmediaopt\\venv\\Lib\\site-packages\\botorch\\optim\\optimize.py:326\u001b[39m, in \u001b[36m_optimize_acqf_batch\u001b[39m\u001b[34m(opt_inputs)\u001b[39m\n\u001b[32m    321\u001b[39m     required_num_restarts -= provided_initial_conditions.shape[\u001b[32m0\u001b[39m]\n\u001b[32m    323\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m opt_inputs.raw_samples \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m required_num_restarts > \u001b[32m0\u001b[39m:\n\u001b[32m    324\u001b[39m     \u001b[38;5;66;03m# pyre-ignore[28]: Unexpected keyword argument `acq_function`\u001b[39;00m\n\u001b[32m    325\u001b[39m     \u001b[38;5;66;03m# to anonymous call.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m     generated_initial_conditions = \u001b[43mopt_inputs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_ic_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    327\u001b[39m \u001b[43m        \u001b[49m\u001b[43macq_function\u001b[49m\u001b[43m=\u001b[49m\u001b[43mopt_inputs\u001b[49m\u001b[43m.\u001b[49m\u001b[43macq_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    328\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbounds\u001b[49m\u001b[43m=\u001b[49m\u001b[43mopt_inputs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    329\u001b[39m \u001b[43m        \u001b[49m\u001b[43mq\u001b[49m\u001b[43m=\u001b[49m\u001b[43mopt_inputs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    330\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_restarts\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequired_num_restarts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    331\u001b[39m \u001b[43m        \u001b[49m\u001b[43mraw_samples\u001b[49m\u001b[43m=\u001b[49m\u001b[43mopt_inputs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraw_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    332\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfixed_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mopt_inputs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfixed_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    333\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    334\u001b[39m \u001b[43m        \u001b[49m\u001b[43minequality_constraints\u001b[49m\u001b[43m=\u001b[49m\u001b[43mopt_inputs\u001b[49m\u001b[43m.\u001b[49m\u001b[43minequality_constraints\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    335\u001b[39m \u001b[43m        \u001b[49m\u001b[43mequality_constraints\u001b[49m\u001b[43m=\u001b[49m\u001b[43mopt_inputs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mequality_constraints\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    336\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mopt_inputs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mic_gen_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    337\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    339\u001b[39m batch_initial_conditions = _combine_initial_conditions(\n\u001b[32m    340\u001b[39m     provided_initial_conditions=provided_initial_conditions,\n\u001b[32m    341\u001b[39m     generated_initial_conditions=generated_initial_conditions,\n\u001b[32m    342\u001b[39m )\n\u001b[32m    344\u001b[39m batch_limit: \u001b[38;5;28mint\u001b[39m = options.get(\n\u001b[32m    345\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mbatch_limit\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    346\u001b[39m     (\n\u001b[32m   (...)\u001b[39m\u001b[32m    350\u001b[39m     ),\n\u001b[32m    351\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\uqkmuroi\\gitcode\\bayesmediaopt\\venv\\Lib\\site-packages\\botorch\\optim\\initializers.py:441\u001b[39m, in \u001b[36mgen_batch_initial_conditions\u001b[39m\u001b[34m(acq_function, bounds, q, num_restarts, raw_samples, fixed_features, options, inequality_constraints, equality_constraints, generator, fixed_X_fantasies)\u001b[39m\n\u001b[32m    436\u001b[39m         batch_limit = X_rnd.shape[\u001b[32m0\u001b[39m]\n\u001b[32m    437\u001b[39m     \u001b[38;5;66;03m# Evaluate the acquisition function on `X_rnd` using `batch_limit`\u001b[39;00m\n\u001b[32m    438\u001b[39m     \u001b[38;5;66;03m# sized chunks.\u001b[39;00m\n\u001b[32m    439\u001b[39m     acq_vals = torch.cat(\n\u001b[32m    440\u001b[39m         [\n\u001b[32m--> \u001b[39m\u001b[32m441\u001b[39m             \u001b[43macq_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m.cpu()\n\u001b[32m    442\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m x_ \u001b[38;5;129;01min\u001b[39;00m X_rnd.split(split_size=batch_limit, dim=\u001b[32m0\u001b[39m)\n\u001b[32m    443\u001b[39m         ],\n\u001b[32m    444\u001b[39m         dim=\u001b[32m0\u001b[39m,\n\u001b[32m    445\u001b[39m     )\n\u001b[32m    447\u001b[39m \u001b[38;5;66;03m# Downselect the initial conditions based on the acquisition function values\u001b[39;00m\n\u001b[32m    448\u001b[39m batch_initial_conditions, _ = init_func(\n\u001b[32m    449\u001b[39m     X=X_rnd, acq_vals=acq_vals, n=num_restarts, **init_kwargs\n\u001b[32m    450\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\uqkmuroi\\gitcode\\bayesmediaopt\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\uqkmuroi\\gitcode\\bayesmediaopt\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\uqkmuroi\\gitcode\\bayesmediaopt\\venv\\Lib\\site-packages\\botorch\\utils\\transforms.py:298\u001b[39m, in \u001b[36mt_batch_mode_transform.<locals>.decorator.<locals>.decorated\u001b[39m\u001b[34m(acqf, X, *args, **kwargs)\u001b[39m\n\u001b[32m    296\u001b[39m \u001b[38;5;66;03m# add t-batch dim\u001b[39;00m\n\u001b[32m    297\u001b[39m X = X \u001b[38;5;28;01mif\u001b[39;00m X.dim() > \u001b[32m2\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m X.unsqueeze(\u001b[32m0\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m298\u001b[39m output = \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[43macqf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    299\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(acqf, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m is_ensemble(acqf.model):\n\u001b[32m    300\u001b[39m     \u001b[38;5;66;03m# IDEA: this could be wrapped into SampleReducingMCAcquisitionFunction\u001b[39;00m\n\u001b[32m    301\u001b[39m     output = (\n\u001b[32m    302\u001b[39m         output.mean(dim=-\u001b[32m1\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m acqf._log \u001b[38;5;28;01melse\u001b[39;00m logmeanexp(output, dim=-\u001b[32m1\u001b[39m)\n\u001b[32m    303\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\uqkmuroi\\gitcode\\bayesmediaopt\\venv\\Lib\\site-packages\\botorch\\acquisition\\analytic.py:795\u001b[39m, in \u001b[36mUpperConfidenceBound.forward\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m    784\u001b[39m \u001b[38;5;129m@t_batch_mode_transform\u001b[39m(expected_q=\u001b[32m1\u001b[39m)\n\u001b[32m    785\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: Tensor) -> Tensor:\n\u001b[32m    786\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Evaluate the Upper Confidence Bound on the candidate set X.\u001b[39;00m\n\u001b[32m    787\u001b[39m \n\u001b[32m    788\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    793\u001b[39m \u001b[33;03m        given design points `X`.\u001b[39;00m\n\u001b[32m    794\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m795\u001b[39m     mean, sigma = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_mean_and_sigma\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    796\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (mean \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.maximize \u001b[38;5;28;01melse\u001b[39;00m -mean) + \u001b[38;5;28mself\u001b[39m.beta.sqrt() * sigma\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\uqkmuroi\\gitcode\\bayesmediaopt\\venv\\Lib\\site-packages\\botorch\\acquisition\\analytic.py:100\u001b[39m, in \u001b[36mAnalyticAcquisitionFunction._mean_and_sigma\u001b[39m\u001b[34m(self, X, compute_sigma, min_var)\u001b[39m\n\u001b[32m     86\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Computes the first and second moments of the model posterior.\u001b[39;00m\n\u001b[32m     87\u001b[39m \n\u001b[32m     88\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     97\u001b[39m \u001b[33;03m    returns a single tensor of means if compute_sigma is True.\u001b[39;00m\n\u001b[32m     98\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     99\u001b[39m \u001b[38;5;28mself\u001b[39m.to(device=X.device)  \u001b[38;5;66;03m# ensures buffers / parameters are on the same device\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m100\u001b[39m posterior = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mposterior\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    101\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposterior_transform\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mposterior_transform\u001b[49m\n\u001b[32m    102\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    103\u001b[39m mean = posterior.mean.squeeze(-\u001b[32m2\u001b[39m).squeeze(-\u001b[32m1\u001b[39m)  \u001b[38;5;66;03m# removing redundant dimensions\u001b[39;00m\n\u001b[32m    104\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m compute_sigma:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\uqkmuroi\\gitcode\\bayesmediaopt\\venv\\Lib\\site-packages\\botorch\\models\\gpytorch.py:448\u001b[39m, in \u001b[36mBatchedMultiOutputGPyTorchModel.posterior\u001b[39m\u001b[34m(self, X, output_indices, observation_noise, posterior_transform)\u001b[39m\n\u001b[32m    442\u001b[39m     X, output_dim_idx = add_output_dim(\n\u001b[32m    443\u001b[39m         X=X, original_batch_shape=\u001b[38;5;28mself\u001b[39m._input_batch_shape\n\u001b[32m    444\u001b[39m     )\n\u001b[32m    445\u001b[39m \u001b[38;5;66;03m# NOTE: BoTorch's GPyTorchModels also inherit from GPyTorch's ExactGP, thus\u001b[39;00m\n\u001b[32m    446\u001b[39m \u001b[38;5;66;03m# self(X) calls GPyTorch's ExactGP's __call__, which computes the posterior,\u001b[39;00m\n\u001b[32m    447\u001b[39m \u001b[38;5;66;03m# rather than e.g. SingleTaskGP's forward, which computes the prior.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m448\u001b[39m mvn = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    449\u001b[39m mvn = \u001b[38;5;28mself\u001b[39m._apply_noise(X=X, mvn=mvn, observation_noise=observation_noise)\n\u001b[32m    450\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_outputs > \u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\uqkmuroi\\gitcode\\bayesmediaopt\\venv\\Lib\\site-packages\\gpytorch\\models\\exact_gp.py:325\u001b[39m, in \u001b[36mExactGP.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    323\u001b[39m         train_input = train_input.expand(*batch_shape, *train_input.shape[-\u001b[32m2\u001b[39m:])\n\u001b[32m    324\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[38;5;28minput\u001b[39m.expand(*batch_shape, *\u001b[38;5;28minput\u001b[39m.shape[-\u001b[32m2\u001b[39m:])\n\u001b[32m--> \u001b[39m\u001b[32m325\u001b[39m     full_inputs.append(\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m=\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[32m    327\u001b[39m \u001b[38;5;66;03m# Get the joint distribution for training/test data\u001b[39;00m\n\u001b[32m    328\u001b[39m full_output = \u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m(*full_inputs, **kwargs)\n",
      "\u001b[31mRuntimeError\u001b[39m: Sizes of tensors must match except in dimension 1. Expected size 6 but got size 4 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "# EXP3 for categorical choices\n",
    "exp3 = EXP3(len(categorical_choices))\n",
    "\n",
    "# Bayesian Optimization Loop\n",
    "num_iterations = 10\n",
    "best_observed = []\n",
    "for i in range(num_iterations):\n",
    "    # Select categorical variables using EXP3\n",
    "    selected_cat_idx = exp3.select_arm()\n",
    "    selected_cat = categorical_choices[selected_cat_idx]\n",
    "\n",
    "    # Optimize continuous variables using BO\n",
    "    acq_func = UpperConfidenceBound(model, beta=2.0)\n",
    "    candidates, _ = optimize_acqf(acq_func, bounds=continuous_bounds, q=1, num_restarts=5, raw_samples=20)\n",
    "\n",
    "    # Form final candidate [h1, h2, x1, x2, x3, x4]\n",
    "    zt = torch.cat((selected_cat, candidates), dim=-1)\n",
    "\n",
    "    # Evaluate function\n",
    "    new_y = objective_function(*zt).unsqueeze(-1)\n",
    "\n",
    "    # Update dataset\n",
    "    train_x = torch.cat([train_x, zt.unsqueeze(0)], dim=0)\n",
    "    train_y = torch.cat([train_y, new_y.unsqueeze(0)], dim=0)\n",
    "\n",
    "    # Update GP model\n",
    "    model.set_train_data(train_x, train_y, strict=False)\n",
    "\n",
    "    # Store best result\n",
    "    best_observed.append(train_y.max().item())\n",
    "\n",
    "    print(f\"Iteration {i+1}: Selected {zt}, Function Value = {new_y.item()}\")\n",
    "\n",
    "# Plot best observed function values\n",
    "plt.plot(range(1, num_iterations+1), best_observed, marker='o')\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Best Observed Value\")\n",
    "plt.title(\"Bayesian Optimization Progress\")\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4️⃣ Expected Outputs\n",
    "Selected categorical variables (e.g., [0,1], [1,2], etc.)\n",
    "\n",
    "Optimized continuous values (e.g., [0.7, 0.5, 0.9, 0.3])\n",
    "\n",
    "Best observed function values plotted over iterations\n",
    "\n",
    "🎯 Summary\n",
    "✅ GP + Hybrid Kernel → Handles categorical & continuous spaces\n",
    "✅ EXP3 → Optimizes categorical variables\n",
    "✅ Bayesian Optimization → Optimizes continuous variables\n",
    "✅ Visual Output → Convergence graph showing BO progress\n",
    "\n",
    "Let me know if you need any refinements! 🚀"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
