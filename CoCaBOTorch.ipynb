{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Continuous Bounds: [[0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]]\n",
      "Labeled Input Space: ['h1', 'h2', 'x1', 'x2', 'x3', 'x4']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def create_input_space(categorical_data, continuous_data):\n",
    "    \"\"\"\n",
    "    Creates a simple structured input space.\n",
    "    \n",
    "    Parameters:\n",
    "    - categorical_data (dict): Keys are categorical variable names, values are lists of possible categories.\n",
    "    - continuous_data (dict): Keys are variable names, values are (min, max) tuples.\n",
    "    \n",
    "    Returns:\n",
    "    - bounds (np.array): Bounds for continuous variables.\n",
    "    - labeled_input_space (list): List of input labels in the format h1, h2, x1, x2...\n",
    "    \"\"\"\n",
    "    labeled_input_space = []\n",
    "    continuous_count = 1\n",
    "    bounds = []\n",
    "    \n",
    "    # Add labels for categorical variables (just integers for simplicity)\n",
    "    categorical_count = 1\n",
    "    for cat_var, categories in categorical_data.items():\n",
    "        labeled_input_space.append(f\"h{categorical_count}\")\n",
    "        categorical_count += 1\n",
    "    \n",
    "    # Add labels for continuous variables and bounds\n",
    "    for cont_var, (min_val, max_val) in continuous_data.items():\n",
    "        bounds.append([min_val, max_val])\n",
    "        labeled_input_space.append(f\"x{continuous_count}\")\n",
    "        continuous_count += 1\n",
    "    \n",
    "  \n",
    "    return np.array(bounds), labeled_input_space\n",
    "\n",
    "# Example case\n",
    "categorical_data = {\n",
    "    \"Category1\": [0, 1],  # 2 choices\n",
    "    \"Category2\": [0, 1, 2]  # 3 choices\n",
    "}\n",
    "continuous_data = {\n",
    "    \"Var1\": (0, 1),\n",
    "    \"Var2\": (0, 1),\n",
    "    \"Var3\": (0, 1),\n",
    "    \"Var4\": (0, 1)\n",
    "}\n",
    "\n",
    "bounds, labeled_input_space = create_input_space(categorical_data, continuous_data)\n",
    "print(\"Continuous Bounds:\", bounds)\n",
    "print(\"Labeled Input Space:\", labeled_input_space)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Categorical Arm: 2\n",
      "Suggested Continuous Candidate: tensor([[0.2908, 0.6281, 0.9655, 1.0000]])\n",
      "Final input zt: tensor([[2.0000, 0.2908, 0.6281, 0.9655, 1.0000]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from botorch.models import SingleTaskGP\n",
    "from gpytorch.likelihoods import GaussianLikelihood\n",
    "from botorch.acquisition import UpperConfidenceBound\n",
    "from botorch.optim import optimize_acqf\n",
    "\n",
    "# Define the EXP3 algorithm for categorical optimization\n",
    "class EXP3:\n",
    "    def __init__(self, n_arms, gamma=0.1):\n",
    "        self.n_arms = n_arms\n",
    "        self.gamma = gamma\n",
    "        self.weights = np.ones(n_arms)  # Initialize the weights\n",
    "        self.total_reward = 0\n",
    "\n",
    "    def select_arm(self):\n",
    "        # Compute the probability distribution over arms\n",
    "        prob_dist = (1 - self.gamma) * (self.weights / np.sum(self.weights)) + (self.gamma / self.n_arms)\n",
    "        # Select an arm based on the probability distribution\n",
    "        return np.random.choice(self.n_arms, p=prob_dist)\n",
    "\n",
    "    def update(self, arm, reward):\n",
    "        # Update the weights based on the observed reward\n",
    "        self.total_reward += reward\n",
    "        self.weights[arm] *= np.exp(self.gamma * reward / (self.n_arms * np.sum(self.weights)))\n",
    "\n",
    "# Example: Define categorical and continuous input spaces\n",
    "categorical_data = {\n",
    "    \"Category1\": [0, 1],  # 2 choices\n",
    "    \"Category2\": [0, 1, 2]  # 3 choices\n",
    "}\n",
    "continuous_data = {\n",
    "    \"Var1\": (0, 1),\n",
    "    \"Var2\": (0, 1),\n",
    "    \"Var3\": (0, 1),\n",
    "    \"Var4\": (0, 1)\n",
    "}\n",
    "\n",
    "# Set up the GP model for continuous variables\n",
    "def create_gp_model(train_x, train_y):\n",
    "    likelihood = GaussianLikelihood()\n",
    "    model = SingleTaskGP(train_x, train_y.unsqueeze(-1), likelihood=likelihood)\n",
    "    return model\n",
    "\n",
    "# Define the acquisition function\n",
    "def optimize_continuous(model, bounds):\n",
    "    acq_func = UpperConfidenceBound(model, beta=2.0)\n",
    "    candidates, _ = optimize_acqf(acq_func, bounds=bounds, q=1, num_restarts=5, raw_samples=20)\n",
    "    return candidates\n",
    "\n",
    "# Initialize the EXP3 model for categorical optimization\n",
    "n_arms = 3  # Example: 3 categorical choices\n",
    "exp3 = EXP3(n_arms)\n",
    "\n",
    "# Example: Initial continuous points for training the GP\n",
    "train_x = torch.tensor([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8], [0.9, 0.8, 0.7, 0.6]], dtype=torch.float64)\n",
    "train_y = torch.tensor([0.5, 0.8, 0.6], dtype=torch.float64)  # Example target values\n",
    "\n",
    "# Train the GP model\n",
    "model = create_gp_model(train_x, train_y)\n",
    "\n",
    "# Get the bounds for the continuous variables\n",
    "bounds = torch.tensor([[0.0, 0.0, 0.0, 0.0], [1.0, 1.0, 1.0, 1.0]])\n",
    "\n",
    "# Simulate the optimization process\n",
    "# Select a categorical arm (e.g., choose a category for Category1 and Category2)\n",
    "selected_arm = exp3.select_arm()\n",
    "print(f\"Selected Categorical Arm: {selected_arm}\")\n",
    "\n",
    "# Optimize the continuous space using the GP model\n",
    "continuous_candidate = optimize_continuous(model, bounds)\n",
    "print(f\"Suggested Continuous Candidate: {continuous_candidate}\")\n",
    "\n",
    "# Combine the results: The final input zt = [ht, xt]\n",
    "\n",
    "# First reshape the selected_arm to match the continuous candidate shape\n",
    "selected_arm_tensor = torch.tensor([selected_arm]).unsqueeze(0) #Shape [1,1] \n",
    "continuous_candidate_tensor = continuous_candidate # Shape [1,4]\n",
    "\n",
    "# Here we assume `ht` is the selected categorical arm, and `xt` is the optimized continuous candidate\n",
    "zt = torch.cat((selected_arm_tensor, continuous_candidate_tensor), dim=-1)  #BECAREFUL HERE AS THE INTEGER CATEGORIES WILL CONVERT TO FLOATS\n",
    "print(f\"Final input zt: {zt}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from gpytorch.kernels import Kernel, MaternKernel, ScaleKernel\n",
    "from gpytorch.constraints import Interval\n",
    "\n",
    "# Define a categorical kernel using a similarity matrix\n",
    "class CategoricalKernel(Kernel):\n",
    "    def __init__(self, similarity_matrix, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.similarity_matrix = similarity_matrix\n",
    "\n",
    "    def forward(self, x1, x2, diag=False, **params):\n",
    "        x1, x2 = x1.long().squeeze(), x2.long().squeeze()\n",
    "        return self.similarity_matrix[x1, x2]\n",
    "\n",
    "# Define the hybrid kernel (continuous + categorical)\n",
    "class HybridKernel(Kernel):\n",
    "    def __init__(self, cont_kernel, cat_kernel, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.cont_kernel = cont_kernel\n",
    "        self.cat_kernel = cat_kernel\n",
    "        self.lambda_param = torch.nn.Parameter(torch.tensor(0.5, dtype=torch.float64), requires_grad=True)\n",
    "        self.register_constraint(\"lambda_param\", Interval(0.0, 1.0))\n",
    "\n",
    "    def forward(self, x1, x2, diag=False, **params):\n",
    "        x1_cat, x1_cont = x1[:, 0], x1[:, 1:]\n",
    "        x2_cat, x2_cont = x2[:, 0], x2[:, 1:]\n",
    "        \n",
    "        k_cat = self.cat_kernel(x1_cat, x2_cat, diag=diag, **params)\n",
    "        k_cont = self.cont_kernel(x1_cont, x2_cont, diag=diag, **params)\n",
    "        \n",
    "        return self.lambda_param * k_cont + (1 - self.lambda_param) * k_cat\n",
    "\n",
    "# Define a similarity matrix for categorical variables (example with 3 categories)\n",
    "similarity_matrix = torch.tensor([\n",
    "    [1.0, 0.5, 0.2],\n",
    "    [0.5, 1.0, 0.3],\n",
    "    [0.2, 0.3, 1.0],\n",
    "], dtype=torch.float64)\n",
    "\n",
    "# Instantiate the kernels\n",
    "cat_kernel = CategoricalKernel(similarity_matrix)\n",
    "cont_kernel = ScaleKernel(MaternKernel(nu=2.5))\n",
    "\n",
    "# Create the hybrid kernel\n",
    "hybrid_kernel = HybridKernel(cont_kernel, cat_kernel)\n",
    "\n",
    "# Apply hybrid kernel to GP model\n",
    "model.covar_module = hybrid_kernel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# below is some chatgpt shit. Dont take it for granted!!!@!!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from botorch.models import SingleTaskGP\n",
    "from gpytorch.likelihoods import GaussianLikelihood\n",
    "from botorch.acquisition import UpperConfidenceBound\n",
    "from botorch.optim import optimize_acqf\n",
    "\n",
    "# Simulated objective function (for testing optimization)\n",
    "def objective_function(h1, h2, x1, x2, x3, x4):\n",
    "    return (h1 + h2) * torch.sin(3 * x1) + 0.5 * torch.cos(2 * x2) + x3**2 - 0.3 * x4\n",
    "\n",
    "# Categorical (integer encoded) and continuous variables\n",
    "categorical_choices = torch.tensor([\n",
    "    [0, 0], [0, 1], [0, 2],  # Category1 (h1) & Category2 (h2)\n",
    "    [1, 0], [1, 1], [1, 2]\n",
    "], dtype=torch.float64)\n",
    "\n",
    "continuous_bounds = torch.tensor([[0, 0, 0, 0], [1, 1, 1, 1]], dtype=torch.float64)\n",
    "\n",
    "# Generate initial training data (random sampling)\n",
    "n_initial = 6\n",
    "train_x_cat = categorical_choices[torch.randint(0, len(categorical_choices), (n_initial,))]\n",
    "train_x_cont = torch.rand((n_initial, 4), dtype=torch.float64)\n",
    "train_x = torch.cat([train_x_cat, train_x_cont], dim=1)\n",
    "\n",
    "train_y = torch.tensor([\n",
    "    objective_function(*point) for point in train_x\n",
    "], dtype=torch.float64).unsqueeze(-1)  # Ensure shape is (N,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\uqkmuroi\\gitcode\\bayesmediaopt\\venv\\Lib\\site-packages\\botorch\\models\\utils\\assorted.py:264: InputDataWarning: Data (input features) is not contained to the unit cube. Please consider min-max scaling the input data.\n",
      "  check_min_max_scaling(\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'gpytorch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m model.covar_module = hybrid_kernel  \u001b[38;5;66;03m# Use hybrid kernel\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Training loop (simple)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m mll = \u001b[43mgpytorch\u001b[49m.mlls.ExactMarginalLogLikelihood(likelihood, model)\n\u001b[32m      8\u001b[39m optimizer = torch.optim.Adam(model.parameters(), lr=\u001b[32m0.1\u001b[39m)\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m50\u001b[39m):  \u001b[38;5;66;03m# Train for 50 iterations\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'gpytorch' is not defined"
     ]
    }
   ],
   "source": [
    "# Create and train the GP model\n",
    "likelihood = GaussianLikelihood()\n",
    "model = SingleTaskGP(train_x, train_y, likelihood=likelihood)\n",
    "model.covar_module = hybrid_kernel  # Use hybrid kernel\n",
    "\n",
    "# Training loop (simple)\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "\n",
    "for i in range(50):  # Train for 50 iterations\n",
    "    optimizer.zero_grad()\n",
    "    output = model(train_x)\n",
    "    loss = -mll(output, train_y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(\"Model Training Completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 1. Expected size 6 but got size 4 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Optimize continuous variables using BO\u001b[39;00m\n\u001b[32m     13\u001b[39m acq_func = UpperConfidenceBound(model, beta=\u001b[32m2.0\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m candidates, _ = \u001b[43moptimize_acqf\u001b[49m\u001b[43m(\u001b[49m\u001b[43macq_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbounds\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontinuous_bounds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_restarts\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraw_samples\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Form final candidate [h1, h2, x1, x2, x3, x4]\u001b[39;00m\n\u001b[32m     17\u001b[39m zt = torch.cat((selected_cat, candidates), dim=-\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\uqkmuroi\\gitcode\\bayesmediaopt\\venv\\Lib\\site-packages\\botorch\\optim\\optimize.py:631\u001b[39m, in \u001b[36moptimize_acqf\u001b[39m\u001b[34m(acq_function, bounds, q, num_restarts, raw_samples, options, inequality_constraints, equality_constraints, nonlinear_inequality_constraints, fixed_features, post_processing_func, batch_initial_conditions, return_best_only, gen_candidates, sequential, ic_generator, timeout_sec, return_full_tree, retry_on_optimization_warning, **ic_gen_kwargs)\u001b[39m\n\u001b[32m    608\u001b[39m     gen_candidates = gen_candidates_scipy\n\u001b[32m    609\u001b[39m opt_acqf_inputs = OptimizeAcqfInputs(\n\u001b[32m    610\u001b[39m     acq_function=acq_function,\n\u001b[32m    611\u001b[39m     bounds=bounds,\n\u001b[32m   (...)\u001b[39m\u001b[32m    629\u001b[39m     ic_gen_kwargs=ic_gen_kwargs,\n\u001b[32m    630\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m631\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_optimize_acqf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopt_inputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mopt_acqf_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\uqkmuroi\\gitcode\\bayesmediaopt\\venv\\Lib\\site-packages\\botorch\\optim\\optimize.py:652\u001b[39m, in \u001b[36m_optimize_acqf\u001b[39m\u001b[34m(opt_inputs)\u001b[39m\n\u001b[32m    649\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _optimize_acqf_sequential_q(opt_inputs=opt_inputs)\n\u001b[32m    651\u001b[39m \u001b[38;5;66;03m# Batch optimization (including the case q=1)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m652\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_optimize_acqf_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopt_inputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mopt_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\uqkmuroi\\gitcode\\bayesmediaopt\\venv\\Lib\\site-packages\\botorch\\optim\\optimize.py:326\u001b[39m, in \u001b[36m_optimize_acqf_batch\u001b[39m\u001b[34m(opt_inputs)\u001b[39m\n\u001b[32m    321\u001b[39m     required_num_restarts -= provided_initial_conditions.shape[\u001b[32m0\u001b[39m]\n\u001b[32m    323\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m opt_inputs.raw_samples \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m required_num_restarts > \u001b[32m0\u001b[39m:\n\u001b[32m    324\u001b[39m     \u001b[38;5;66;03m# pyre-ignore[28]: Unexpected keyword argument `acq_function`\u001b[39;00m\n\u001b[32m    325\u001b[39m     \u001b[38;5;66;03m# to anonymous call.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m     generated_initial_conditions = \u001b[43mopt_inputs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_ic_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    327\u001b[39m \u001b[43m        \u001b[49m\u001b[43macq_function\u001b[49m\u001b[43m=\u001b[49m\u001b[43mopt_inputs\u001b[49m\u001b[43m.\u001b[49m\u001b[43macq_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    328\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbounds\u001b[49m\u001b[43m=\u001b[49m\u001b[43mopt_inputs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    329\u001b[39m \u001b[43m        \u001b[49m\u001b[43mq\u001b[49m\u001b[43m=\u001b[49m\u001b[43mopt_inputs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    330\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_restarts\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequired_num_restarts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    331\u001b[39m \u001b[43m        \u001b[49m\u001b[43mraw_samples\u001b[49m\u001b[43m=\u001b[49m\u001b[43mopt_inputs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraw_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    332\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfixed_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mopt_inputs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfixed_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    333\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    334\u001b[39m \u001b[43m        \u001b[49m\u001b[43minequality_constraints\u001b[49m\u001b[43m=\u001b[49m\u001b[43mopt_inputs\u001b[49m\u001b[43m.\u001b[49m\u001b[43minequality_constraints\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    335\u001b[39m \u001b[43m        \u001b[49m\u001b[43mequality_constraints\u001b[49m\u001b[43m=\u001b[49m\u001b[43mopt_inputs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mequality_constraints\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    336\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mopt_inputs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mic_gen_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    337\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    339\u001b[39m batch_initial_conditions = _combine_initial_conditions(\n\u001b[32m    340\u001b[39m     provided_initial_conditions=provided_initial_conditions,\n\u001b[32m    341\u001b[39m     generated_initial_conditions=generated_initial_conditions,\n\u001b[32m    342\u001b[39m )\n\u001b[32m    344\u001b[39m batch_limit: \u001b[38;5;28mint\u001b[39m = options.get(\n\u001b[32m    345\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mbatch_limit\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    346\u001b[39m     (\n\u001b[32m   (...)\u001b[39m\u001b[32m    350\u001b[39m     ),\n\u001b[32m    351\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\uqkmuroi\\gitcode\\bayesmediaopt\\venv\\Lib\\site-packages\\botorch\\optim\\initializers.py:441\u001b[39m, in \u001b[36mgen_batch_initial_conditions\u001b[39m\u001b[34m(acq_function, bounds, q, num_restarts, raw_samples, fixed_features, options, inequality_constraints, equality_constraints, generator, fixed_X_fantasies)\u001b[39m\n\u001b[32m    436\u001b[39m         batch_limit = X_rnd.shape[\u001b[32m0\u001b[39m]\n\u001b[32m    437\u001b[39m     \u001b[38;5;66;03m# Evaluate the acquisition function on `X_rnd` using `batch_limit`\u001b[39;00m\n\u001b[32m    438\u001b[39m     \u001b[38;5;66;03m# sized chunks.\u001b[39;00m\n\u001b[32m    439\u001b[39m     acq_vals = torch.cat(\n\u001b[32m    440\u001b[39m         [\n\u001b[32m--> \u001b[39m\u001b[32m441\u001b[39m             \u001b[43macq_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m.cpu()\n\u001b[32m    442\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m x_ \u001b[38;5;129;01min\u001b[39;00m X_rnd.split(split_size=batch_limit, dim=\u001b[32m0\u001b[39m)\n\u001b[32m    443\u001b[39m         ],\n\u001b[32m    444\u001b[39m         dim=\u001b[32m0\u001b[39m,\n\u001b[32m    445\u001b[39m     )\n\u001b[32m    447\u001b[39m \u001b[38;5;66;03m# Downselect the initial conditions based on the acquisition function values\u001b[39;00m\n\u001b[32m    448\u001b[39m batch_initial_conditions, _ = init_func(\n\u001b[32m    449\u001b[39m     X=X_rnd, acq_vals=acq_vals, n=num_restarts, **init_kwargs\n\u001b[32m    450\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\uqkmuroi\\gitcode\\bayesmediaopt\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\uqkmuroi\\gitcode\\bayesmediaopt\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\uqkmuroi\\gitcode\\bayesmediaopt\\venv\\Lib\\site-packages\\botorch\\utils\\transforms.py:298\u001b[39m, in \u001b[36mt_batch_mode_transform.<locals>.decorator.<locals>.decorated\u001b[39m\u001b[34m(acqf, X, *args, **kwargs)\u001b[39m\n\u001b[32m    296\u001b[39m \u001b[38;5;66;03m# add t-batch dim\u001b[39;00m\n\u001b[32m    297\u001b[39m X = X \u001b[38;5;28;01mif\u001b[39;00m X.dim() > \u001b[32m2\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m X.unsqueeze(\u001b[32m0\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m298\u001b[39m output = \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[43macqf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    299\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(acqf, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m is_ensemble(acqf.model):\n\u001b[32m    300\u001b[39m     \u001b[38;5;66;03m# IDEA: this could be wrapped into SampleReducingMCAcquisitionFunction\u001b[39;00m\n\u001b[32m    301\u001b[39m     output = (\n\u001b[32m    302\u001b[39m         output.mean(dim=-\u001b[32m1\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m acqf._log \u001b[38;5;28;01melse\u001b[39;00m logmeanexp(output, dim=-\u001b[32m1\u001b[39m)\n\u001b[32m    303\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\uqkmuroi\\gitcode\\bayesmediaopt\\venv\\Lib\\site-packages\\botorch\\acquisition\\analytic.py:795\u001b[39m, in \u001b[36mUpperConfidenceBound.forward\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m    784\u001b[39m \u001b[38;5;129m@t_batch_mode_transform\u001b[39m(expected_q=\u001b[32m1\u001b[39m)\n\u001b[32m    785\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: Tensor) -> Tensor:\n\u001b[32m    786\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Evaluate the Upper Confidence Bound on the candidate set X.\u001b[39;00m\n\u001b[32m    787\u001b[39m \n\u001b[32m    788\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    793\u001b[39m \u001b[33;03m        given design points `X`.\u001b[39;00m\n\u001b[32m    794\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m795\u001b[39m     mean, sigma = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_mean_and_sigma\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    796\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (mean \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.maximize \u001b[38;5;28;01melse\u001b[39;00m -mean) + \u001b[38;5;28mself\u001b[39m.beta.sqrt() * sigma\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\uqkmuroi\\gitcode\\bayesmediaopt\\venv\\Lib\\site-packages\\botorch\\acquisition\\analytic.py:100\u001b[39m, in \u001b[36mAnalyticAcquisitionFunction._mean_and_sigma\u001b[39m\u001b[34m(self, X, compute_sigma, min_var)\u001b[39m\n\u001b[32m     86\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Computes the first and second moments of the model posterior.\u001b[39;00m\n\u001b[32m     87\u001b[39m \n\u001b[32m     88\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     97\u001b[39m \u001b[33;03m    returns a single tensor of means if compute_sigma is True.\u001b[39;00m\n\u001b[32m     98\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     99\u001b[39m \u001b[38;5;28mself\u001b[39m.to(device=X.device)  \u001b[38;5;66;03m# ensures buffers / parameters are on the same device\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m100\u001b[39m posterior = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mposterior\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    101\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposterior_transform\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mposterior_transform\u001b[49m\n\u001b[32m    102\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    103\u001b[39m mean = posterior.mean.squeeze(-\u001b[32m2\u001b[39m).squeeze(-\u001b[32m1\u001b[39m)  \u001b[38;5;66;03m# removing redundant dimensions\u001b[39;00m\n\u001b[32m    104\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m compute_sigma:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\uqkmuroi\\gitcode\\bayesmediaopt\\venv\\Lib\\site-packages\\botorch\\models\\gpytorch.py:448\u001b[39m, in \u001b[36mBatchedMultiOutputGPyTorchModel.posterior\u001b[39m\u001b[34m(self, X, output_indices, observation_noise, posterior_transform)\u001b[39m\n\u001b[32m    442\u001b[39m     X, output_dim_idx = add_output_dim(\n\u001b[32m    443\u001b[39m         X=X, original_batch_shape=\u001b[38;5;28mself\u001b[39m._input_batch_shape\n\u001b[32m    444\u001b[39m     )\n\u001b[32m    445\u001b[39m \u001b[38;5;66;03m# NOTE: BoTorch's GPyTorchModels also inherit from GPyTorch's ExactGP, thus\u001b[39;00m\n\u001b[32m    446\u001b[39m \u001b[38;5;66;03m# self(X) calls GPyTorch's ExactGP's __call__, which computes the posterior,\u001b[39;00m\n\u001b[32m    447\u001b[39m \u001b[38;5;66;03m# rather than e.g. SingleTaskGP's forward, which computes the prior.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m448\u001b[39m mvn = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    449\u001b[39m mvn = \u001b[38;5;28mself\u001b[39m._apply_noise(X=X, mvn=mvn, observation_noise=observation_noise)\n\u001b[32m    450\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_outputs > \u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\uqkmuroi\\gitcode\\bayesmediaopt\\venv\\Lib\\site-packages\\gpytorch\\models\\exact_gp.py:325\u001b[39m, in \u001b[36mExactGP.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    323\u001b[39m         train_input = train_input.expand(*batch_shape, *train_input.shape[-\u001b[32m2\u001b[39m:])\n\u001b[32m    324\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[38;5;28minput\u001b[39m.expand(*batch_shape, *\u001b[38;5;28minput\u001b[39m.shape[-\u001b[32m2\u001b[39m:])\n\u001b[32m--> \u001b[39m\u001b[32m325\u001b[39m     full_inputs.append(\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m=\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[32m    327\u001b[39m \u001b[38;5;66;03m# Get the joint distribution for training/test data\u001b[39;00m\n\u001b[32m    328\u001b[39m full_output = \u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m(*full_inputs, **kwargs)\n",
      "\u001b[31mRuntimeError\u001b[39m: Sizes of tensors must match except in dimension 1. Expected size 6 but got size 4 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "# EXP3 for categorical choices\n",
    "exp3 = EXP3(len(categorical_choices))\n",
    "\n",
    "# Bayesian Optimization Loop\n",
    "num_iterations = 10\n",
    "best_observed = []\n",
    "for i in range(num_iterations):\n",
    "    # Select categorical variables using EXP3\n",
    "    selected_cat_idx = exp3.select_arm()\n",
    "    selected_cat = categorical_choices[selected_cat_idx]\n",
    "\n",
    "    # Optimize continuous variables using BO\n",
    "    acq_func = UpperConfidenceBound(model, beta=2.0)\n",
    "    candidates, _ = optimize_acqf(acq_func, bounds=continuous_bounds, q=1, num_restarts=5, raw_samples=20)\n",
    "\n",
    "    # Form final candidate [h1, h2, x1, x2, x3, x4]\n",
    "    zt = torch.cat((selected_cat, candidates), dim=-1)\n",
    "\n",
    "    # Evaluate function\n",
    "    new_y = objective_function(*zt).unsqueeze(-1)\n",
    "\n",
    "    # Update dataset\n",
    "    train_x = torch.cat([train_x, zt.unsqueeze(0)], dim=0)\n",
    "    train_y = torch.cat([train_y, new_y.unsqueeze(0)], dim=0)\n",
    "\n",
    "    # Update GP model\n",
    "    model.set_train_data(train_x, train_y, strict=False)\n",
    "\n",
    "    # Store best result\n",
    "    best_observed.append(train_y.max().item())\n",
    "\n",
    "    print(f\"Iteration {i+1}: Selected {zt}, Function Value = {new_y.item()}\")\n",
    "\n",
    "# Plot best observed function values\n",
    "plt.plot(range(1, num_iterations+1), best_observed, marker='o')\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Best Observed Value\")\n",
    "plt.title(\"Bayesian Optimization Progress\")\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4Ô∏è‚É£ Expected Outputs\n",
    "Selected categorical variables (e.g., [0,1], [1,2], etc.)\n",
    "\n",
    "Optimized continuous values (e.g., [0.7, 0.5, 0.9, 0.3])\n",
    "\n",
    "Best observed function values plotted over iterations\n",
    "\n",
    "üéØ Summary\n",
    "‚úÖ GP + Hybrid Kernel ‚Üí Handles categorical & continuous spaces\n",
    "‚úÖ EXP3 ‚Üí Optimizes categorical variables\n",
    "‚úÖ Bayesian Optimization ‚Üí Optimizes continuous variables\n",
    "‚úÖ Visual Output ‚Üí Convergence graph showing BO progress\n",
    "\n",
    "Let me know if you need any refinements! üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
