{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/s/PhD/gitrepo/Torch-Practice/venv/lib/python3.13/site-packages/torch/_subclasses/functional_tensor.py:275: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:81.)\n",
      "  cpu = _conversion_method_template(device=torch.device(\"cpu\"))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Create a tensor\n",
    "x = torch.tensor([1, 2, 3])\n",
    "y = torch.ones_like(x)  # Creates a tensor of ones with the same shape as x\n",
    "\n",
    "# Basic tensor operations\n",
    "z = x + y  # Element-wise addition\n",
    "w = torch.matmul(x, y)  # Matrix multiplication (dot product if 1D)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 4.])\n"
     ]
    }
   ],
   "source": [
    "# Example with autograd\n",
    "x = torch.tensor([1.0, 2.0], requires_grad=True)\n",
    "y = x**2\n",
    "z = y.sum()\n",
    "\n",
    "# Compute gradients\n",
    "z.backward()\n",
    "\n",
    "# Access gradients\n",
    "print(x.grad)  # Gradient of z with respect to x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.layer1 = nn.Linear(10, 5)  # Input: 10 features, Output: 5 features\n",
    "        self.layer2 = nn.ReLU()\n",
    "        self.layer3 = nn.Linear(5, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Model and optimizer\n",
    "model = SimpleNN()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Zero the gradients, perform a backward pass, and update weights\n",
    "optimizer.zero_grad()  # Clears old gradients\n",
    "output = model(torch.randn(1, 10))  # Example input\n",
    "loss = output.sum()  # Just an example loss function\n",
    "loss.backward()\n",
    "optimizer.step()  # Updates parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gpytorch\n",
    "\n",
    "class GPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(GPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.RBFKernel()\n",
    "        self.likelihood = likelihood\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "# Training a GP model with GPytorch involves defining a likelihood and optimizing the model parameters using PyTorch's optimizers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/s/PhD/gitrepo/Torch-Practice/venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/var/folders/_4/f_rgwft14s14_j1xnpz_zn7c0000gn/T/ipykernel_12979/3905559278.py:10: InputDataWarning: The model inputs are of type torch.float32. It is strongly recommended to use double precision in BoTorch, as this improves both precision and stability and can help avoid numerical errors. See https://github.com/pytorch/botorch/discussions/1444\n",
      "  gp = SingleTaskGP(train_x, train_y)\n",
      "/Users/s/PhD/gitrepo/Torch-Practice/venv/lib/python3.13/site-packages/botorch/acquisition/analytic.py:331: NumericsWarning: ExpectedImprovement has known numerical issues that lead to suboptimal optimization performance. It is strongly recommended to simply replace\n",
      "\n",
      "\t ExpectedImprovement \t --> \t LogExpectedImprovement \n",
      "\n",
      "instead, which fixes the issues and has the same API. See https://arxiv.org/abs/2310.20708 for details.\n",
      "  legacy_ei_numerics_warning(legacy_name=type(self).__name__)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "optimize_acqf() missing 1 required positional argument: 'q'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mbotorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01moptim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m optimize_acqf\n\u001b[32m     17\u001b[39m acqf = ExpectedImprovement(gp, best_f=train_y.max())\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m next_point, _ = \u001b[43moptimize_acqf\u001b[49m\u001b[43m(\u001b[49m\u001b[43macqf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbounds\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0.\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1.\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_restarts\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraw_samples\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: optimize_acqf() missing 1 required positional argument: 'q'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from botorch.models import SingleTaskGP\n",
    "from botorch.optim import optimize_acqf\n",
    "\n",
    "# Define some data points and a GP model\n",
    "train_x = torch.rand(10, 1)  # 10 data points, 1 feature each\n",
    "train_y = torch.sin(train_x)\n",
    "\n",
    "# Build a single-task GP model\n",
    "gp = SingleTaskGP(train_x, train_y)\n",
    "\n",
    "# Define an acquisition function (e.g., Expected Improvement)\n",
    "# and optimize it to find the next sample\n",
    "from botorch.acquisition import ExpectedImprovement\n",
    "from botorch.optim import optimize_acqf\n",
    "\n",
    "acqf = ExpectedImprovement(gp, best_f=train_y.max())\n",
    "next_point, _ = optimize_acqf(acqf, bounds=torch.tensor([[0.], [1.]]), num_restarts=5, raw_samples=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_4/f_rgwft14s14_j1xnpz_zn7c0000gn/T/ipykernel_13358/1056962898.py:11: InputDataWarning: The model inputs are of type torch.float32. It is strongly recommended to use double precision in BoTorch, as this improves both precision and stability and can help avoid numerical errors. See https://github.com/pytorch/botorch/discussions/1444\n",
      "  gp = SingleTaskGP(train_x, train_y)\n",
      "/Users/s/PhD/gitrepo/Torch-Practice/venv/lib/python3.13/site-packages/botorch/acquisition/analytic.py:331: NumericsWarning: ExpectedImprovement has known numerical issues that lead to suboptimal optimization performance. It is strongly recommended to simply replace\n",
      "\n",
      "\t ExpectedImprovement \t --> \t LogExpectedImprovement \n",
      "\n",
      "instead, which fixes the issues and has the same API. See https://arxiv.org/abs/2310.20708 for details.\n",
      "  legacy_ei_numerics_warning(legacy_name=type(self).__name__)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from botorch.models import SingleTaskGP\n",
    "from botorch.acquisition import ExpectedImprovement\n",
    "from botorch.optim import optimize_acqf\n",
    "\n",
    "# Define some data points and a GP model\n",
    "train_x = torch.rand(10, 1)  # 10 data points, 1 feature each\n",
    "train_y = torch.sin(train_x)\n",
    "\n",
    "# Build a single-task GP model\n",
    "gp = SingleTaskGP(train_x, train_y)\n",
    "\n",
    "# Define an acquisition function (e.g., Expected Improvement)\n",
    "acqf = ExpectedImprovement(gp, best_f=train_y.max())\n",
    "\n",
    "# Specify the number of candidates to optimize over\n",
    "q = 1  # For single candidate optimization\n",
    "\n",
    "#define boudnds\n",
    "bounds = torch.tensor([[0.],[1.]])\n",
    "\n",
    "# Optimize the acquisition function to find the next sample\n",
    "next_point, _ = optimize_acqf(\n",
    "    acqf, \n",
    "    bounds=bounds,  #pass\n",
    "    num_restarts=5,  # Number of random restarts for optimization\n",
    "    raw_samples=20,  # Number of raw candidates to sample before optimization\n",
    "    q=q  # Number of candidates to optimize over at once\n",
    ")\n",
    "\n",
    "print(next_point)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_4/f_rgwft14s14_j1xnpz_zn7c0000gn/T/ipykernel_13358/782265510.py:11: InputDataWarning: The model inputs are of type torch.float32. It is strongly recommended to use double precision in BoTorch, as this improves both precision and stability and can help avoid numerical errors. See https://github.com/pytorch/botorch/discussions/1444\n",
      "  gp = SingleTaskGP(train_x, train_y)\n",
      "/Users/s/PhD/gitrepo/Torch-Practice/venv/lib/python3.13/site-packages/botorch/acquisition/analytic.py:331: NumericsWarning: ExpectedImprovement has known numerical issues that lead to suboptimal optimization performance. It is strongly recommended to simply replace\n",
      "\n",
      "\t ExpectedImprovement \t --> \t LogExpectedImprovement \n",
      "\n",
      "instead, which fixes the issues and has the same API. See https://arxiv.org/abs/2310.20708 for details.\n",
      "  legacy_ei_numerics_warning(legacy_name=type(self).__name__)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from botorch.models import SingleTaskGP\n",
    "from botorch.acquisition import ExpectedImprovement\n",
    "from botorch.optim import optimize_acqf\n",
    "\n",
    "# Define some data points and a GP model\n",
    "train_x = torch.rand(10, 1)  # 10 data points, 1 feature each\n",
    "train_y = torch.sin(train_x)\n",
    "\n",
    "# Build a single-task GP model\n",
    "gp = SingleTaskGP(train_x, train_y)\n",
    "\n",
    "# Define an acquisition function (e.g., Expected Improvement)\n",
    "acqf = ExpectedImprovement(gp, best_f=train_y.max())\n",
    "\n",
    "# Specify the number of candidates to optimize over\n",
    "q = 1  # For single candidate optimization\n",
    "\n",
    "# Define the bounds of the search space (ensure correct shape: 2 x 1)\n",
    "bounds = torch.tensor([[0.], [1.]])  # Lower and upper bounds in the 1D space\n",
    "\n",
    "# Optimize the acquisition function to find the next sample\n",
    "next_point, _ = optimize_acqf(\n",
    "    acqf, \n",
    "    bounds=bounds,  # Pass bounds here\n",
    "    num_restarts=5,  # Number of random restarts for optimization\n",
    "    raw_samples=20,  # Number of raw candidates to sample before optimization\n",
    "    q=q  # Number of candidates to optimize over at once\n",
    ")\n",
    "\n",
    "print(next_point)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN Epoch 0, Loss: 0.5697742104530334\n",
      "NN Epoch 50, Loss: 0.2620859742164612\n",
      "NN Epoch 100, Loss: 0.1349755823612213\n",
      "NN Epoch 150, Loss: 0.03566164895892143\n",
      "Warning: Output is not scalar. Shape: torch.Size([10])\n",
      "Warning: Loss is not scalar. Shape: torch.Size([10])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "grad can be implicitly created only for scalar outputs",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 79\u001b[39m\n\u001b[32m     76\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(loss, torch.Tensor) \u001b[38;5;129;01mand\u001b[39;00m loss.shape != torch.Size([]):\n\u001b[32m     77\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mWarning: Loss is not scalar. Shape:\u001b[39m\u001b[33m\"\u001b[39m, loss.shape)\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     80\u001b[39m gp_optimizer.step()\n\u001b[32m     82\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m epoch % \u001b[32m20\u001b[39m == \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PhD/gitrepo/Torch-Practice/venv/lib/python3.13/site-packages/torch/_tensor.py:626\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    616\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    617\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    618\u001b[39m         Tensor.backward,\n\u001b[32m    619\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    624\u001b[39m         inputs=inputs,\n\u001b[32m    625\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    628\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PhD/gitrepo/Torch-Practice/venv/lib/python3.13/site-packages/torch/autograd/__init__.py:340\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    331\u001b[39m inputs = (\n\u001b[32m    332\u001b[39m     (inputs,)\n\u001b[32m    333\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inputs, (torch.Tensor, graph.GradientEdge))\n\u001b[32m   (...)\u001b[39m\u001b[32m    336\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m ()\n\u001b[32m    337\u001b[39m )\n\u001b[32m    339\u001b[39m grad_tensors_ = _tensor_or_tensors_to_tuple(grad_tensors, \u001b[38;5;28mlen\u001b[39m(tensors))\n\u001b[32m--> \u001b[39m\u001b[32m340\u001b[39m grad_tensors_ = \u001b[43m_make_grads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_grads_batched\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    341\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    342\u001b[39m     retain_graph = create_graph\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PhD/gitrepo/Torch-Practice/venv/lib/python3.13/site-packages/torch/autograd/__init__.py:198\u001b[39m, in \u001b[36m_make_grads\u001b[39m\u001b[34m(outputs, grads, is_grads_batched)\u001b[39m\n\u001b[32m    196\u001b[39m     out_numel_is_1 = out.numel() == \u001b[32m1\u001b[39m\n\u001b[32m    197\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m out_numel_is_1:\n\u001b[32m--> \u001b[39m\u001b[32m198\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    199\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mgrad can be implicitly created only for scalar outputs\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    200\u001b[39m     )\n\u001b[32m    201\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m out_dtype.is_floating_point:\n\u001b[32m    202\u001b[39m     msg = (\n\u001b[32m    203\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mgrad can be implicitly created only for real scalar outputs\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    204\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mout_dtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    205\u001b[39m     )\n",
      "\u001b[31mRuntimeError\u001b[39m: grad can be implicitly created only for scalar outputs"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import gpytorch\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Step 1: Define a 2-layer Neural Network model\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(1, 50)  # 1 input feature, 50 output features\n",
    "        self.fc2 = nn.Linear(50, 1)  # 50 input features, 1 output feature\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))  # Apply ReLU activation function\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Step 2: Define a Gaussian Process Model using GPytorch\n",
    "class GPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(GPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()  # Constant Mean function\n",
    "        self.covar_module = gpytorch.kernels.RBFKernel()  # RBF Kernel\n",
    "        self.likelihood = likelihood\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "# Step 3: Generate toy training data\n",
    "train_x = torch.linspace(0, 1, 10).reshape(-1, 1)  # 10 training points\n",
    "train_y = torch.sin(train_x * (2 * torch.pi)) + 0.2 * torch.randn_like(train_x)  # Sine curve + noise\n",
    "\n",
    "# Step 4: Define the Neural Network Model and Train It\n",
    "nn_model = SimpleNN()\n",
    "nn_optimizer = optim.Adam(nn_model.parameters(), lr=0.01)\n",
    "nn_loss_fn = nn.MSELoss()\n",
    "\n",
    "# Train the neural network\n",
    "num_epochs = 200\n",
    "for epoch in range(num_epochs):\n",
    "    nn_model.train()\n",
    "    nn_optimizer.zero_grad()\n",
    "    nn_pred = nn_model(train_x)\n",
    "    nn_loss = nn_loss_fn(nn_pred, train_y)\n",
    "    nn_loss.backward()\n",
    "    nn_optimizer.step()\n",
    "\n",
    "    if epoch % 50 == 0:\n",
    "        print(f\"NN Epoch {epoch}, Loss: {nn_loss.item()}\")\n",
    "\n",
    "# Step 5: Define the GP Model and Train It\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "gp_model = GPModel(train_x, train_y, likelihood)\n",
    "\n",
    "# Set up optimizer and loss function for GP\n",
    "gp_optimizer = torch.optim.Adam(gp_model.parameters(), lr=0.1)\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, gp_model)\n",
    "\n",
    "# Train the GP model\n",
    "gp_model.train()\n",
    "likelihood.train()\n",
    "\n",
    "for epoch in range(100):\n",
    "    gp_optimizer.zero_grad()\n",
    "    output = gp_model(train_x)\n",
    "    # Ensure the output is a scalar and that the loss can be backpropagated\n",
    "    if output.mean.shape != torch.Size([]):  # Check if it's not scalar\n",
    "        print(\"Warning: Output is not scalar. Shape:\", output.mean.shape)\n",
    "\n",
    "    loss = -mll(output, train_y)\n",
    "\n",
    "    # Ensure loss is scalar\n",
    "    if isinstance(loss, torch.Tensor) and loss.shape != torch.Size([]):\n",
    "        print(\"Warning: Loss is not scalar. Shape:\", loss.shape)\n",
    "\n",
    "    loss.backward()\n",
    "    gp_optimizer.step()\n",
    "\n",
    "    if epoch % 20 == 0:\n",
    "        print(f\"GP Epoch {epoch}, Loss: {loss.item()}\")\n",
    "\n",
    "# Step 6: Evaluate both models and make predictions\n",
    "# Neural Network Prediction\n",
    "nn_model.eval()\n",
    "nn_pred = nn_model(train_x).detach()\n",
    "\n",
    "# Gaussian Process Prediction\n",
    "gp_model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    gp_pred = likelihood(gp_model(train_x))\n",
    "\n",
    "# Step 7: Plot the Results\n",
    "# Plotting both Neural Network and GP predictions with confidence intervals\n",
    "test_x = torch.linspace(0, 1, 1000).reshape(-1, 1)\n",
    "\n",
    "# Neural Network Predictions for test points\n",
    "nn_test_pred = nn_model(test_x).detach()\n",
    "\n",
    "# GP Predictions for test points\n",
    "with torch.no_grad():\n",
    "    gp_test_pred = likelihood(gp_model(test_x))\n",
    "\n",
    "# Plot Training Data\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Neural Network Predictions\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(train_x.numpy(), train_y.numpy(), 'kx', label=\"Training Data\")\n",
    "plt.plot(test_x.numpy(), nn_test_pred.numpy(), 'r', label=\"NN Prediction\")\n",
    "plt.title(\"Neural Network Prediction vs Training Data\")\n",
    "plt.legend()\n",
    "\n",
    "# GP Predictions\n",
    "plt.subplot(2, 1, 2)\n",
    "mean = gp_test_pred.mean\n",
    "lower, upper = gp_test_pred.confidence_region()\n",
    "plt.plot(train_x.numpy(), train_y.numpy(), 'kx', label=\"Training Data\")\n",
    "plt.plot(test_x.numpy(), mean.numpy(), 'r', label=\"GP Prediction Mean\")\n",
    "plt.fill_between(test_x.numpy(), lower.numpy(), upper.numpy(), color='red', alpha=0.2, label=\"GP Confidence Region\")\n",
    "plt.title(\"Gaussian Process Prediction vs Training Data\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Step 8: Compare Metrics\n",
    "# Calculate MSE for both models\n",
    "nn_mse = nn_loss_fn(nn_pred, train_y).item()\n",
    "gp_mse = ((gp_pred.mean - train_y) ** 2).mean().item()\n",
    "\n",
    "print(f\"Neural Network MSE: {nn_mse}\")\n",
    "print(f\"Gaussian Process MSE: {gp_mse}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
