{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/s/PhD/gitrepo/bayesmediaopt/venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from pyDOE3 import lhs\n",
    "\n",
    "from botorch.models import SingleTaskGP\n",
    "from botorch.models.transforms import Standardize, Normalize\n",
    "from botorch.fit import fit_gpytorch_mll\n",
    "from botorch.acquisition.monte_carlo import qUpperConfidenceBound\n",
    "from botorch.acquisition.logei import qLogNoisyExpectedImprovement\n",
    "from botorch.optim import optimize_acqf\n",
    "\n",
    "from gpytorch import kernels, means, likelihoods\n",
    "from gpytorch.priors import LogNormalPrior\n",
    "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
    "from gpytorch.constraints import Interval\n",
    "from gpytorch.priors import SmoothedBoxPrior\n",
    "\n",
    "true_optimum = torch.tensor([7.0, 33.5]) # answers determined by querying objective function\n",
    "max_samples = 100 #This is initial sample + (batch size x number of iterations)\n",
    "\n",
    "\n",
    "nu = 2.5\n",
    "lower_noise_bound = .1\n",
    "upper_noise_bound = .3\n",
    "bounds = torch.tensor([\n",
    "    [6.0, 20.0],  # Lower bounds ph, temp\n",
    "    [8.0, 40.0]   # Upper bounds ph, temp\n",
    "], dtype=torch.double)\n",
    "dim = bounds.shape[1]# Extracts number of dimensions from bounds variable\n",
    "\n",
    "def euclidean_distance(x1, x2):\n",
    "    return torch.norm(x1-x2).item()\n",
    "\n",
    "def objective_function(X, pHopt =7, pHopt2 = 5.5, temp_opt =35, temp_opt2=30,  a = 100, b = 20, c = 1, noise_level = 0, seed = None): # changed noise level to 0\n",
    "    if seed is not None:\n",
    "            torch.manual_seed(seed) # set for reproducibility\n",
    "    pH, temp = X[:, 0], X[:, 1]\n",
    "\n",
    "    # First peak at (pH=7, temp=35)\n",
    "    pH_term1 = torch.exp(-0.5 * ((pH - pHopt) / 1.5)**2)  # Gaussian term for pH with width 1.5\n",
    "    temp_term1 = torch.exp(-0.5 * ((temp - temp_opt) / 5.0)**2)  # Gaussian term for temp with width 5.0\n",
    "\n",
    "    # Second peak at (pH=5.5, temp=30)\n",
    "    pH_term2 = torch.exp(-0.5 * ((pH - pHopt2) / 1.5)**2)\n",
    "    temp_term2 = torch.exp(-0.5 * ((temp - temp_opt2) / 5.0)**2)\n",
    "\n",
    "    # Stronger Sinusoidal Modulation\n",
    "    sin_component = torch.sin(2 * pH) * torch.cos(1.5 * temp)  # Higher frequency\n",
    "    wave_strength = 1.5  # Scale up the wave effect\n",
    "\n",
    "    # Combine the two peaks with the stronger sinusoidal variation\n",
    "    y = (pH_term1 * temp_term1 + pH_term2 * temp_term2) * (1 + wave_strength * sin_component) *100\n",
    "\n",
    "    noise = noise_level*torch.randn_like(y) # stddev = noise_level\n",
    "    \n",
    "    return y + noise\n",
    "\n",
    "# GP Model definition\n",
    "class GPModel(SingleTaskGP):\n",
    "    def __init__(self, train_X, train_Y, fixed_noise=False, noise_level=0, #changed this to None, since it's defined in the loop\n",
    "                 lengthscale_prior=None, outputscale_prior=None,\n",
    "                 lengthscale_constraint = None, outputscale_constraint=None):\n",
    "\n",
    "        if fixed_noise:\n",
    "            print(f\"Training with FIXED noise: {noise_level} = std dev.\")\n",
    "            noise_variance = (noise_level * train_Y.mean()).pow(2)\n",
    "            train_Yvar = torch.full_like(train_Y, noise_variance)\n",
    "            likelihood = None\n",
    "            super().__init__(\n",
    "                train_X, train_Y, train_Yvar=train_Yvar, likelihood=likelihood,\n",
    "                outcome_transform=Standardize(m=1),\n",
    "                input_transform=Normalize(d=dim)\n",
    "            )\n",
    "        else:\n",
    "            #print(\"Training with LEARNABLE noise (Gaussian Likelihood).\")\n",
    "            likelihood = likelihoods.GaussianLikelihood()\n",
    "            super().__init__(\n",
    "                train_X, train_Y, likelihood=likelihood,\n",
    "                outcome_transform=Standardize(m=1),\n",
    "                input_transform=Normalize(d=dim)\n",
    "            )\n",
    "            lower_noise = lower_noise_bound**2  # lower noise bound\n",
    "            upper_noise = upper_noise_bound**2  # upper noise bound\n",
    "\n",
    "            # Add a **prior** (softly nudges during training)\n",
    "            \n",
    "            self.likelihood.noise_covar.register_prior(\n",
    "                \"noise_prior\",\n",
    "                SmoothedBoxPrior(lower_noise, upper_noise),\n",
    "                \"raw_noise\"\n",
    "            )\n",
    "            \n",
    "            # Add a **constraint** (hard bounding box)\n",
    "            self.likelihood.noise_covar.register_constraint(\n",
    "                \"raw_noise\",\n",
    "                Interval(lower_noise, upper_noise)\n",
    "            )\n",
    "            \n",
    "\n",
    "        self.mean_module = means.ConstantMean()#ZeroMean() # this is default ConstantMean() in GPyTorch. Worth investigating\n",
    "\n",
    "        matern_kernel = kernels.MaternKernel(\n",
    "            nu=nu,\n",
    "            ard_num_dims=dim,\n",
    "            lengthscale_prior=lengthscale_prior,\n",
    "            lengthscale_constraint=lengthscale_constraint,\n",
    "        )\n",
    "\n",
    "        self.covar_module = kernels.ScaleKernel(\n",
    "            base_kernel=matern_kernel,\n",
    "            outputscale_prior=outputscale_prior,\n",
    "            outputscale_constraint=outputscale_constraint,\n",
    "        )\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "# Training function\n",
    "def train_GP_model(train_X, train_Y, fixed_noise=False, noise_level=0,# changed this to None, as it's defined in the loop\n",
    "                   lengthscale_prior=None, outputscale_prior=None,\n",
    "                   lengthscale_constraint = None, outputscale_constraint=None): \n",
    "    model = GPModel(\n",
    "        train_X, train_Y,\n",
    "        fixed_noise=fixed_noise,\n",
    "        noise_level=noise_level,\n",
    "        lengthscale_prior=lengthscale_prior,\n",
    "        outputscale_prior=outputscale_prior,\n",
    "        lengthscale_constraint = lengthscale_constraint, \n",
    "        outputscale_constraint=outputscale_constraint\n",
    "    )\n",
    "    mll = ExactMarginalLogLikelihood(model.likelihood, model)\n",
    "    \n",
    "    #===== Fit the model =====#\n",
    "    fit_gpytorch_mll(mll) # This is the default (turn off the custom training loop below if using this)\n",
    "\n",
    "    # to use a custom optimizer and make this model fully customizable you can use the bit below \n",
    "    # make sure to turn off the fit_gpytorch_mll above\n",
    "    '''\n",
    "    # Custom training loop\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.05)\n",
    "    model.train()\n",
    "    mll.train()\n",
    "    training_iter = 50\n",
    "    for i in range(training_iter):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(train_X)\n",
    "        loss = -mll(output, train_Y.squeeze(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f\"Iter {i+1}/{training_iter} - Loss: {loss.item():.3f}\")\n",
    "    #===== End of custom training loop =====#\n",
    "    '''\n",
    "    return model, mll\n",
    "\n",
    "\n",
    "batch_sizes = [1, 5, 10]\n",
    "initial_samples = [4, 24]\n",
    "noise_levels = [0, 20]\n",
    "betas = [0.01, 5.0]\n",
    "seeds = [0, 1]\n",
    "\n",
    "results = []\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    for initial_sample in initial_samples:\n",
    "        for noise_level in noise_levels:\n",
    "            for beta in betas:\n",
    "                for seed in seeds:\n",
    "                    # initialize initial sample through LHS \n",
    "                    lhs_design = torch.tensor(\n",
    "                            lhs(n = dim, samples = initial_sample, criterion = 'maximin', random_state=seed), \n",
    "                        dtype=torch.double\n",
    "                        )\n",
    "                    # flexibly set the bounds using 'bounds' variable (allowing for dimensional scale-up)\n",
    "                    scaled_lhs_design = bounds[0] + (bounds[1]-bounds[0])*lhs_design\n",
    "                    # initialize training data as dictated by LHS and objective function\n",
    "                    train_X = scaled_lhs_design\n",
    "                    train_Y = objective_function(train_X, noise_level) # i don't use seed here to make sure the noise is different for each sample\n",
    "                    train_Y = train_Y.unsqueeze(-1)\n",
    "                    total_sample = initial_sample\n",
    "                    max_iters = int((max_samples - initial_sample) / batch_size) # number of iterations to run\n",
    "                    converged_95 = None\n",
    "                    converged_99 = None\n",
    "                    n_iters = 0\n",
    "\n",
    "                    for iteration in range(max_iters):\n",
    "                        model, mll = train_GP_model(train_X, train_Y, noise_level= noise_level, fixed_noise=False)\n",
    "\n",
    "                        model.eval()\n",
    "                        model.likelihood.eval()\n",
    "\n",
    "                        acq_func = qUpperConfidenceBound(model=model, beta=beta)\n",
    "\n",
    "                        candidate, _ = optimize_acqf(\n",
    "                            acq_function=acq_func,\n",
    "                            bounds=bounds,\n",
    "                            q=batch_size,\n",
    "                            num_restarts=5,\n",
    "                            raw_samples=20\n",
    "                        )\n",
    "\n",
    "                        new_y = objective_function(candidate, noise_level= noise_level)\n",
    "                        train_X = torch.cat([train_X, candidate], dim=0)\n",
    "                        train_Y = torch.cat([train_Y, new_y.unsqueeze(-1)], dim=0)\n",
    "                        total_sample += batch_size\n",
    "                        n_iters += 1\n",
    "\n",
    "                        best_idx = train_Y.argmax()\n",
    "                        best_X = train_X[best_idx]\n",
    "\n",
    "                        dist_to_optim = euclidean_distance(best_X, true_optimum)\n",
    "                        if converged_95 is None and dist_to_optim <= 0.05:\n",
    "                            converged_95 = total_sample\n",
    "                        if converged_99 is None and dist_to_optim <= 0.01:\n",
    "                            converged_99 = total_sample\n",
    "\n",
    "                    results.append({\n",
    "                        'batch_size': batch_size,\n",
    "                        'initial_sample': initial_sample,\n",
    "                        'noise_level': noise_level,\n",
    "                        'beta': beta,\n",
    "                        'seed': seed,\n",
    "                        'converged_95': converged_95,\n",
    "                        'converged_99': converged_99,\n",
    "                        'final dist to optimum': dist_to_optim,\n",
    "                        'final best_x': best_X,\n",
    "                        'final best_y': train_Y[best_idx],\n",
    "                        'iterations': n_iters\n",
    "                    })\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "print(df)\n",
    "\n",
    "import openpyxl\n",
    "\n",
    "df.to_csv(\"newUCB tests1\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
