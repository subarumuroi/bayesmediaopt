{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayesian Optimizer from ChatGPT as reference for a working model suggests next experiment variables to explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next batch of experiments: [[3.98155016 0.2281117 ]\n",
      " [4.46467641 0.04498378]\n",
      " [4.48987202 0.05497378]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\uqkmuroi\\gitcode\\bayesmediaopt\\venv\\Lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:442: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "from scipy.stats import qmc\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern\n",
    "\n",
    "class BayesianOptimizer:\n",
    "    def __init__(self, bounds, is_categorical, batch_size=3, beta=2.0):\n",
    "        self.bounds = np.array(bounds)  # [(low, high), ...] for each dimension\n",
    "        self.is_categorical = np.array(is_categorical)  # Boolean mask for categorical variables\n",
    "        self.batch_size = batch_size\n",
    "        self.beta = beta  # Controls exploration vs. exploitation\n",
    "        \n",
    "        # Define GP model with Matern kernel\n",
    "        self.kernel = Matern(length_scale=1.0, nu=2.5)\n",
    "        self.gp = GaussianProcessRegressor(kernel=self.kernel, alpha=1e-6, normalize_y=True)\n",
    "\n",
    "        # Store observed data\n",
    "        self.X_train = None\n",
    "        self.y_train = None\n",
    "\n",
    "    def add_observations(self, X_new, y_new):\n",
    "        \"\"\"Update the dataset with new observations.\"\"\"\n",
    "        if self.X_train is None:\n",
    "            self.X_train = np.array(X_new)\n",
    "            self.y_train = np.array(y_new)\n",
    "        else:\n",
    "            self.X_train = np.vstack((self.X_train, X_new))\n",
    "            self.y_train = np.append(self.y_train, y_new)\n",
    "        self.gp.fit(self.X_train, self.y_train)  # Retrain GP\n",
    "\n",
    "    def ucb_acquisition(self, X):\n",
    "        \"\"\"Upper Confidence Bound (UCB) acquisition function.\"\"\"\n",
    "        mean, std = self.gp.predict(X, return_std=True)\n",
    "        return mean + self.beta * std  # Encourages exploration & exploitation\n",
    "\n",
    "    def optimize_acquisition(self):\n",
    "        \"\"\"Finds the next experiment to run using different strategies for continuous & categorical variables.\"\"\"\n",
    "        if np.any(self.is_categorical):\n",
    "            # Latin Hypercube Sampling (LHS) for categorical variables\n",
    "            sampler = qmc.LatinHypercube(d=len(self.bounds))\n",
    "            sample_points = qmc.scale(sampler.random(n=10000), self.bounds[:, 0], self.bounds[:, 1])\n",
    "            best_idx = np.argmax(self.ucb_acquisition(sample_points))\n",
    "            return sample_points[best_idx]\n",
    "        else:\n",
    "            # Use LBFGS for continuous optimization\n",
    "            best_x = None\n",
    "            best_value = -np.inf\n",
    "            for _ in range(10):  # Multi-start optimization\n",
    "                x0 = np.random.uniform(self.bounds[:, 0], self.bounds[:, 1])\n",
    "                res = minimize(lambda x: -self.ucb_acquisition(x.reshape(1, -1)), x0, bounds=self.bounds, method=\"L-BFGS-B\")\n",
    "                if res.fun < best_value:\n",
    "                    best_value = res.fun\n",
    "                    best_x = res.x\n",
    "            return best_x\n",
    "\n",
    "    def batch_selection(self):\n",
    "        \"\"\"Select multiple experiments using the 'constant liar' approach.\"\"\"\n",
    "        selected_points = []\n",
    "        for _ in range(self.batch_size):\n",
    "            next_x = self.optimize_acquisition()\n",
    "            selected_points.append(next_x)\n",
    "            \n",
    "            # \"Lying\" step: Assume a mean value for the next point before real data comes in\n",
    "            fake_y = self.gp.predict(next_x.reshape(1, -1)).mean()\n",
    "            self.add_observations(next_x.reshape(1, -1), fake_y)\n",
    "        \n",
    "        return np.array(selected_points)\n",
    "\n",
    "# Example usage\n",
    "bounds = [(0, 10), (0, 5)]  # Example bounds for 2 variables\n",
    "is_categorical = [False, True]  # First variable is continuous, second is categorical\n",
    "\n",
    "bo = BayesianOptimizer(bounds, is_categorical)\n",
    "\n",
    "# Assume we already have some observations\n",
    "X_initial = np.array([[2, 1], [4, 0], [6, 1]])  # Example (continuous, categorical)\n",
    "y_initial = np.array([0.5, 1.2, 0.8])  # Example target values\n",
    "bo.add_observations(X_initial, y_initial)\n",
    "\n",
    "# Get the next batch of experiments\n",
    "next_experiments = bo.batch_selection()\n",
    "print(\"Next batch of experiments:\", next_experiments)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
