{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the GPyTorch version of a custom bayesian optimizer being developed for IDEABio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import qmc\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, Matern, RationalQuadratic, Kernel, Hyperparameter\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "import GPy\n",
    "import gpytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an old GPy attempt at a category overlap kernel for reference (can delete later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CategoryOverlapKernelGPy(GPy.kern.Kern):\n",
    "    \"\"\"GPy implementation of a categorical overlap kernel.\"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, variance=1.0, active_dims=None, name='catoverlap'):\n",
    "        super().__init__(input_dim, active_dims=active_dims, name=name)\n",
    "        self.variance = GPy.core.parameterization.Param('variance', variance)\n",
    "        self.link_parameter(self.variance)\n",
    "\n",
    "    def K(self, X, X2=None):\n",
    "        if X2 is None:\n",
    "            X2 = X\n",
    "\n",
    "        diff = X[:, None] - X2[None, :]\n",
    "        diff[np.where(np.abs(diff))] = 1  # Mark different categories\n",
    "        k_cat = self.variance * (1 - np.mean(diff, axis=-1))  # Normalize overlap\n",
    "        return k_cat\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just a test to ensure kernel matrix looks correct for categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPy Kernel Matrix:\n",
      " [[1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 1. 0. 0. 0.]\n",
      " [0. 1. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "X_test = np.array([[4], [2], [2], [1], [0], [8]]) #each row is a category\n",
    "\n",
    "# Create kernel and compute the kernel matrix\n",
    "gpy_kernel = CategoryOverlapKernelGPy(input_dim=1, variance=1.0)\n",
    "K_gpy = gpy_kernel.K(X_test)\n",
    "print(\"GPy Kernel Matrix:\\n\", K_gpy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a chatgpt generated model for a simple GP using Matern kernel for me to study how this all works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: tensor([0.4926, 0.4977]), Std: tensor([0.0669, 0.0671])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gpytorch\n",
    "\n",
    "# Define a simple Gaussian Process model using GPyTorch\n",
    "class SimpleGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super().__init__(train_x, train_y, likelihood)\n",
    "        \n",
    "        # Define the mean function (constant mean assumes data has an underlying constant trend)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        \n",
    "        # Define the covariance function (Matern kernel models spatial correlations with smoothness parameter nu=2.5)\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.MaternKernel(nu=2.5))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Compute the mean and covariance matrix for input data\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        \n",
    "        # Return a Gaussian distribution with computed mean and covariance\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "# Wrapper class for training and prediction using the GP model\n",
    "class SimpleGPR:\n",
    "    def __init__(self, input_dim, noise_var=1e-5):\n",
    "        self.input_dim = input_dim  # Number of input dimensions (features)\n",
    "        self.likelihood = gpytorch.likelihoods.GaussianLikelihood()  # Gaussian likelihood models observation noise\n",
    "        self.model = None  # Placeholder for GP model\n",
    "        self.X, self.y = None, None  # Storage for training data\n",
    "        self.noise_var = noise_var  # Noise variance\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        # Store training data (ensure y is a 1D tensor)\n",
    "        self.X, self.y = X, y.squeeze()\n",
    "        \n",
    "        # Initialize the Gaussian Process model\n",
    "        self.model = SimpleGPModel(self.X, self.y, self.likelihood)\n",
    "        \n",
    "        # Set the model and likelihood to training mode\n",
    "        self.model.train()\n",
    "        self.likelihood.train()\n",
    "        \n",
    "        # Use Adam optimizer to maximize marginal likelihood\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=0.1)\n",
    "        \n",
    "        # Define the loss function (negative marginal log likelihood)\n",
    "        mll = gpytorch.mlls.ExactMarginalLogLikelihood(self.likelihood, self.model)\n",
    "        \n",
    "        # Training loop (optimize parameters for 50 iterations)\n",
    "        for _ in range(50):\n",
    "            optimizer.zero_grad()  # Reset gradients\n",
    "            output = self.model(self.X)  # Compute GP predictions\n",
    "            loss = -mll(output, self.y)  # Compute negative log likelihood\n",
    "            loss.backward()  # Compute gradients\n",
    "            optimizer.step()  # Update model parameters\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # Set the model and likelihood to evaluation mode\n",
    "        self.model.eval()\n",
    "        self.likelihood.eval()\n",
    "        \n",
    "        # Disable gradient computation for prediction\n",
    "        with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "            preds = self.model(X)  # Get GP predictions\n",
    "        \n",
    "        # Return mean prediction and standard deviation\n",
    "        return preds.mean, preds.variance.sqrt()\n",
    "\n",
    "# Example Usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Example training data (3 continuous input features)\n",
    "    X = torch.tensor([\n",
    "        [0.5, 0.2, 0.1],\n",
    "        [0.2, 0.8, 0.3],\n",
    "        [0.7, 0.5, 0.6]\n",
    "    ])  # Training inputs\n",
    "    y = torch.tensor([0.3, 0.7, 0.5])  # Corresponding target values\n",
    "    \n",
    "    # Instantiate and train the Gaussian Process Regression model\n",
    "    gpr = SimpleGPR(input_dim=3)\n",
    "    gpr.fit(X, y)\n",
    "    \n",
    "    # New test points for prediction (3 features each)\n",
    "    new_X = torch.tensor([\n",
    "        [0.6, 0.3, 0.4],\n",
    "        [0.3, 0.7, 0.2]\n",
    "    ])\n",
    "    mean, std = gpr.predict(new_X)\n",
    "    \n",
    "    # Print mean and standard deviation of predictions\n",
    "    print(f\"Mean: {mean}, Std: {std}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This below is the same as above but with chatgpt trying to integrate the cat overlap. it will clearly not work, but could potentially be useful for further understanding gpytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " c:\\Users\\uqkmuroi\\.virtualenvs\\EnzymeKineticGPR-62lwjgN4\\Lib\\site-packages\\linear_operator\\utils\\cholesky.py:40: NumericalWarning:A not p.d., added jitter of 1.0e-06 to the diagonal\n",
      " c:\\Users\\uqkmuroi\\.virtualenvs\\EnzymeKineticGPR-62lwjgN4\\Lib\\site-packages\\linear_operator\\utils\\cholesky.py:40: NumericalWarning:A not p.d., added jitter of 1.0e-05 to the diagonal\n",
      " c:\\Users\\uqkmuroi\\.virtualenvs\\EnzymeKineticGPR-62lwjgN4\\Lib\\site-packages\\linear_operator\\utils\\cholesky.py:40: NumericalWarning:A not p.d., added jitter of 1.0e-04 to the diagonal\n"
     ]
    },
    {
     "ename": "NotPSDError",
     "evalue": "Matrix not positive definite after repeatedly adding jitter up to 1.0e-04.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotPSDError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 90\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;66;03m# Instantiate and train the Gaussian Process Regression model\u001b[39;00m\n\u001b[0;32m     89\u001b[0m gpr \u001b[38;5;241m=\u001b[39m SimpleGPR(input_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, categorical_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 90\u001b[0m \u001b[43mgpr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;66;03m# New test points for prediction (3 continuous features + 1 categorical)\u001b[39;00m\n\u001b[0;32m     93\u001b[0m new_X \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([\n\u001b[0;32m     94\u001b[0m     [\u001b[38;5;241m0.6\u001b[39m, \u001b[38;5;241m0.3\u001b[39m, \u001b[38;5;241m0.4\u001b[39m, \u001b[38;5;241m1\u001b[39m],\n\u001b[0;32m     95\u001b[0m     [\u001b[38;5;241m0.3\u001b[39m, \u001b[38;5;241m0.7\u001b[39m, \u001b[38;5;241m0.2\u001b[39m, \u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     96\u001b[0m ])\n",
      "Cell \u001b[1;32mIn[18], line 65\u001b[0m, in \u001b[0;36mSimpleGPR.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m     63\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()  \u001b[38;5;66;03m# Reset gradients\u001b[39;00m\n\u001b[0;32m     64\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX)  \u001b[38;5;66;03m# Compute GP predictions\u001b[39;00m\n\u001b[1;32m---> 65\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[43mmll\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Compute negative log likelihood\u001b[39;00m\n\u001b[0;32m     66\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()  \u001b[38;5;66;03m# Compute gradients\u001b[39;00m\n\u001b[0;32m     67\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\uqkmuroi\\.virtualenvs\\EnzymeKineticGPR-62lwjgN4\\Lib\\site-packages\\gpytorch\\module.py:82\u001b[0m, in \u001b[0;36mModule.__call__\u001b[1;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tensor, Distribution, LinearOperator]:\n\u001b[1;32m---> 82\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     83\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m     84\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [_validate_module_outputs(output) \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m outputs]\n",
      "File \u001b[1;32mc:\\Users\\uqkmuroi\\.virtualenvs\\EnzymeKineticGPR-62lwjgN4\\Lib\\site-packages\\gpytorch\\mlls\\exact_marginal_log_likelihood.py:82\u001b[0m, in \u001b[0;36mExactMarginalLogLikelihood.forward\u001b[1;34m(self, function_dist, target, *params, **kwargs)\u001b[0m\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNaN observation policy \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfill\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is not supported by ExactMarginalLogLikelihood!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     81\u001b[0m \u001b[38;5;66;03m# Get the log prob of the marginal distribution\u001b[39;00m\n\u001b[1;32m---> 82\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     83\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_other_terms(res, params)\n\u001b[0;32m     85\u001b[0m \u001b[38;5;66;03m# Scale by the amount of data we have\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\uqkmuroi\\.virtualenvs\\EnzymeKineticGPR-62lwjgN4\\Lib\\site-packages\\gpytorch\\distributions\\multivariate_normal.py:250\u001b[0m, in \u001b[0;36mMultivariateNormal.log_prob\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;66;03m# Get log determininant and first part of quadratic form\u001b[39;00m\n\u001b[0;32m    249\u001b[0m covar \u001b[38;5;241m=\u001b[39m covar\u001b[38;5;241m.\u001b[39mevaluate_kernel()\n\u001b[1;32m--> 250\u001b[0m inv_quad, logdet \u001b[38;5;241m=\u001b[39m \u001b[43mcovar\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minv_quad_logdet\u001b[49m\u001b[43m(\u001b[49m\u001b[43minv_quad_rhs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdiff\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogdet\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    252\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28msum\u001b[39m([inv_quad, logdet, diff\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m math\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m math\u001b[38;5;241m.\u001b[39mpi)])\n\u001b[0;32m    253\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[1;32mc:\\Users\\uqkmuroi\\.virtualenvs\\EnzymeKineticGPR-62lwjgN4\\Lib\\site-packages\\linear_operator\\operators\\_linear_operator.py:1709\u001b[0m, in \u001b[0;36mLinearOperator.inv_quad_logdet\u001b[1;34m(self, inv_quad_rhs, logdet, reduce_inv_quad)\u001b[0m\n\u001b[0;32m   1707\u001b[0m             will_need_cholesky \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1708\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m will_need_cholesky:\n\u001b[1;32m-> 1709\u001b[0m         cholesky \u001b[38;5;241m=\u001b[39m CholLinearOperator(TriangularLinearOperator(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcholesky\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[0;32m   1710\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cholesky\u001b[38;5;241m.\u001b[39minv_quad_logdet(\n\u001b[0;32m   1711\u001b[0m         inv_quad_rhs\u001b[38;5;241m=\u001b[39minv_quad_rhs,\n\u001b[0;32m   1712\u001b[0m         logdet\u001b[38;5;241m=\u001b[39mlogdet,\n\u001b[0;32m   1713\u001b[0m         reduce_inv_quad\u001b[38;5;241m=\u001b[39mreduce_inv_quad,\n\u001b[0;32m   1714\u001b[0m     )\n\u001b[0;32m   1716\u001b[0m \u001b[38;5;66;03m# Short circuit to inv_quad function if we're not computing logdet\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\uqkmuroi\\.virtualenvs\\EnzymeKineticGPR-62lwjgN4\\Lib\\site-packages\\linear_operator\\operators\\_linear_operator.py:1311\u001b[0m, in \u001b[0;36mLinearOperator.cholesky\u001b[1;34m(self, upper)\u001b[0m\n\u001b[0;32m   1301\u001b[0m \u001b[38;5;129m@_implements\u001b[39m(torch\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mcholesky)\n\u001b[0;32m   1302\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcholesky\u001b[39m(\n\u001b[0;32m   1303\u001b[0m     \u001b[38;5;28mself\u001b[39m: Float[LinearOperator, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*batch N N\u001b[39m\u001b[38;5;124m\"\u001b[39m], upper: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1304\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Float[LinearOperator, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*batch N N\u001b[39m\u001b[38;5;124m\"\u001b[39m]:  \u001b[38;5;66;03m# returns TriangularLinearOperator\u001b[39;00m\n\u001b[0;32m   1305\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1306\u001b[0m \u001b[38;5;124;03m    Cholesky-factorizes the LinearOperator.\u001b[39;00m\n\u001b[0;32m   1307\u001b[0m \n\u001b[0;32m   1308\u001b[0m \u001b[38;5;124;03m    :param upper: Upper triangular or lower triangular factor (default: False).\u001b[39;00m\n\u001b[0;32m   1309\u001b[0m \u001b[38;5;124;03m    :return: Cholesky factor (lower or upper triangular)\u001b[39;00m\n\u001b[0;32m   1310\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1311\u001b[0m     chol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cholesky\u001b[49m\u001b[43m(\u001b[49m\u001b[43mupper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m   1312\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m upper:\n\u001b[0;32m   1313\u001b[0m         chol \u001b[38;5;241m=\u001b[39m chol\u001b[38;5;241m.\u001b[39m_transpose_nonbatch()\n",
      "File \u001b[1;32mc:\\Users\\uqkmuroi\\.virtualenvs\\EnzymeKineticGPR-62lwjgN4\\Lib\\site-packages\\linear_operator\\utils\\memoize.py:59\u001b[0m, in \u001b[0;36m_cached.<locals>.g\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     57\u001b[0m kwargs_pkl \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mdumps(kwargs)\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_in_cache(\u001b[38;5;28mself\u001b[39m, cache_name, \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl):\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _add_to_cache(\u001b[38;5;28mself\u001b[39m, cache_name, \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl)\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _get_from_cache(\u001b[38;5;28mself\u001b[39m, cache_name, \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl)\n",
      "File \u001b[1;32mc:\\Users\\uqkmuroi\\.virtualenvs\\EnzymeKineticGPR-62lwjgN4\\Lib\\site-packages\\linear_operator\\operators\\_linear_operator.py:528\u001b[0m, in \u001b[0;36mLinearOperator._cholesky\u001b[1;34m(self, upper)\u001b[0m\n\u001b[0;32m    525\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m TriangularLinearOperator(evaluated_mat\u001b[38;5;241m.\u001b[39mclamp_min(\u001b[38;5;241m0.0\u001b[39m)\u001b[38;5;241m.\u001b[39msqrt())\n\u001b[0;32m    527\u001b[0m \u001b[38;5;66;03m# contiguous call is necessary here\u001b[39;00m\n\u001b[1;32m--> 528\u001b[0m cholesky \u001b[38;5;241m=\u001b[39m \u001b[43mpsd_safe_cholesky\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluated_mat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mupper\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[0;32m    529\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m TriangularLinearOperator(cholesky, upper\u001b[38;5;241m=\u001b[39mupper)\n",
      "File \u001b[1;32mc:\\Users\\uqkmuroi\\.virtualenvs\\EnzymeKineticGPR-62lwjgN4\\Lib\\site-packages\\linear_operator\\utils\\cholesky.py:65\u001b[0m, in \u001b[0;36mpsd_safe_cholesky\u001b[1;34m(A, upper, out, jitter, max_tries)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpsd_safe_cholesky\u001b[39m(A, upper\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, jitter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, max_tries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     51\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute the Cholesky decomposition of A. If A is only p.s.d, add a small jitter to the diagonal.\u001b[39;00m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;124;03m        :attr:`A` (Tensor):\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;124;03m            Number of attempts (with successively increasing jitter) to make before raising an error.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 65\u001b[0m     L \u001b[38;5;241m=\u001b[39m \u001b[43m_psd_safe_cholesky\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjitter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjitter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_tries\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m upper:\n\u001b[0;32m     67\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\uqkmuroi\\.virtualenvs\\EnzymeKineticGPR-62lwjgN4\\Lib\\site-packages\\linear_operator\\utils\\cholesky.py:47\u001b[0m, in \u001b[0;36m_psd_safe_cholesky\u001b[1;34m(A, out, jitter, max_tries)\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39many(info):\n\u001b[0;32m     46\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m L\n\u001b[1;32m---> 47\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m NotPSDError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMatrix not positive definite after repeatedly adding jitter up to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mjitter_new\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.1e\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNotPSDError\u001b[0m: Matrix not positive definite after repeatedly adding jitter up to 1.0e-04."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gpytorch\n",
    "\n",
    "# Define a custom categorical kernel using category overlap\n",
    "class CategoricalKernel(gpytorch.kernels.Kernel):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(has_lengthscale=False, **kwargs)  # No lengthscale for categorical kernel\n",
    "        self.overlap_variance = torch.nn.Parameter(torch.tensor(1.0))  # Trainable variance parameter\n",
    "\n",
    "    def forward(self, x1, x2, **params):\n",
    "        # Compute category overlap matrix (1 if same, 0 if different)\n",
    "        overlap_matrix = (x1[:, None] == x2[None, :]).float().sum(dim=-1)\n",
    "        return self.overlap_variance * overlap_matrix\n",
    "\n",
    "# Define a Gaussian Process model using both Matern kernel (for continuous) and categorical kernel\n",
    "class SimpleGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood, cat_dim):\n",
    "        super().__init__(train_x, train_y, likelihood)\n",
    "        \n",
    "        # Define the mean function (assumes an underlying constant trend)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        \n",
    "        # Define the covariance function\n",
    "        self.continuous_covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.MaternKernel(nu=2.5))\n",
    "        self.categorical_covar_module = CategoricalKernel()\n",
    "        \n",
    "        # Combine continuous and categorical kernels additively\n",
    "        self.covar_module = self.continuous_covar_module + self.categorical_covar_module\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "# Wrapper class for training and prediction using the GP model\n",
    "class SimpleGPR:\n",
    "    def __init__(self, input_dim=3, categorical_dim=1, noise_var=1e-5):\n",
    "        self.input_dim = input_dim  # Number of continuous input dimensions\n",
    "        self.categorical_dim = categorical_dim  # Number of categorical variables\n",
    "        self.likelihood = gpytorch.likelihoods.GaussianLikelihood()  # Gaussian likelihood models observation noise\n",
    "        self.model = None  # Placeholder for GP model\n",
    "        self.X, self.y = None, None  # Storage for training data\n",
    "        self.noise_var = noise_var  # Noise variance\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.X, self.y = X, y.squeeze()\n",
    "        \n",
    "        # Initialize the Gaussian Process model\n",
    "        self.model = SimpleGPModel(self.X, self.y, self.likelihood, self.categorical_dim)\n",
    "        \n",
    "        # Set the model and likelihood to training mode\n",
    "        self.model.train()\n",
    "        self.likelihood.train()\n",
    "        \n",
    "        # Use Adam optimizer to maximize marginal likelihood\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=0.1)\n",
    "        \n",
    "        # Define the loss function (negative marginal log likelihood)\n",
    "        mll = gpytorch.mlls.ExactMarginalLogLikelihood(self.likelihood, self.model)\n",
    "        \n",
    "        # Training loop (optimize parameters for 50 iterations)\n",
    "        for _ in range(50):\n",
    "            optimizer.zero_grad()  # Reset gradients\n",
    "            output = self.model(self.X)  # Compute GP predictions\n",
    "            loss = -mll(output, self.y)  # Compute negative log likelihood\n",
    "            loss.backward()  # Compute gradients\n",
    "            optimizer.step()  # Update model parameters\n",
    "    \n",
    "    def predict(self, X):\n",
    "        self.model.eval()\n",
    "        self.likelihood.eval()\n",
    "        \n",
    "        with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "            preds = self.model(X)\n",
    "        \n",
    "        return preds.mean, preds.variance.sqrt()\n",
    "\n",
    "# Example Usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Example training data (3 continuous input features + 1 categorical variable)\n",
    "    X = torch.tensor([\n",
    "        [0.5, 0.2, 0.1, 0],\n",
    "        [0.2, 0.8, 0.3, 1],\n",
    "        [0.7, 0.5, 0.6, 0]\n",
    "    ])  # Training inputs (last column is categorical)\n",
    "    y = torch.tensor([0.3, 0.7, 0.5])  # Corresponding target values\n",
    "    \n",
    "    # Instantiate and train the Gaussian Process Regression model\n",
    "    gpr = SimpleGPR(input_dim=3, categorical_dim=1)\n",
    "    gpr.fit(X, y)\n",
    "    \n",
    "    # New test points for prediction (3 continuous features + 1 categorical)\n",
    "    new_X = torch.tensor([\n",
    "        [0.6, 0.3, 0.4, 1],\n",
    "        [0.3, 0.7, 0.2, 0]\n",
    "    ])\n",
    "    mean, std = gpr.predict(new_X)\n",
    "    \n",
    "    print(f\"Mean: {mean}, Std: {std}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an attempt at making the GPyTorch model work with the cocabo concept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gpytorch\n",
    "\n",
    "class CategoryOverlapKernel(gpytorch.kernels.Kernel):\n",
    "    \"\"\"Custom kernel for cateogrical varaibles using category overlap similarity.\"\"\"\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(has_lengthsacle = False, **kwargs)\n",
    "    \n",
    "    def forward(self, x1, x2, diag = False, **params):\n",
    "        if diag:\n",
    "            return torch.ones(x1.shape[0], dtype=x1.dtype, device=x1.device)\n",
    "        \n",
    "        #check if categorical values are teh same (Kronecker delta function)\n",
    "        overlap = x1[:, None] == x2[None, :].float()\n",
    "        return overlap\n",
    "    \n",
    "    \n",
    "class CombinedKernel(gpytorch.kernels.Kernel):\n",
    "    \"\"\"Combined Matern kernel and Category Overlap Kernel\"\"\"\n",
    "\n",
    "    def __init__(self, matern_nu=2.5, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.matern_kernel = gpytorch.kernels.MaternKernel(nu=matern_nu)\n",
    "        self.category_kernel = CategoryOverlapKernel()\n",
    "\n",
    "    def forward(self, x1, x2, diag = False, **params):\n",
    "        #split cont and cat features\n",
    "        x1_cont, x1_cat = x1[..., :-1], x1[..., -1]\n",
    "        x2_cont, x2_cat = x2[..., :-1], x2[..., -1]\n",
    "\n",
    "        # computer kernel values\n",
    "        matern_val = self.matern_kernel(x1_cont, x2_cont, diag=diag, **params)\n",
    "        category_val = self.category_kernel(x1_cat, x2_cat, diag=diag, **params)\n",
    "\n",
    "        #combine kernels\n",
    "        return matern_val * category_val\n",
    "    \n",
    "class CustomGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__ (self, train_x, train_y, likelihood):\n",
    "        super().__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = CombinedKernel()\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a boiler plate for a sci-kit learn implementation, technically I should do both for good practice, but ultimately the team will probably want to develop in GPyTorch for consistency, longevity, and for future development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best continuous: [0.6], Best categorical: [1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\n",
    "\n",
    "class SimpleCoCaBO:\n",
    "    def __init__(self, continuous_dim, categorical_dim, kernel=None, noise_var=1e-5):\n",
    "        # Initialize the model with dimensions of continuous and categorical variables\n",
    "        self.continuous_dim = continuous_dim\n",
    "        self.categorical_dim = categorical_dim\n",
    "        \n",
    "        # Define the kernel for the Gaussian Process (RBF + constant kernel)\n",
    "        self.kernel = kernel if kernel else C(1.0, (1e-4, 1e1)) * RBF(1.0, (1e-4, 1e1))\n",
    "        \n",
    "        # Initialize the Gaussian Process Regressor\n",
    "        self.gpr = GaussianProcessRegressor(kernel=self.kernel, alpha=noise_var)\n",
    "\n",
    "        # Storage for past data\n",
    "        self.X = []  # Stores continuous + categorical variables\n",
    "        self.y = []  # Stores corresponding objective function values\n",
    "\n",
    "    def fit(self, X_cont, X_cat, y):\n",
    "        \"\"\"Fit the Gaussian Process model on both continuous and categorical data.\"\"\"\n",
    "        # Combine continuous and categorical data\n",
    "        X_combined = np.hstack((X_cont, X_cat))\n",
    "        \n",
    "        # Fit the Gaussian Process Regressor model\n",
    "        self.gpr.fit(X_combined, y)\n",
    "        \n",
    "        # Store the data for future optimization\n",
    "        self.X.extend(X_combined)\n",
    "        self.y.extend(y)\n",
    "\n",
    "    def predict(self, X_cont, X_cat):\n",
    "        \"\"\"Predict mean and variance for new points.\"\"\"\n",
    "        X_combined = np.hstack((X_cont, X_cat))\n",
    "        mean, std = self.gpr.predict(X_combined, return_std=True)\n",
    "        return mean, std\n",
    "\n",
    "    def ucb(self, X_cont, X_cat, kappa=2.0):\n",
    "        \"\"\"Upper Confidence Bound (UCB) acquisition function.\"\"\"\n",
    "        mean, std = self.predict(X_cont, X_cat)\n",
    "        ucb_values = mean + kappa * std\n",
    "        return ucb_values\n",
    "\n",
    "    def optimize(self, X_cont, X_cat, kappa=2.0):\n",
    "        \"\"\"Optimize the acquisition function (UCB).\"\"\"\n",
    "        ucb_values = self.ucb(X_cont, X_cat, kappa)\n",
    "        best_idx = np.argmax(ucb_values)  # Select the index with the highest UCB value\n",
    "        return X_cont[best_idx], X_cat[best_idx]\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Example continuous and categorical variables\n",
    "    X_cont = np.array([[0.5], [0.2], [0.7]])  # Example continuous variables\n",
    "    X_cat = np.array([[0], [1], [0]])  # Example categorical variables (just encoded as 0 or 1)\n",
    "    y = np.array([0.3, 0.7, 0.5])  # Objective values\n",
    "\n",
    "    # Instantiate the SimpleCoCaBO object\n",
    "    optimizer = SimpleCoCaBO(continuous_dim=1, categorical_dim=1)\n",
    "\n",
    "    # Fit the model to the data\n",
    "    optimizer.fit(X_cont, X_cat, y)\n",
    "\n",
    "    # Predict UCB values for new points\n",
    "    new_cont = np.array([[0.6], [0.3]])  # New continuous points to evaluate\n",
    "    new_cat = np.array([[1], [0]])  # New categorical points\n",
    "\n",
    "    best_cont, best_cat = optimizer.optimize(new_cont, new_cat)\n",
    "    print(f\"Best continuous: {best_cont}, Best categorical: {best_cat}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EnzymeKineticGPR-62lwjgN4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
